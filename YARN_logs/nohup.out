#              -  Get data from following url
#					*"http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev"
#
#              -  Inserting data into t3_ocir_open.cool_yarn_logs
#                 Usage :
#                 ./yarnLog.sh 
#				  Example :
#                  sh -vx yarnLog.sh ocirappdev
#
# History:
# Created By: Bakul 
#===========================================================================================================

#!/bin/ksh

. /CTRLFW/OCIR/data/yarn_logs/envproperties_test.cfg
+ . /CTRLFW/OCIR/data/yarn_logs/envproperties_test.cfg
#Spark Job configurations
export COOL_JAR_NAME=CoolPocTest.jar
++ export COOL_JAR_NAME=CoolPocTest.jar
++ COOL_JAR_NAME=CoolPocTest.jar
export COOL_SPARK_JOB_CLASS_NAME=com.scb.cib.CoolLogtest
++ export COOL_SPARK_JOB_CLASS_NAME=com.scb.cib.CoolLogtest
++ COOL_SPARK_JOB_CLASS_NAME=com.scb.cib.CoolLogtest
export COOL_SPARK_JOB_DF_NAME=com.scb.cib.DfDatatest
++ export COOL_SPARK_JOB_DF_NAME=com.scb.cib.DfDatatest
++ COOL_SPARK_JOB_DF_NAME=com.scb.cib.DfDatatest

export HIVE_CONFIG_RESOURCES=/usr/hdp/current/spark-client/conf/hive-site.xml
++ export HIVE_CONFIG_RESOURCES=/usr/hdp/current/spark-client/conf/hive-site.xml
++ HIVE_CONFIG_RESOURCES=/usr/hdp/current/spark-client/conf/hive-site.xml
export HIVE_JARS_PATH=/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar
++ export HIVE_JARS_PATH=/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar
++ HIVE_JARS_PATH=/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar
export SPARK_DRIVER_MEMORY=1G
++ export SPARK_DRIVER_MEMORY=1G
++ SPARK_DRIVER_MEMORY=1G
export SPARK_EXEC_MEMORY=2G
++ export SPARK_EXEC_MEMORY=2G
++ SPARK_EXEC_MEMORY=2G
export SPARK_NUM_EXECUTORS=1
++ export SPARK_NUM_EXECUTORS=1
++ SPARK_NUM_EXECUTORS=1
export SPARK_NUM_CORES=1
++ export SPARK_NUM_CORES=1
++ SPARK_NUM_CORES=1
export SPARK_SHUFFLE_PARTITIONS=4
++ export SPARK_SHUFFLE_PARTITIONS=4
++ SPARK_SHUFFLE_PARTITIONS=4
export SPARK_DEFAULT_PARALLELISM=4
++ export SPARK_DEFAULT_PARALLELISM=4
++ SPARK_DEFAULT_PARALLELISM=4
export HIVE_FILE=/usr/hdp/current/spark-client/conf/hive-site.xml
++ export HIVE_FILE=/usr/hdp/current/spark-client/conf/hive-site.xml
++ HIVE_FILE=/usr/hdp/current/spark-client/conf/hive-site.xml

#Environment Path
log_path=/CTRLFW/OCIR/data/yarn_logs/
+ log_path=/CTRLFW/OCIR/data/yarn_logs/
jar_path=/CTRLFW/OCIR/data/yarn_logs/
+ jar_path=/CTRLFW/OCIR/data/yarn_logs/
config_Path=/CTRLFW/OCIR/data/yarn_logs/
+ config_Path=/CTRLFW/OCIR/data/yarn_logs/
file_path=/CTRLFW/OCIR/data/yarn_logs/
+ file_path=/CTRLFW/OCIR/data/yarn_logs/

#File patih
log_file=${log_path}/"yarn_log`date -u +'%Y%m%d%H%M%S'`.log"
date -u +'%Y%m%d%H%M%S'
++ date -u +%Y%m%d%H%M%S
+ log_file=/CTRLFW/OCIR/data/yarn_logs//yarn_log20200316022015.log
start_time_file=${config_Path}/startTime_test.txt
+ start_time_file=/CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
start_timebkp_file=${config_Path}/startTimebkp_test.txt
+ start_timebkp_file=/CTRLFW/OCIR/data/yarn_logs//startTimebkp_test.txt

#Clear old log files
rm -f ${log_path}/*.log
+ rm -f /CTRLFW/OCIR/data/yarn_logs//yarn_log20200313095648.log
  
#CHECKING THE INPUT PARAMETERS
if [ $# -lt 1 ]
then
  echo "Error, Not Enough Arguments."
  echo "Usage : `basename $0` [odate] "
  userName=ocirappdev
  #exit 1
fi
+ '[' 0 -lt 1 ']'
+ echo 'Error, Not Enough Arguments.'
Error, Not Enough Arguments.
basename $0
++ basename yarnLog_test.sh
+ echo 'Usage : yarnLog_test.sh [odate] '
Usage : yarnLog_test.sh [odate] 
+ userName=ocirappdev

#TAKING THE INPUT VARIABLES VALUE
userName=$1
+ userName=


if [ $# -eq 2 ]
then
  echo "setting user and startTime as per input"
  START_TIME=$2
else
  #Start Time
  START_TIME=`cat $start_time_file`
fi
+ '[' 0 -eq 2 ']'
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME=1583913635

max_attempts=2
+ max_attempts=2
attempt_num=1
+ attempt_num=1

while [ ${attempt_num} -le ${max_attempts} ]; do
  #take backup of the current start time
  cat ${start_time_file} >> ${start_timebkp_file}

  #Start Time
  START_TIME1=`cat $start_time_file`
  START_TIME=$(date +%s)
  echo ${START_TIME} >> $log_file

  export SPARK_MAJOR_VERSION=2


  #Spark-JOB to fetch the API data and load it into table
  #spark-submit --class ${COOL_SPARK_JOB_CLASS_NAME} --master yarn --driver-memory ${SPARK_DRIVER_MEMORY} --executor-cores ${SPARK_NUM_CORES} --executor-memory ${SPARK_EXEC_MEMORY} --num-executors ${SPARK_NUM_EXECUTORS} --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=${SPARK_SHUFFLE_PARTITIONS} --conf spark.default.parallelism=${SPARK_DEFAULT_PARALLELISM}  --jars ${HIVE_JARS_PATH} $jar_path/${COOL_JAR_NAME} ${START_TIME1} ${userName} 2>&1 | tee ${log_file}

  if [ $? -eq 0 ]; then
    echo " API data loaded successfully for tracking " >> ${log_file}
    #date_time=$(grep "CURRENT TIME" ${log_file} | cut -d\| -f2)
    #prepare starttime file for next run
    grep "CURRENT TIME" ${log_file} | cut -d\| -f2 > ${start_time_file}
  else
    echo "API data load failed" >> ${log_file}
    #exit 1
  fi

  END_TIME=$(date +%s)
  DIFF_TIME=$(( ${END_TIME} - ${START_TIME} ))
  echo "END_TIME: " ${END_TIME} >> ${log_file}
  echo "Total time taken: " ${DIFF_TIME} >> ${log_file}
  attempt_num=$((attempt_num+1))
  sleep 3
done
+ '[' 1 -le 2 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1583913635
++ date +%s
+ START_TIME=1584325215
+ echo 1584325215
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200316022015.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584325215
+ DIFF_TIME=0
+ echo 'END_TIME: ' 1584325215
+ echo 'Total time taken: ' 0
+ attempt_num=2
+ sleep 3
+ '[' 2 -le 2 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=
++ date +%s
+ START_TIME=1584325218
+ echo 1584325218
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200316022015.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584325218
+ DIFF_TIME=0
+ echo 'END_TIME: ' 1584325218
+ echo 'Total time taken: ' 0
+ attempt_num=3
+ sleep 3
+ '[' 3 -le 2 ']'

df -h > ${file_path}/test_dfdata.txt
+ df -h
hadoop fs -df -h > ${file_path}/test_hdfsdata.txt
+ hadoop fs -df -h
sed 's/  */,/g' ${file_path}/test_dfdata.txt > ${file_path}/test_dfdata2.txt
+ sed 's/  */,/g' /CTRLFW/OCIR/data/yarn_logs//test_dfdata.txt
sed 's/  */,/g' ${file_path}/test_hdfsdata.txt > ${file_path}/test_hdfsdata2.txt
+ sed 's/  */,/g' /CTRLFW/OCIR/data/yarn_logs//test_hdfsdata.txt

START_TIME=$(date +%s)
++ date +%s
+ START_TIME=1584325226
echo ${START_TIME} >> $log_file
+ echo 1584325226

file1=${file_path}/test_dfdata2.txt
+ file1=/CTRLFW/OCIR/data/yarn_logs//test_dfdata2.txt
file2=${file_path}/test_hdfsdata2.txt
+ file2=/CTRLFW/OCIR/data/yarn_logs//test_hdfsdata2.txt

export SPARK_MAJOR_VERSION=2
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
#Spark-JOB to fetch the API data and load it into table
spark-submit --class ${COOL_SPARK_JOB_DF_NAME} --master yarn --driver-memory ${SPARK_DRIVER_MEMORY} --executor-cores ${SPARK_NUM_CORES} --executor-memory ${SPARK_EXEC_MEMORY} --num-executors ${SPARK_NUM_EXECUTORS} --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=${SPARK_SHUFFLE_PARTITIONS} --conf spark.default.parallelism=${SPARK_DEFAULT_PARALLELISM}  --jars ${HIVE_JARS_PATH} $jar_path/${COOL_JAR_NAME} ${file1} ${file2} 2>&1 | tee ${log_file}
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200316022015.log
+ spark-submit --class com.scb.cib.DfDatatest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar /CTRLFW/OCIR/data/yarn_logs//test_dfdata2.txt /CTRLFW/OCIR/data/yarn_logs//test_hdfsdata2.txt
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/16 10:20:27 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/16 10:20:29 INFO SparkContext: Submitted application: DFDatatest
20/03/16 10:20:29 INFO SecurityManager: Changing view acls to: 1619795
20/03/16 10:20:29 INFO SecurityManager: Changing modify acls to: 1619795
20/03/16 10:20:29 INFO SecurityManager: Changing view acls groups to: 
20/03/16 10:20:29 INFO SecurityManager: Changing modify acls groups to: 
20/03/16 10:20:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/16 10:20:29 INFO Utils: Successfully started service 'sparkDriver' on port 35483.
20/03/16 10:20:29 INFO SparkEnv: Registering MapOutputTracker
20/03/16 10:20:29 INFO SparkEnv: Registering BlockManagerMaster
20/03/16 10:20:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/16 10:20:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/16 10:20:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2ab7bafb-df86-44d4-ae75-fa9e560f7076
20/03/16 10:20:29 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/16 10:20:29 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/16 10:20:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/03/16 10:20:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4040
20/03/16 10:20:30 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:35483/jars/CoolPocTest.jar with timestamp 1584325230100
20/03/16 10:20:31 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/16 10:20:31 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/16 10:20:31 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/16 10:20:31 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/16 10:20:31 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/16 10:20:31 INFO Client: Setting up container launch context for our AM
20/03/16 10:20:31 INFO Client: Setting up the launch environment for our AM container
20/03/16 10:20:31 INFO Client: Preparing resources for our AM container
20/03/16 10:20:31 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/16 10:20:31 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 6909629 for 1619795 on ha-hdfs:nnscbhaastest
20/03/16 10:20:33 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/16 10:20:33 INFO metastore: Connected to metastore.
20/03/16 10:20:52 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 e1 24 d6 b0 8a 01 71 05 31 5a b0 8e 0e 6a 8e 02 6a
20/03/16 10:20:52 ERROR SparkContext: Error initializing SparkContext.
org.apache.hadoop.security.AccessControlException: Permission denied: user=1619795, access=WRITE, inode="/user/1619795/.sparkStaging/application_1583994958990_47015":hdfs:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:353)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:325)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:246)
	at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkDefaultEnforcer(RangerHdfsAuthorizer.java:428)
	at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkPermission(RangerHdfsAuthorizer.java:278)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1950)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1934)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1917)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:71)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4181)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1109)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:645)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3089)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:3057)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1181)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1177)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1195)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1169)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1924)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:617)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:427)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:845)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:170)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:173)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:509)
	at com.scb.cib.DfDatatest$.main(DfDatatest.scala:24)
	at com.scb.cib.DfDatatest.main(DfDatatest.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:782)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=1619795, access=WRITE, inode="/user/1619795/.sparkStaging/application_1583994958990_47015":hdfs:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:353)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:325)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:246)
	at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkDefaultEnforcer(RangerHdfsAuthorizer.java:428)
	at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkPermission(RangerHdfsAuthorizer.java:278)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1950)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1934)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1917)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:71)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4181)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1109)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:645)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)
	at org.apache.hadoop.ipc.Client.call(Client.java:1498)
	at org.apache.hadoop.ipc.Client.call(Client.java:1398)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at com.sun.proxy.$Proxy12.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)
	at com.sun.proxy.$Proxy13.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3087)
	... 25 more
20/03/16 10:20:52 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4040
20/03/16 10:20:52 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
20/03/16 10:20:52 INFO YarnClientSchedulerBackend: Stopped
20/03/16 10:20:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/16 10:20:52 INFO MemoryStore: MemoryStore cleared
20/03/16 10:20:52 INFO BlockManager: BlockManager stopped
20/03/16 10:20:52 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/16 10:20:52 WARN MetricsSystem: Stopping a MetricsSystem that is not running
20/03/16 10:20:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/16 10:20:53 INFO SparkContext: Successfully stopped SparkContext
Exception in thread "main" org.apache.hadoop.security.AccessControlException: Permission denied: user=1619795, access=WRITE, inode="/user/1619795/.sparkStaging/application_1583994958990_47015":hdfs:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:353)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:325)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:246)
	at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkDefaultEnforcer(RangerHdfsAuthorizer.java:428)
	at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkPermission(RangerHdfsAuthorizer.java:278)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1950)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1934)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1917)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:71)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4181)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1109)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:645)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3089)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:3057)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1181)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1177)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1195)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1169)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1924)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:617)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:427)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:845)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:170)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:173)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:509)
	at com.scb.cib.DfDatatest$.main(DfDatatest.scala:24)
	at com.scb.cib.DfDatatest.main(DfDatatest.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:782)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=1619795, access=WRITE, inode="/user/1619795/.sparkStaging/application_1583994958990_47015":hdfs:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:353)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:325)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:246)
	at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkDefaultEnforcer(RangerHdfsAuthorizer.java:428)
	at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkPermission(RangerHdfsAuthorizer.java:278)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1950)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1934)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1917)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:71)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4181)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1109)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:645)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)
	at org.apache.hadoop.ipc.Client.call(Client.java:1498)
	at org.apache.hadoop.ipc.Client.call(Client.java:1398)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at com.sun.proxy.$Proxy12.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)
	at com.sun.proxy.$Proxy13.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3087)
	... 25 more
20/03/16 10:20:53 INFO ShutdownHookManager: Shutdown hook called
20/03/16 10:20:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-77720d24-2dfe-469f-9088-06aadb23577a

if [ $? -eq 0 ]; then
  echo " DF and HDFS data loaded successfully for tracking " >> ${log_file}
  #date_time=$(grep "CURRENT TIME" ${log_file} | cut -d\| -f2)
  #prepare starttime file for next run
  #grep "CURRENT TIME" ${log_file} | cut -d\| -f2 > ${start_time_file}
else
  echo "DF and HDFS data load failed" >> ${log_file}
  #exit 1
fi
+ '[' 0 -eq 0 ']'
+ echo ' DF and HDFS data loaded successfully for tracking '

END_TIME=$(date +%s)
++ date +%s
+ END_TIME=1584325253
DIFF_TIME=$(( ${END_TIME} - ${START_TIME} ))
+ DIFF_TIME=27
echo "END_TIME: " ${END_TIME} >> ${log_file}
+ echo 'END_TIME: ' 1584325253
echo "Total time taken: " ${DIFF_TIME} >> ${log_file}
+ echo 'Total time taken: ' 27


exit 0
+ exit 0
#              -  Get data from following url
#					*"http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev"
#
#              -  Inserting data into t3_ocir_open.cool_yarn_logs
#                 Usage :
#                 ./yarnLog.sh 
#				  Example :
#                  sh -vx yarnLog.sh ocirappdev
#
# History:
# Created By: Bakul 
#===========================================================================================================

#!/bin/ksh

. /CTRLFW/OCIR/data/yarn_logs/envproperties_test.cfg
+ . /CTRLFW/OCIR/data/yarn_logs/envproperties_test.cfg
#Spark Job configurations
export COOL_JAR_NAME=CoolPocTest.jar
++ export COOL_JAR_NAME=CoolPocTest.jar
++ COOL_JAR_NAME=CoolPocTest.jar
export COOL_SPARK_JOB_CLASS_NAME=com.scb.cib.CoolLogtest
++ export COOL_SPARK_JOB_CLASS_NAME=com.scb.cib.CoolLogtest
++ COOL_SPARK_JOB_CLASS_NAME=com.scb.cib.CoolLogtest
export COOL_SPARK_JOB_DF_NAME=com.scb.cib.DfDatatest
++ export COOL_SPARK_JOB_DF_NAME=com.scb.cib.DfDatatest
++ COOL_SPARK_JOB_DF_NAME=com.scb.cib.DfDatatest

export HIVE_CONFIG_RESOURCES=/usr/hdp/current/spark-client/conf/hive-site.xml
++ export HIVE_CONFIG_RESOURCES=/usr/hdp/current/spark-client/conf/hive-site.xml
++ HIVE_CONFIG_RESOURCES=/usr/hdp/current/spark-client/conf/hive-site.xml
export HIVE_JARS_PATH=/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar
++ export HIVE_JARS_PATH=/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar
++ HIVE_JARS_PATH=/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar
export SPARK_DRIVER_MEMORY=1G
++ export SPARK_DRIVER_MEMORY=1G
++ SPARK_DRIVER_MEMORY=1G
export SPARK_EXEC_MEMORY=2G
++ export SPARK_EXEC_MEMORY=2G
++ SPARK_EXEC_MEMORY=2G
export SPARK_NUM_EXECUTORS=1
++ export SPARK_NUM_EXECUTORS=1
++ SPARK_NUM_EXECUTORS=1
export SPARK_NUM_CORES=1
++ export SPARK_NUM_CORES=1
++ SPARK_NUM_CORES=1
export SPARK_SHUFFLE_PARTITIONS=4
++ export SPARK_SHUFFLE_PARTITIONS=4
++ SPARK_SHUFFLE_PARTITIONS=4
export SPARK_DEFAULT_PARALLELISM=4
++ export SPARK_DEFAULT_PARALLELISM=4
++ SPARK_DEFAULT_PARALLELISM=4
export HIVE_FILE=/usr/hdp/current/spark-client/conf/hive-site.xml
++ export HIVE_FILE=/usr/hdp/current/spark-client/conf/hive-site.xml
++ HIVE_FILE=/usr/hdp/current/spark-client/conf/hive-site.xml

#Environment Path
log_path=/CTRLFW/OCIR/data/yarn_logs/
+ log_path=/CTRLFW/OCIR/data/yarn_logs/
jar_path=/CTRLFW/OCIR/data/yarn_logs/
+ jar_path=/CTRLFW/OCIR/data/yarn_logs/
config_Path=/CTRLFW/OCIR/data/yarn_logs/
+ config_Path=/CTRLFW/OCIR/data/yarn_logs/
file_path=/CTRLFW/OCIR/data/yarn_logs/
+ file_path=/CTRLFW/OCIR/data/yarn_logs/

#File patih
log_file=${log_path}/"yarn_log`date -u +'%Y%m%d%H%M%S'`.log"
date -u +'%Y%m%d%H%M%S'
++ date -u +%Y%m%d%H%M%S
+ log_file=/CTRLFW/OCIR/data/yarn_logs//yarn_log20200318021504.log
start_time_file=${config_Path}/startTime_test.txt
+ start_time_file=/CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
start_timebkp_file=${config_Path}/startTimebkp_test.txt
+ start_timebkp_file=/CTRLFW/OCIR/data/yarn_logs//startTimebkp_test.txt

#Clear old log files
rm -f ${log_path}/*.log
+ rm -f /CTRLFW/OCIR/data/yarn_logs//yarn_log20200316022015.log
  
#CHECKING THE INPUT PARAMETERS
if [ $# -lt 1 ]
then
  echo "Error, Not Enough Arguments."
  echo "Usage : `basename $0` [odate] "
  userName=ocirappdev
  #exit 1
fi
+ '[' 0 -lt 1 ']'
+ echo 'Error, Not Enough Arguments.'
Error, Not Enough Arguments.
basename $0
++ basename yarnLog_test.sh
+ echo 'Usage : yarnLog_test.sh [odate] '
Usage : yarnLog_test.sh [odate] 
+ userName=ocirappdev

#TAKING THE INPUT VARIABLES VALUE
userName=$1
+ userName=


if [ $# -eq 2 ]
then
  echo "setting user and startTime as per input"
  START_TIME=$2
else
  #Start Time
  START_TIME=`cat $start_time_file`
fi
+ '[' 0 -eq 2 ']'
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME=

max_attempts=2
+ max_attempts=2
attempt_num=1
+ attempt_num=1

while [ ${attempt_num} -le ${max_attempts} ]; do
  #take backup of the current start time
  cat ${start_time_file} >> ${start_timebkp_file}

  #Start Time
  START_TIME1=`cat $start_time_file`
  START_TIME=$(date +%s)
  echo ${START_TIME} >> $log_file

  export SPARK_MAJOR_VERSION=2


  #Spark-JOB to fetch the API data and load it into table
  #spark-submit --class ${COOL_SPARK_JOB_CLASS_NAME} --master yarn --driver-memory ${SPARK_DRIVER_MEMORY} --executor-cores ${SPARK_NUM_CORES} --executor-memory ${SPARK_EXEC_MEMORY} --num-executors ${SPARK_NUM_EXECUTORS} --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=${SPARK_SHUFFLE_PARTITIONS} --conf spark.default.parallelism=${SPARK_DEFAULT_PARALLELISM}  --jars ${HIVE_JARS_PATH} $jar_path/${COOL_JAR_NAME} ${START_TIME1} ${userName} 2>&1 | tee ${log_file}

  if [ $? -eq 0 ]; then
    echo " API data loaded successfully for tracking " >> ${log_file}
    #date_time=$(grep "CURRENT TIME" ${log_file} | cut -d\| -f2)
    #prepare starttime file for next run
    grep "CURRENT TIME" ${log_file} | cut -d\| -f2 > ${start_time_file}
  else
    echo "API data load failed" >> ${log_file}
    #exit 1
  fi

  END_TIME=$(date +%s)
  DIFF_TIME=$(( ${END_TIME} - ${START_TIME} ))
  echo "END_TIME: " ${END_TIME} >> ${log_file}
  echo "Total time taken: " ${DIFF_TIME} >> ${log_file}
  attempt_num=$((attempt_num+1))
  sleep 3
done
+ '[' 1 -le 2 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=
++ date +%s
+ START_TIME=1584497704
+ echo 1584497704
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318021504.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584497704
+ DIFF_TIME=0
+ echo 'END_TIME: ' 1584497704
+ echo 'Total time taken: ' 0
+ attempt_num=2
+ sleep 3
+ '[' 2 -le 2 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=
++ date +%s
+ START_TIME=1584497707
+ echo 1584497707
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318021504.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584497707
+ DIFF_TIME=0
+ echo 'END_TIME: ' 1584497707
+ echo 'Total time taken: ' 0
+ attempt_num=3
+ sleep 3
+ '[' 3 -le 2 ']'

df -h > ${file_path}/test_dfdata.txt
+ df -h
hadoop fs -df -h > ${file_path}/test_hdfsdata.txt
+ hadoop fs -df -h
sed 's/  */,/g' ${file_path}/test_dfdata.txt > ${file_path}/test_dfdata2.txt
+ sed 's/  */,/g' /CTRLFW/OCIR/data/yarn_logs//test_dfdata.txt
sed 's/  */,/g' ${file_path}/test_hdfsdata.txt > ${file_path}/test_hdfsdata2.txt
+ sed 's/  */,/g' /CTRLFW/OCIR/data/yarn_logs//test_hdfsdata.txt

START_TIME=$(date +%s)
++ date +%s
+ START_TIME=1584497715
echo ${START_TIME} >> $log_file
+ echo 1584497715

file1=${file_path}/test_dfdata2.txt
+ file1=/CTRLFW/OCIR/data/yarn_logs//test_dfdata2.txt
file2=${file_path}/test_hdfsdata2.txt
+ file2=/CTRLFW/OCIR/data/yarn_logs//test_hdfsdata2.txt

export SPARK_MAJOR_VERSION=2
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
#Spark-JOB to fetch the API data and load it into table
spark-submit --class ${COOL_SPARK_JOB_DF_NAME} --master yarn --driver-memory ${SPARK_DRIVER_MEMORY} --executor-cores ${SPARK_NUM_CORES} --executor-memory ${SPARK_EXEC_MEMORY} --num-executors ${SPARK_NUM_EXECUTORS} --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=${SPARK_SHUFFLE_PARTITIONS} --conf spark.default.parallelism=${SPARK_DEFAULT_PARALLELISM}  --jars ${HIVE_JARS_PATH} $jar_path/${COOL_JAR_NAME} ${file1} ${file2} 2>&1 | tee ${log_file}
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318021504.log
+ spark-submit --class com.scb.cib.DfDatatest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar /CTRLFW/OCIR/data/yarn_logs//test_dfdata2.txt /CTRLFW/OCIR/data/yarn_logs//test_hdfsdata2.txt
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 10:15:16 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 10:15:17 INFO SparkContext: Submitted application: DFDatatest
20/03/18 10:15:17 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 10:15:17 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 10:15:17 INFO SecurityManager: Changing view acls groups to: 
20/03/18 10:15:17 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 10:15:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 10:15:17 INFO Utils: Successfully started service 'sparkDriver' on port 46784.
20/03/18 10:15:17 INFO SparkEnv: Registering MapOutputTracker
20/03/18 10:15:17 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 10:15:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 10:15:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 10:15:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1b1c96f2-6de7-4fe6-960b-87830f1f0461
20/03/18 10:15:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 10:15:17 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 10:15:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/18 10:15:18 INFO Utils: Successfully started service 'SparkUI' on port 4041.
20/03/18 10:15:18 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4041
20/03/18 10:15:18 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:46784/jars/CoolPocTest.jar with timestamp 1584497718080
20/03/18 10:15:19 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 10:15:19 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 10:15:19 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 10:15:19 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 10:15:19 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 10:15:19 INFO Client: Setting up container launch context for our AM
20/03/18 10:15:19 INFO Client: Setting up the launch environment for our AM container
20/03/18 10:15:19 INFO Client: Preparing resources for our AM container
20/03/18 10:15:19 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 10:15:19 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7042425 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 10:15:21 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 10:15:21 INFO metastore: Connected to metastore.
20/03/18 10:15:38 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 eb 6c c1 dc 8a 01 71 0f 79 45 dc 8e 01 8b 8e 02 73
20/03/18 10:15:38 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 10:15:38 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 10:15:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_66597/datanucleus-api-jdo-3.2.6.jar
20/03/18 10:15:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_66597/datanucleus-core-3.2.10.jar
20/03/18 10:15:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_66597/datanucleus-rdbms-3.2.9.jar
20/03/18 10:15:38 INFO Client: Uploading resource file:/tmp/spark-496e3755-c8d4-4ac0-86eb-00042e00f13d/__spark_conf__7153085086254809819.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_66597/__spark_conf__.zip
20/03/18 10:15:38 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 10:15:38 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 10:15:38 INFO SecurityManager: Changing view acls groups to: 
20/03/18 10:15:38 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 10:15:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 10:15:38 INFO Client: Submitting application application_1583994958990_66597 to ResourceManager
20/03/18 10:15:38 INFO YarnClientImpl: Submitted application application_1583994958990_66597
20/03/18 10:15:38 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_66597 and attemptId None
20/03/18 10:15:39 INFO Client: Application report for application_1583994958990_66597 (state: ACCEPTED)
20/03/18 10:15:39 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584497738914
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_66597/
	 user: 1619795
20/03/18 10:15:40 INFO Client: Application report for application_1583994958990_66597 (state: ACCEPTED)
20/03/18 10:15:41 INFO Client: Application report for application_1583994958990_66597 (state: ACCEPTED)
20/03/18 10:15:42 INFO Client: Application report for application_1583994958990_66597 (state: ACCEPTED)
20/03/18 10:15:43 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_66597,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_66597), /proxy/application_1583994958990_66597
20/03/18 10:15:43 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 10:15:43 INFO Client: Application report for application_1583994958990_66597 (state: RUNNING)
20/03/18 10:15:43 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.48
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584497738914
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_66597/
	 user: 1619795
20/03/18 10:15:43 INFO YarnClientSchedulerBackend: Application application_1583994958990_66597 has started running.
20/03/18 10:15:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39811.
20/03/18 10:15:43 INFO NettyBlockTransferService: Server created on 10.20.174.137:39811
20/03/18 10:15:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 10:15:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 39811, None)
20/03/18 10:15:43 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:39811 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 39811, None)
20/03/18 10:15:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 39811, None)
20/03/18 10:15:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 39811, None)
20/03/18 10:15:44 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_66597
20/03/18 10:15:44 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 10:15:48 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
20/03/18 10:15:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 424.8 KB, free 365.9 MB)
20/03/18 10:15:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.3 KB, free 365.8 MB)
20/03/18 10:15:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:39811 (size: 38.3 KB, free: 366.3 MB)
20/03/18 10:15:48 INFO SparkContext: Created broadcast 0 from textFile at DfDatatest.scala:42
20/03/18 10:15:48 INFO FileInputFormat: Total input paths to process : 1
20/03/18 10:15:48 INFO SparkContext: Starting job: first at DfDatatest.scala:43
20/03/18 10:15:48 INFO DAGScheduler: Got job 0 (first at DfDatatest.scala:43) with 1 output partitions
20/03/18 10:15:48 INFO DAGScheduler: Final stage: ResultStage 0 (first at DfDatatest.scala:43)
20/03/18 10:15:48 INFO DAGScheduler: Parents of final stage: List()
20/03/18 10:15:48 INFO DAGScheduler: Missing parents: List()
20/03/18 10:15:48 INFO DAGScheduler: Submitting ResultStage 0 (file:///CTRLFW/OCIR/data/yarn_logs//test_dfdata2.txt MapPartitionsRDD[1] at textFile at DfDatatest.scala:42), which has no missing parents
20/03/18 10:15:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 365.8 MB)
20/03/18 10:15:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2001.0 B, free 365.8 MB)
20/03/18 10:15:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:39811 (size: 2001.0 B, free: 366.3 MB)
20/03/18 10:15:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 10:15:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (file:///CTRLFW/OCIR/data/yarn_logs//test_dfdata2.txt MapPartitionsRDD[1] at textFile at DfDatatest.scala:42) (first 15 tasks are for partitions Vector(0))
20/03/18 10:15:48 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/03/18 10:15:49 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.23:38208) with ID 1
20/03/18 10:15:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas016.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4880 bytes)
20/03/18 10:15:49 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas016.global.standardchartered.com:45661 with 912.3 MB RAM, BlockManagerId(1, hklpathas016.global.standardchartered.com, 45661, None)
20/03/18 10:15:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas016.global.standardchartered.com:45661 (size: 2001.0 B, free: 912.3 MB)
20/03/18 10:15:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas016.global.standardchartered.com:45661 (size: 38.3 KB, free: 912.3 MB)
20/03/18 10:15:51 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, hklpathas016.global.standardchartered.com, executor 1): java.io.FileNotFoundException: File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:348)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:786)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:251)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:250)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

20/03/18 10:15:51 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1, hklpathas016.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4880 bytes)
20/03/18 10:15:51 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on hklpathas016.global.standardchartered.com, executor 1: java.io.FileNotFoundException (File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist) [duplicate 1]
20/03/18 10:15:51 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, hklpathas016.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4880 bytes)
20/03/18 10:15:51 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on hklpathas016.global.standardchartered.com, executor 1: java.io.FileNotFoundException (File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist) [duplicate 2]
20/03/18 10:15:51 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, hklpathas016.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4880 bytes)
20/03/18 10:15:51 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on hklpathas016.global.standardchartered.com, executor 1: java.io.FileNotFoundException (File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist) [duplicate 3]
20/03/18 10:15:51 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
20/03/18 10:15:51 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 10:15:51 INFO YarnScheduler: Cancelling stage 0
20/03/18 10:15:51 INFO DAGScheduler: ResultStage 0 (first at DfDatatest.scala:43) failed in 2.334 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, hklpathas016.global.standardchartered.com, executor 1): java.io.FileNotFoundException: File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:348)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:786)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:251)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:250)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
20/03/18 10:15:51 INFO DAGScheduler: Job 0 failed: first at DfDatatest.scala:43, took 2.431016 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, hklpathas016.global.standardchartered.com, executor 1): java.io.FileNotFoundException: File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:348)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:786)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:251)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:250)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)
	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1327)
	at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1368)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.first(RDD.scala:1367)
	at com.scb.cib.DfDatatest$.main(DfDatatest.scala:43)
	at com.scb.cib.DfDatatest.main(DfDatatest.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:782)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:348)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:786)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:251)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:250)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
20/03/18 10:15:51 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 10:15:51 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4041
20/03/18 10:15:51 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 10:15:51 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 10:15:51 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 10:15:51 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 10:15:51 INFO YarnClientSchedulerBackend: Stopped
20/03/18 10:15:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 10:15:51 INFO MemoryStore: MemoryStore cleared
20/03/18 10:15:51 INFO BlockManager: BlockManager stopped
20/03/18 10:15:51 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 10:15:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 10:15:51 INFO SparkContext: Successfully stopped SparkContext
20/03/18 10:15:51 INFO ShutdownHookManager: Shutdown hook called
20/03/18 10:15:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-496e3755-c8d4-4ac0-86eb-00042e00f13d

if [ $? -eq 0 ]; then
  echo " DF and HDFS data loaded successfully for tracking " >> ${log_file}
  #date_time=$(grep "CURRENT TIME" ${log_file} | cut -d\| -f2)
  #prepare starttime file for next run
  #grep "CURRENT TIME" ${log_file} | cut -d\| -f2 > ${start_time_file}
else
  echo "DF and HDFS data load failed" >> ${log_file}
  #exit 1
fi
+ '[' 0 -eq 0 ']'
+ echo ' DF and HDFS data loaded successfully for tracking '

END_TIME=$(date +%s)
++ date +%s
+ END_TIME=1584497751
DIFF_TIME=$(( ${END_TIME} - ${START_TIME} ))
+ DIFF_TIME=36
echo "END_TIME: " ${END_TIME} >> ${log_file}
+ echo 'END_TIME: ' 1584497751
echo "Total time taken: " ${DIFF_TIME} >> ${log_file}
+ echo 'Total time taken: ' 36


exit 0
+ exit 0
#              -  Get data from following url
#					*"http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev"
#
#              -  Inserting data into t3_ocir_open.cool_yarn_logs
#                 Usage :
#                 ./yarnLog.sh 
#				  Example :
#                  sh -vx yarnLog.sh ocirappdev
#
# History:
# Created By: Bakul 
#===========================================================================================================

#!/bin/ksh

. /CTRLFW/OCIR/data/yarn_logs/envproperties_test.cfg
+ . /CTRLFW/OCIR/data/yarn_logs/envproperties_test.cfg
#Spark Job configurations
export COOL_JAR_NAME=CoolPocTest.jar
++ export COOL_JAR_NAME=CoolPocTest.jar
++ COOL_JAR_NAME=CoolPocTest.jar
export COOL_SPARK_JOB_CLASS_NAME=com.scb.cib.CoolLogtest
++ export COOL_SPARK_JOB_CLASS_NAME=com.scb.cib.CoolLogtest
++ COOL_SPARK_JOB_CLASS_NAME=com.scb.cib.CoolLogtest
export COOL_SPARK_JOB_DF_NAME=com.scb.cib.DfDatatest
++ export COOL_SPARK_JOB_DF_NAME=com.scb.cib.DfDatatest
++ COOL_SPARK_JOB_DF_NAME=com.scb.cib.DfDatatest

export HIVE_CONFIG_RESOURCES=/usr/hdp/current/spark-client/conf/hive-site.xml
++ export HIVE_CONFIG_RESOURCES=/usr/hdp/current/spark-client/conf/hive-site.xml
++ HIVE_CONFIG_RESOURCES=/usr/hdp/current/spark-client/conf/hive-site.xml
export HIVE_JARS_PATH=/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar
++ export HIVE_JARS_PATH=/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar
++ HIVE_JARS_PATH=/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar
export SPARK_DRIVER_MEMORY=1G
++ export SPARK_DRIVER_MEMORY=1G
++ SPARK_DRIVER_MEMORY=1G
export SPARK_EXEC_MEMORY=2G
++ export SPARK_EXEC_MEMORY=2G
++ SPARK_EXEC_MEMORY=2G
export SPARK_NUM_EXECUTORS=1
++ export SPARK_NUM_EXECUTORS=1
++ SPARK_NUM_EXECUTORS=1
export SPARK_NUM_CORES=1
++ export SPARK_NUM_CORES=1
++ SPARK_NUM_CORES=1
export SPARK_SHUFFLE_PARTITIONS=4
++ export SPARK_SHUFFLE_PARTITIONS=4
++ SPARK_SHUFFLE_PARTITIONS=4
export SPARK_DEFAULT_PARALLELISM=4
++ export SPARK_DEFAULT_PARALLELISM=4
++ SPARK_DEFAULT_PARALLELISM=4
export HIVE_FILE=/usr/hdp/current/spark-client/conf/hive-site.xml
++ export HIVE_FILE=/usr/hdp/current/spark-client/conf/hive-site.xml
++ HIVE_FILE=/usr/hdp/current/spark-client/conf/hive-site.xml

#Environment Path
log_path=/CTRLFW/OCIR/data/yarn_logs/
+ log_path=/CTRLFW/OCIR/data/yarn_logs/
jar_path=/CTRLFW/OCIR/data/yarn_logs/
+ jar_path=/CTRLFW/OCIR/data/yarn_logs/
config_Path=/CTRLFW/OCIR/data/yarn_logs/
+ config_Path=/CTRLFW/OCIR/data/yarn_logs/
file_path=/CTRLFW/OCIR/data/yarn_logs/
+ file_path=/CTRLFW/OCIR/data/yarn_logs/

#File patih
log_file=${log_path}/"yarn_log`date -u +'%Y%m%d%H%M%S'`.log"
date -u +'%Y%m%d%H%M%S'
++ date -u +%Y%m%d%H%M%S
+ log_file=/CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
start_time_file=${config_Path}/startTime_test.txt
+ start_time_file=/CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
start_timebkp_file=${config_Path}/startTimebkp_test.txt
+ start_timebkp_file=/CTRLFW/OCIR/data/yarn_logs//startTimebkp_test.txt

#Clear old log files
rm -f ${log_path}/*.log
+ rm -f /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318021504.log
  
#CHECKING THE INPUT PARAMETERS
if [ $# -lt 1 ]
then
  echo "Error, Not Enough Arguments."
  echo "Usage : `basename $0` [odate] "
  userName=ocirappdev
  #exit 1
fi
+ '[' 0 -lt 1 ']'
+ echo 'Error, Not Enough Arguments.'
Error, Not Enough Arguments.
basename $0
++ basename yarnLog_test.sh
+ echo 'Usage : yarnLog_test.sh [odate] '
Usage : yarnLog_test.sh [odate] 
+ userName=ocirappdev

#TAKING THE INPUT VARIABLES VALUE
#userName=$1


if [ $# -eq 2 ]
then
  echo "setting user and startTime as per input"
  START_TIME=$2
else
  #Start Time
  START_TIME=`cat $start_time_file`
fi
+ '[' 0 -eq 2 ']'
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME=1584092542

max_attempts=24
+ max_attempts=24
attempt_num=1
+ attempt_num=1

while [ ${attempt_num} -le ${max_attempts} ]; do
  #take backup of the current start time
  cat ${start_time_file} >> ${start_timebkp_file}

  #Start Time
  START_TIME1=`cat $start_time_file`
  START_TIME=$(date +%s)
  echo ${START_TIME} >> $log_file

  export SPARK_MAJOR_VERSION=2


  #Spark-JOB to fetch the API data and load it into table
  spark-submit --class ${COOL_SPARK_JOB_CLASS_NAME} --master yarn --driver-memory ${SPARK_DRIVER_MEMORY} --executor-cores ${SPARK_NUM_CORES} --executor-memory ${SPARK_EXEC_MEMORY} --num-executors ${SPARK_NUM_EXECUTORS} --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=${SPARK_SHUFFLE_PARTITIONS} --conf spark.default.parallelism=${SPARK_DEFAULT_PARALLELISM}  --jars ${HIVE_JARS_PATH} $jar_path/${COOL_JAR_NAME} ${START_TIME1} ${userName} 2>&1 | tee ${log_file}

  if [ $? -eq 0 ]; then
    echo " API data loaded successfully for tracking " >> ${log_file}
    #date_time=$(grep "CURRENT TIME" ${log_file} | cut -d\| -f2)
    #prepare starttime file for next run
    grep "CURRENT TIME" ${log_file} | cut -d\| -f2 > ${start_time_file}
  else
    echo "API data load failed" >> ${log_file}
    #exit 1
  fi

  END_TIME=$(date +%s)
  DIFF_TIME=$(( ${END_TIME} - ${START_TIME} ))
  echo "END_TIME: " ${END_TIME} >> ${log_file}
  echo "Total time taken: " ${DIFF_TIME} >> ${log_file}
  attempt_num=$((attempt_num+1))
  sleep 30m
done
+ '[' 1 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584092542
++ date +%s
+ START_TIME=1584525161
+ echo 1584525161
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584092542 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 17:52:42 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 17:52:43 INFO SparkContext: Submitted application: CoolLogtest
20/03/18 17:52:43 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 17:52:43 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 17:52:43 INFO SecurityManager: Changing view acls groups to: 
20/03/18 17:52:43 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 17:52:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 17:52:43 INFO Utils: Successfully started service 'sparkDriver' on port 42882.
20/03/18 17:52:43 INFO SparkEnv: Registering MapOutputTracker
20/03/18 17:52:43 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 17:52:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 17:52:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 17:52:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-328ef025-32cd-46ac-9e5c-242d4ebe3468
20/03/18 17:52:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 17:52:43 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 17:52:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/03/18 17:52:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4040
20/03/18 17:52:44 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:42882/jars/CoolPocTest.jar with timestamp 1584525164153
20/03/18 17:52:45 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 17:52:45 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 17:52:45 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 17:52:45 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 17:52:45 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 17:52:45 INFO Client: Setting up container launch context for our AM
20/03/18 17:52:45 INFO Client: Setting up the launch environment for our AM container
20/03/18 17:52:45 INFO Client: Preparing resources for our AM container
20/03/18 17:52:45 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 17:52:45 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7060612 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 17:52:46 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 17:52:46 INFO metastore: Connected to metastore.
20/03/18 17:53:04 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ed 0f 8c ce 8a 01 71 11 1c 10 ce 8e 02 79 8e 02 7a
20/03/18 17:53:04 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 17:53:04 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 17:53:04 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_69470/datanucleus-api-jdo-3.2.6.jar
20/03/18 17:53:04 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_69470/datanucleus-core-3.2.10.jar
20/03/18 17:53:04 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_69470/datanucleus-rdbms-3.2.9.jar
20/03/18 17:53:05 INFO Client: Uploading resource file:/tmp/spark-698d0f5f-38b1-4981-8dc2-7949d71818f8/__spark_conf__4360675283679243312.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_69470/__spark_conf__.zip
20/03/18 17:53:05 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 17:53:05 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 17:53:05 INFO SecurityManager: Changing view acls groups to: 
20/03/18 17:53:05 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 17:53:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 17:53:05 INFO Client: Submitting application application_1583994958990_69470 to ResourceManager
20/03/18 17:53:05 INFO YarnClientImpl: Submitted application application_1583994958990_69470
20/03/18 17:53:05 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_69470 and attemptId None
20/03/18 17:53:06 INFO Client: Application report for application_1583994958990_69470 (state: ACCEPTED)
20/03/18 17:53:06 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584525185249
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_69470/
	 user: 1619795
20/03/18 17:53:07 INFO Client: Application report for application_1583994958990_69470 (state: ACCEPTED)
20/03/18 17:53:08 INFO Client: Application report for application_1583994958990_69470 (state: ACCEPTED)
20/03/18 17:53:09 INFO Client: Application report for application_1583994958990_69470 (state: ACCEPTED)
20/03/18 17:53:09 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_69470,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_69470), /proxy/application_1583994958990_69470
20/03/18 17:53:09 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 17:53:10 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 17:53:10 INFO Client: Application report for application_1583994958990_69470 (state: RUNNING)
20/03/18 17:53:10 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.49
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584525185249
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_69470/
	 user: 1619795
20/03/18 17:53:10 INFO YarnClientSchedulerBackend: Application application_1583994958990_69470 has started running.
20/03/18 17:53:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41577.
20/03/18 17:53:10 INFO NettyBlockTransferService: Server created on 10.20.174.137:41577
20/03/18 17:53:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 17:53:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 41577, None)
20/03/18 17:53:10 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:41577 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 41577, None)
20/03/18 17:53:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 41577, None)
20/03/18 17:53:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 41577, None)
20/03/18 17:53:10 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_69470
20/03/18 17:53:14 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584092542
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584092542
*************************************** CURRENT TIME |1584525194|************************
20/03/18 17:53:14 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/18 17:53:14 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/18 17:53:14 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/18 17:53:14 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/18 17:53:15 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 17:53:15 INFO metastore: Connected to metastore.
20/03/18 17:53:26 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.142.67:42926) with ID 1
20/03/18 17:53:26 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas011.global.standardchartered.com:42197 with 912.3 MB RAM, BlockManagerId(1, hklpathas011.global.standardchartered.com, 42197, None)
20/03/18 17:53:32 INFO SessionState: Created local directory: /tmp/1619795
20/03/18 17:53:32 INFO SessionState: Created local directory: /tmp/a3a795fc-5597-46eb-90fb-c855644451dc_resources
20/03/18 17:53:32 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/a3a795fc-5597-46eb-90fb-c855644451dc
20/03/18 17:53:32 INFO SessionState: Created local directory: /tmp/1619795/a3a795fc-5597-46eb-90fb-c855644451dc
20/03/18 17:53:32 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/a3a795fc-5597-46eb-90fb-c855644451dc/_tmp_space.db
20/03/18 17:53:32 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 17:53:32 INFO SessionState: Created local directory: /tmp/2b9a77f9-3e3f-4ed0-9fab-0bdeb0c1e127_resources
20/03/18 17:53:32 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/2b9a77f9-3e3f-4ed0-9fab-0bdeb0c1e127
20/03/18 17:53:32 INFO SessionState: Created local directory: /tmp/1619795/2b9a77f9-3e3f-4ed0-9fab-0bdeb0c1e127
20/03/18 17:53:32 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/2b9a77f9-3e3f-4ed0-9fab-0bdeb0c1e127/_tmp_space.db
20/03/18 17:53:32 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 17:53:32 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/18 17:53:33 INFO CodeGenerator: Code generated in 155.020421 ms
20/03/18 17:53:33 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/18 17:53:33 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/18 17:53:33 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/18 17:53:33 INFO DAGScheduler: Parents of final stage: List()
20/03/18 17:53:33 INFO DAGScheduler: Missing parents: List()
20/03/18 17:53:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/18 17:53:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 17:53:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 17:53:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:41577 (size: 5.0 KB, free: 366.3 MB)
20/03/18 17:53:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/18 17:53:34 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 17:53:34 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/18 17:53:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas011.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 17:53:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas011.global.standardchartered.com:42197 (size: 5.0 KB, free: 912.3 MB)
20/03/18 17:53:34 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas011.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 17:53:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 731 ms on hklpathas011.global.standardchartered.com (executor 1) (1/4)
20/03/18 17:53:34 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas011.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 17:53:34 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 25 ms on hklpathas011.global.standardchartered.com (executor 1) (2/4)
20/03/18 17:53:34 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas011.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 41906 bytes)
20/03/18 17:53:34 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 15 ms on hklpathas011.global.standardchartered.com (executor 1) (3/4)
20/03/18 17:53:34 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 105 ms on hklpathas011.global.standardchartered.com (executor 1) (4/4)
20/03/18 17:53:34 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 17:53:34 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.871 s
20/03/18 17:53:34 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.048442 s
20/03/18 17:53:35 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/18 17:53:35 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/18 17:53:35 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/18 17:53:35 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/18 17:53:35 INFO DAGScheduler: Parents of final stage: List()
20/03/18 17:53:35 INFO DAGScheduler: Missing parents: List()
20/03/18 17:53:35 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/18 17:53:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 17:53:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 17:53:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:41577 (size: 5.0 KB, free: 366.3 MB)
20/03/18 17:53:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 17:53:35 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 17:53:35 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/18 17:53:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas011.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 17:53:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:41577 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 17:53:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas011.global.standardchartered.com:42197 (size: 5.0 KB, free: 912.3 MB)
20/03/18 17:53:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas011.global.standardchartered.com:42197 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 17:53:35 INFO ContextCleaner: Cleaned accumulator 1
20/03/18 17:53:35 INFO ContextCleaner: Cleaned accumulator 0
20/03/18 17:53:35 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas011.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 17:53:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 128 ms on hklpathas011.global.standardchartered.com (executor 1) (1/4)
20/03/18 17:53:35 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas011.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 17:53:35 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 8 ms on hklpathas011.global.standardchartered.com (executor 1) (2/4)
20/03/18 17:53:35 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas011.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 41906 bytes)
20/03/18 17:53:35 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpathas011.global.standardchartered.com (executor 1) (3/4)
20/03/18 17:53:35 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 13 ms on hklpathas011.global.standardchartered.com (executor 1) (4/4)
20/03/18 17:53:35 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/18 17:53:35 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.156 s
20/03/18 17:53:35 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.224083 s
20/03/18 17:53:35 INFO ContextCleaner: Cleaned accumulator 52
20/03/18 17:53:35 INFO ContextCleaner: Cleaned accumulator 51
20/03/18 17:53:35 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:41577 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 17:53:35 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas011.global.standardchartered.com:42197 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 17:53:36 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_17-53-36_218_2013439454635828821-1
20/03/18 17:53:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/18 17:53:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/18 17:53:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/18 17:53:36 INFO CodeGenerator: Code generated in 70.711775 ms
20/03/18 17:53:36 INFO CodeGenerator: Code generated in 30.848842 ms
20/03/18 17:53:36 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/18 17:53:36 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/18 17:53:36 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/18 17:53:36 INFO DAGScheduler: Parents of final stage: List()
20/03/18 17:53:36 INFO DAGScheduler: Missing parents: List()
20/03/18 17:53:36 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/18 17:53:36 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 309.2 KB, free 366.0 MB)
20/03/18 17:53:36 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 110.2 KB, free 365.9 MB)
20/03/18 17:53:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:41577 (size: 110.2 KB, free: 366.2 MB)
20/03/18 17:53:36 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/18 17:53:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/18 17:53:36 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/18 17:53:36 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas011.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 79713 bytes)
20/03/18 17:53:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas011.global.standardchartered.com:42197 (size: 110.2 KB, free: 912.2 MB)
20/03/18 17:53:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2062 ms on hklpathas011.global.standardchartered.com (executor 1) (1/1)
20/03/18 17:53:38 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/18 17:53:38 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.064 s
20/03/18 17:53:38 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.170516 s
20/03/18 17:53:38 INFO FileFormatWriter: Job null committed.
20/03/18 17:53:39 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18
20/03/18 17:53:39 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/18 17:53:39 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_17-53-36_218_2013439454635828821-1/-ext-10000/ods=2020-03-18/part-00000-c6f1a8df-f6e2-4057-a1cf-e4b3f116d8b5.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18/part-00000-c6f1a8df-f6e2-4057-a1cf-e4b3f116d8b5.c000, Status:true
20/03/18 17:53:39 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_17-53-36_218_2013439454635828821-1/-ext-10000/ods=2020-03-18 with partSpec {ods=2020-03-18}
20/03/18 17:53:39 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 17:53:39 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4040
20/03/18 17:53:39 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 17:53:39 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 17:53:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 17:53:39 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 17:53:39 INFO YarnClientSchedulerBackend: Stopped
20/03/18 17:53:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 17:53:39 INFO MemoryStore: MemoryStore cleared
20/03/18 17:53:39 INFO BlockManager: BlockManager stopped
20/03/18 17:53:39 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 17:53:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 17:53:39 INFO SparkContext: Successfully stopped SparkContext
20/03/18 17:53:39 INFO ShutdownHookManager: Shutdown hook called
20/03/18 17:53:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-698d0f5f-38b1-4981-8dc2-7949d71818f8
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584525219
+ DIFF_TIME=58
+ echo 'END_TIME: ' 1584525219
+ echo 'Total time taken: ' 58
+ attempt_num=2
+ sleep 30m
+ '[' 2 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584525194
++ date +%s
+ START_TIME=1584527019
+ echo 1584527019
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584525194 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 18:23:40 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 18:23:41 INFO SparkContext: Submitted application: CoolLogtest
20/03/18 18:23:41 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 18:23:41 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 18:23:41 INFO SecurityManager: Changing view acls groups to: 
20/03/18 18:23:41 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 18:23:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 18:23:42 INFO Utils: Successfully started service 'sparkDriver' on port 39992.
20/03/18 18:23:42 INFO SparkEnv: Registering MapOutputTracker
20/03/18 18:23:42 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 18:23:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 18:23:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 18:23:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4965fec4-1ba2-47db-9a2e-99d0aa49f88c
20/03/18 18:23:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 18:23:42 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 18:23:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/03/18 18:23:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4040
20/03/18 18:23:42 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:39992/jars/CoolPocTest.jar with timestamp 1584527022475
20/03/18 18:23:43 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 18:23:43 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 18:23:43 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 18:23:43 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 18:23:43 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 18:23:43 INFO Client: Setting up container launch context for our AM
20/03/18 18:23:43 INFO Client: Setting up the launch environment for our AM container
20/03/18 18:23:43 INFO Client: Preparing resources for our AM container
20/03/18 18:23:43 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 18:23:43 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7061641 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 18:23:45 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 18:23:45 INFO metastore: Connected to metastore.
20/03/18 18:24:02 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ed 2b e7 65 8a 01 71 11 38 6b 65 8e 02 8a 8e 02 7a
20/03/18 18:24:02 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 18:24:02 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 18:24:02 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_69807/datanucleus-api-jdo-3.2.6.jar
20/03/18 18:24:02 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_69807/datanucleus-core-3.2.10.jar
20/03/18 18:24:02 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_69807/datanucleus-rdbms-3.2.9.jar
20/03/18 18:24:03 INFO Client: Uploading resource file:/tmp/spark-89b92fe5-3e01-49a1-b5e0-ff911690d88a/__spark_conf__2243265722278937560.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_69807/__spark_conf__.zip
20/03/18 18:24:03 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 18:24:03 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 18:24:03 INFO SecurityManager: Changing view acls groups to: 
20/03/18 18:24:03 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 18:24:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 18:24:03 INFO Client: Submitting application application_1583994958990_69807 to ResourceManager
20/03/18 18:24:03 INFO YarnClientImpl: Submitted application application_1583994958990_69807
20/03/18 18:24:03 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_69807 and attemptId None
20/03/18 18:24:04 INFO Client: Application report for application_1583994958990_69807 (state: ACCEPTED)
20/03/18 18:24:04 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584527043548
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_69807/
	 user: 1619795
20/03/18 18:24:05 INFO Client: Application report for application_1583994958990_69807 (state: ACCEPTED)
20/03/18 18:24:06 INFO Client: Application report for application_1583994958990_69807 (state: ACCEPTED)
20/03/18 18:24:07 INFO Client: Application report for application_1583994958990_69807 (state: ACCEPTED)
20/03/18 18:24:08 INFO Client: Application report for application_1583994958990_69807 (state: ACCEPTED)
20/03/18 18:24:09 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_69807,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_69807), /proxy/application_1583994958990_69807
20/03/18 18:24:09 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 18:24:09 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 18:24:09 INFO Client: Application report for application_1583994958990_69807 (state: RUNNING)
20/03/18 18:24:09 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.142.66
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584527043548
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_69807/
	 user: 1619795
20/03/18 18:24:09 INFO YarnClientSchedulerBackend: Application application_1583994958990_69807 has started running.
20/03/18 18:24:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39723.
20/03/18 18:24:09 INFO NettyBlockTransferService: Server created on 10.20.174.137:39723
20/03/18 18:24:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 18:24:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 39723, None)
20/03/18 18:24:09 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:39723 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 39723, None)
20/03/18 18:24:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 39723, None)
20/03/18 18:24:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 39723, None)
20/03/18 18:24:10 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_69807
20/03/18 18:24:12 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584525194
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584525194
*************************************** CURRENT TIME |1584527052|************************
20/03/18 18:24:12 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/18 18:24:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/18 18:24:13 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/18 18:24:13 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/18 18:24:13 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 18:24:13 INFO metastore: Connected to metastore.
20/03/18 18:24:31 INFO SessionState: Created local directory: /tmp/1f70a6d2-4e05-417e-b838-d5e72f74f3af_resources
20/03/18 18:24:31 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/1f70a6d2-4e05-417e-b838-d5e72f74f3af
20/03/18 18:24:31 INFO SessionState: Created local directory: /tmp/1619795/1f70a6d2-4e05-417e-b838-d5e72f74f3af
20/03/18 18:24:31 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/1f70a6d2-4e05-417e-b838-d5e72f74f3af/_tmp_space.db
20/03/18 18:24:31 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 18:24:31 INFO SessionState: Created local directory: /tmp/810b8f03-7018-41c8-8305-1a5b33d2d43a_resources
20/03/18 18:24:31 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/810b8f03-7018-41c8-8305-1a5b33d2d43a
20/03/18 18:24:31 INFO SessionState: Created local directory: /tmp/1619795/810b8f03-7018-41c8-8305-1a5b33d2d43a
20/03/18 18:24:31 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/810b8f03-7018-41c8-8305-1a5b33d2d43a/_tmp_space.db
20/03/18 18:24:31 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 18:24:31 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/18 18:24:32 INFO CodeGenerator: Code generated in 144.468912 ms
20/03/18 18:24:32 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/18 18:24:32 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/18 18:24:32 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/18 18:24:32 INFO DAGScheduler: Parents of final stage: List()
20/03/18 18:24:32 INFO DAGScheduler: Missing parents: List()
20/03/18 18:24:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/18 18:24:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 18:24:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 18:24:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:39723 (size: 5.0 KB, free: 366.3 MB)
20/03/18 18:24:32 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/18 18:24:32 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 18:24:32 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/18 18:24:38 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.142.46:41722) with ID 1
20/03/18 18:24:38 INFO BlockManagerMasterEndpoint: Registering block manager hklpadhaa006.global.standardchartered.com:35534 with 912.3 MB RAM, BlockManagerId(1, hklpadhaa006.global.standardchartered.com, 35534, None)
20/03/18 18:24:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpadhaa006.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 18:24:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpadhaa006.global.standardchartered.com:35534 (size: 5.0 KB, free: 912.3 MB)
20/03/18 18:24:39 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpadhaa006.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 18:24:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 919 ms on hklpadhaa006.global.standardchartered.com (executor 1) (1/4)
20/03/18 18:24:39 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpadhaa006.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 18:24:39 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 26 ms on hklpadhaa006.global.standardchartered.com (executor 1) (2/4)
20/03/18 18:24:39 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpadhaa006.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 41906 bytes)
20/03/18 18:24:39 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 22 ms on hklpadhaa006.global.standardchartered.com (executor 1) (3/4)
20/03/18 18:24:39 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 195 ms on hklpadhaa006.global.standardchartered.com (executor 1) (4/4)
20/03/18 18:24:39 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 18:24:39 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 7.441 s
20/03/18 18:24:39 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 7.614802 s
20/03/18 18:24:40 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/18 18:24:40 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/18 18:24:40 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/18 18:24:40 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/18 18:24:40 INFO DAGScheduler: Parents of final stage: List()
20/03/18 18:24:40 INFO DAGScheduler: Missing parents: List()
20/03/18 18:24:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/18 18:24:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 18:24:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 18:24:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:39723 (size: 5.0 KB, free: 366.3 MB)
20/03/18 18:24:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 18:24:40 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 18:24:40 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/18 18:24:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpadhaa006.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 18:24:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpadhaa006.global.standardchartered.com:35534 (size: 5.0 KB, free: 912.3 MB)
20/03/18 18:24:40 INFO ContextCleaner: Cleaned accumulator 1
20/03/18 18:24:40 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpadhaa006.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 18:24:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 56 ms on hklpadhaa006.global.standardchartered.com (executor 1) (1/4)
20/03/18 18:24:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:39723 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 18:24:40 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpadhaa006.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 18:24:40 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 15 ms on hklpadhaa006.global.standardchartered.com (executor 1) (2/4)
20/03/18 18:24:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpadhaa006.global.standardchartered.com:35534 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 18:24:40 INFO ContextCleaner: Cleaned accumulator 0
20/03/18 18:24:40 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpadhaa006.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 41906 bytes)
20/03/18 18:24:40 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 16 ms on hklpadhaa006.global.standardchartered.com (executor 1) (3/4)
20/03/18 18:24:40 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 104 ms on hklpadhaa006.global.standardchartered.com (executor 1) (4/4)
20/03/18 18:24:40 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/18 18:24:40 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.190 s
20/03/18 18:24:40 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.198330 s
20/03/18 18:24:40 INFO ContextCleaner: Cleaned accumulator 52
20/03/18 18:24:40 INFO ContextCleaner: Cleaned accumulator 51
20/03/18 18:24:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:39723 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 18:24:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpadhaa006.global.standardchartered.com:35534 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 18:24:41 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_18-24-41_164_6844916860082486400-1
20/03/18 18:24:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/18 18:24:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/18 18:24:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/18 18:24:41 INFO CodeGenerator: Code generated in 58.920336 ms
20/03/18 18:24:41 INFO CodeGenerator: Code generated in 27.468071 ms
20/03/18 18:24:41 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/18 18:24:41 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/18 18:24:41 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/18 18:24:41 INFO DAGScheduler: Parents of final stage: List()
20/03/18 18:24:41 INFO DAGScheduler: Missing parents: List()
20/03/18 18:24:41 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/18 18:24:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 309.2 KB, free 366.0 MB)
20/03/18 18:24:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 110.2 KB, free 365.9 MB)
20/03/18 18:24:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:39723 (size: 110.2 KB, free: 366.2 MB)
20/03/18 18:24:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/18 18:24:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/18 18:24:41 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/18 18:24:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpadhaa006.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 79713 bytes)
20/03/18 18:24:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpadhaa006.global.standardchartered.com:35534 (size: 110.2 KB, free: 912.2 MB)
20/03/18 18:24:44 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 3054 ms on hklpadhaa006.global.standardchartered.com (executor 1) (1/1)
20/03/18 18:24:44 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/18 18:24:44 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 3.055 s
20/03/18 18:24:44 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 3.132302 s
20/03/18 18:24:44 INFO FileFormatWriter: Job null committed.
20/03/18 18:24:44 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/18 18:24:44 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_18-24-41_164_6844916860082486400-1/-ext-10000/ods=2020-03-18/part-00000-ac61706c-ce1c-4feb-8810-20f92e6de500.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18/part-00000-ac61706c-ce1c-4feb-8810-20f92e6de500.c000, Status:true
20/03/18 18:24:45 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_18-24-41_164_6844916860082486400-1/-ext-10000/ods=2020-03-18 with partSpec {ods=2020-03-18}
20/03/18 18:24:45 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 18:24:45 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4040
20/03/18 18:24:45 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 18:24:45 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 18:24:45 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 18:24:45 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 18:24:45 INFO YarnClientSchedulerBackend: Stopped
20/03/18 18:24:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 18:24:45 INFO MemoryStore: MemoryStore cleared
20/03/18 18:24:45 INFO BlockManager: BlockManager stopped
20/03/18 18:24:45 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 18:24:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 18:24:45 INFO SparkContext: Successfully stopped SparkContext
20/03/18 18:24:45 INFO ShutdownHookManager: Shutdown hook called
20/03/18 18:24:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-89b92fe5-3e01-49a1-b5e0-ff911690d88a
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584527085
+ DIFF_TIME=66
+ echo 'END_TIME: ' 1584527085
+ echo 'Total time taken: ' 66
+ attempt_num=3
+ sleep 30m
+ '[' 3 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584527052
++ date +%s
+ START_TIME=1584528885
+ echo 1584528885
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584527052 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 18:54:46 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 18:54:47 INFO SparkContext: Submitted application: CoolLogtest
20/03/18 18:54:47 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 18:54:47 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 18:54:47 INFO SecurityManager: Changing view acls groups to: 
20/03/18 18:54:47 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 18:54:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 18:54:48 INFO Utils: Successfully started service 'sparkDriver' on port 37537.
20/03/18 18:54:48 INFO SparkEnv: Registering MapOutputTracker
20/03/18 18:54:48 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 18:54:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 18:54:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 18:54:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0c85d0cd-a492-4db2-9c62-1abdd49a32aa
20/03/18 18:54:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 18:54:48 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 18:54:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/18 18:54:48 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/18 18:54:48 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
20/03/18 18:54:48 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
20/03/18 18:54:48 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
20/03/18 18:54:48 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
20/03/18 18:54:48 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
20/03/18 18:54:48 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
20/03/18 18:54:48 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.
20/03/18 18:54:48 INFO Utils: Successfully started service 'SparkUI' on port 4049.
20/03/18 18:54:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4049
20/03/18 18:54:48 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:37537/jars/CoolPocTest.jar with timestamp 1584528888511
20/03/18 18:54:49 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 18:54:49 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 18:54:49 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 18:54:49 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 18:54:49 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 18:54:49 INFO Client: Setting up container launch context for our AM
20/03/18 18:54:49 INFO Client: Setting up the launch environment for our AM container
20/03/18 18:54:49 INFO Client: Preparing resources for our AM container
20/03/18 18:54:49 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 18:54:49 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7063854 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 18:54:51 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 18:54:51 INFO metastore: Connected to metastore.
20/03/18 18:55:07 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ed 48 5e a2 8a 01 71 11 54 e2 a2 8e 02 9f 8e 02 7a
20/03/18 18:55:08 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 18:55:08 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 18:55:08 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_70683/datanucleus-api-jdo-3.2.6.jar
20/03/18 18:55:08 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_70683/datanucleus-core-3.2.10.jar
20/03/18 18:55:08 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_70683/datanucleus-rdbms-3.2.9.jar
20/03/18 18:55:08 INFO Client: Uploading resource file:/tmp/spark-0eb8f276-5f99-41aa-87db-7595efc483fd/__spark_conf__295983103434411462.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_70683/__spark_conf__.zip
20/03/18 18:55:08 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 18:55:08 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 18:55:08 INFO SecurityManager: Changing view acls groups to: 
20/03/18 18:55:08 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 18:55:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 18:55:08 INFO Client: Submitting application application_1583994958990_70683 to ResourceManager
20/03/18 18:55:08 INFO YarnClientImpl: Submitted application application_1583994958990_70683
20/03/18 18:55:08 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_70683 and attemptId None
20/03/18 18:55:09 INFO Client: Application report for application_1583994958990_70683 (state: ACCEPTED)
20/03/18 18:55:09 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584528908679
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_70683/
	 user: 1619795
20/03/18 18:55:10 INFO Client: Application report for application_1583994958990_70683 (state: ACCEPTED)
20/03/18 18:55:11 INFO Client: Application report for application_1583994958990_70683 (state: ACCEPTED)
20/03/18 18:55:12 INFO Client: Application report for application_1583994958990_70683 (state: ACCEPTED)
20/03/18 18:55:13 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_70683,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_70683), /proxy/application_1583994958990_70683
20/03/18 18:55:13 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 18:55:13 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 18:55:13 INFO Client: Application report for application_1583994958990_70683 (state: RUNNING)
20/03/18 18:55:13 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.59
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584528908679
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_70683/
	 user: 1619795
20/03/18 18:55:13 INFO YarnClientSchedulerBackend: Application application_1583994958990_70683 has started running.
20/03/18 18:55:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41025.
20/03/18 18:55:13 INFO NettyBlockTransferService: Server created on 10.20.174.137:41025
20/03/18 18:55:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 18:55:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 41025, None)
20/03/18 18:55:13 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:41025 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 41025, None)
20/03/18 18:55:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 41025, None)
20/03/18 18:55:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 41025, None)
20/03/18 18:55:14 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_70683
20/03/18 18:55:18 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.23:42628) with ID 1
20/03/18 18:55:18 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/03/18 18:55:18 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas016.global.standardchartered.com:34662 with 912.3 MB RAM, BlockManagerId(1, hklpathas016.global.standardchartered.com, 34662, None)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584527052
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584527052
*************************************** CURRENT TIME |1584528918|************************
20/03/18 18:55:19 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/18 18:55:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/18 18:55:19 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/18 18:55:19 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/18 18:55:19 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 18:55:19 INFO metastore: Connected to metastore.
20/03/18 18:55:37 INFO SessionState: Created local directory: /tmp/b430f937-9aac-4257-88bb-2210cfeb23f1_resources
20/03/18 18:55:37 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/b430f937-9aac-4257-88bb-2210cfeb23f1
20/03/18 18:55:37 INFO SessionState: Created local directory: /tmp/1619795/b430f937-9aac-4257-88bb-2210cfeb23f1
20/03/18 18:55:37 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/b430f937-9aac-4257-88bb-2210cfeb23f1/_tmp_space.db
20/03/18 18:55:37 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 18:55:37 INFO SessionState: Created local directory: /tmp/68cabdf8-e311-4714-bfbb-09984a258262_resources
20/03/18 18:55:37 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/68cabdf8-e311-4714-bfbb-09984a258262
20/03/18 18:55:37 INFO SessionState: Created local directory: /tmp/1619795/68cabdf8-e311-4714-bfbb-09984a258262
20/03/18 18:55:37 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/68cabdf8-e311-4714-bfbb-09984a258262/_tmp_space.db
20/03/18 18:55:37 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 18:55:37 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/18 18:55:38 INFO CodeGenerator: Code generated in 188.414113 ms
20/03/18 18:55:38 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/18 18:55:38 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/18 18:55:38 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/18 18:55:38 INFO DAGScheduler: Parents of final stage: List()
20/03/18 18:55:38 INFO DAGScheduler: Missing parents: List()
20/03/18 18:55:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/18 18:55:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 18:55:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 18:55:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:41025 (size: 5.0 KB, free: 366.3 MB)
20/03/18 18:55:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/18 18:55:39 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 18:55:39 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/18 18:55:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas016.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 18:55:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas016.global.standardchartered.com:34662 (size: 5.0 KB, free: 912.3 MB)
20/03/18 18:55:39 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas016.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 18:55:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 654 ms on hklpathas016.global.standardchartered.com (executor 1) (1/4)
20/03/18 18:55:39 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas016.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 18:55:39 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 28 ms on hklpathas016.global.standardchartered.com (executor 1) (2/4)
20/03/18 18:55:39 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas016.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 44106 bytes)
20/03/18 18:55:39 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 15 ms on hklpathas016.global.standardchartered.com (executor 1) (3/4)
20/03/18 18:55:39 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 119 ms on hklpathas016.global.standardchartered.com (executor 1) (4/4)
20/03/18 18:55:39 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 18:55:39 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.811 s
20/03/18 18:55:39 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.011024 s
20/03/18 18:55:39 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/18 18:55:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:41025 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 18:55:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas016.global.standardchartered.com:34662 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 18:55:40 INFO ContextCleaner: Cleaned accumulator 1
20/03/18 18:55:40 INFO ContextCleaner: Cleaned accumulator 0
20/03/18 18:55:40 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/18 18:55:40 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/18 18:55:40 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/18 18:55:40 INFO DAGScheduler: Parents of final stage: List()
20/03/18 18:55:40 INFO DAGScheduler: Missing parents: List()
20/03/18 18:55:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/18 18:55:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 18:55:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 18:55:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:41025 (size: 5.0 KB, free: 366.3 MB)
20/03/18 18:55:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 18:55:40 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 18:55:40 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/18 18:55:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas016.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 18:55:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas016.global.standardchartered.com:34662 (size: 5.0 KB, free: 912.3 MB)
20/03/18 18:55:40 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas016.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 18:55:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 115 ms on hklpathas016.global.standardchartered.com (executor 1) (1/4)
20/03/18 18:55:40 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas016.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 18:55:40 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 8 ms on hklpathas016.global.standardchartered.com (executor 1) (2/4)
20/03/18 18:55:40 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas016.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 44106 bytes)
20/03/18 18:55:40 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpathas016.global.standardchartered.com (executor 1) (3/4)
20/03/18 18:55:40 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 14 ms on hklpathas016.global.standardchartered.com (executor 1) (4/4)
20/03/18 18:55:40 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/18 18:55:40 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.142 s
20/03/18 18:55:40 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.151273 s
20/03/18 18:55:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:41025 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 18:55:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas016.global.standardchartered.com:34662 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 18:55:40 INFO ContextCleaner: Cleaned accumulator 51
20/03/18 18:55:40 INFO ContextCleaner: Cleaned accumulator 52
20/03/18 18:55:41 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_18-55-41_183_447768207727802749-1
20/03/18 18:55:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/18 18:55:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/18 18:55:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/18 18:55:41 INFO CodeGenerator: Code generated in 69.310937 ms
20/03/18 18:55:41 INFO CodeGenerator: Code generated in 45.133624 ms
20/03/18 18:55:41 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/18 18:55:41 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/18 18:55:41 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/18 18:55:41 INFO DAGScheduler: Parents of final stage: List()
20/03/18 18:55:41 INFO DAGScheduler: Missing parents: List()
20/03/18 18:55:41 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/18 18:55:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 309.2 KB, free 366.0 MB)
20/03/18 18:55:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 110.2 KB, free 365.9 MB)
20/03/18 18:55:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:41025 (size: 110.2 KB, free: 366.2 MB)
20/03/18 18:55:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/18 18:55:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/18 18:55:41 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/18 18:55:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas016.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 84113 bytes)
20/03/18 18:55:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas016.global.standardchartered.com:34662 (size: 110.2 KB, free: 912.2 MB)
20/03/18 18:55:44 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2373 ms on hklpathas016.global.standardchartered.com (executor 1) (1/1)
20/03/18 18:55:44 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/18 18:55:44 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.374 s
20/03/18 18:55:44 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.472110 s
20/03/18 18:55:44 INFO FileFormatWriter: Job null committed.
20/03/18 18:55:44 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/18 18:55:44 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_18-55-41_183_447768207727802749-1/-ext-10000/ods=2020-03-18/part-00000-26adc6dc-91b6-4966-af1c-42925fdfc5ca.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18/part-00000-26adc6dc-91b6-4966-af1c-42925fdfc5ca.c000, Status:true
20/03/18 18:55:44 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_18-55-41_183_447768207727802749-1/-ext-10000/ods=2020-03-18 with partSpec {ods=2020-03-18}
20/03/18 18:55:44 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 18:55:44 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4049
20/03/18 18:55:44 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 18:55:44 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 18:55:44 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 18:55:44 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 18:55:44 INFO YarnClientSchedulerBackend: Stopped
20/03/18 18:55:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 18:55:44 INFO MemoryStore: MemoryStore cleared
20/03/18 18:55:44 INFO BlockManager: BlockManager stopped
20/03/18 18:55:44 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 18:55:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 18:55:44 INFO SparkContext: Successfully stopped SparkContext
20/03/18 18:55:44 INFO ShutdownHookManager: Shutdown hook called
20/03/18 18:55:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-0eb8f276-5f99-41aa-87db-7595efc483fd
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584528945
+ DIFF_TIME=60
+ echo 'END_TIME: ' 1584528945
+ echo 'Total time taken: ' 60
+ attempt_num=4
+ sleep 30m
+ '[' 4 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584528918
++ date +%s
+ START_TIME=1584530745
+ echo 1584530745
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584528918 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 19:25:46 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 19:25:47 INFO SparkContext: Submitted application: CoolLogtest
20/03/18 19:25:47 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 19:25:47 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 19:25:47 INFO SecurityManager: Changing view acls groups to: 
20/03/18 19:25:47 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 19:25:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 19:25:47 INFO Utils: Successfully started service 'sparkDriver' on port 45588.
20/03/18 19:25:47 INFO SparkEnv: Registering MapOutputTracker
20/03/18 19:25:47 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 19:25:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 19:25:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 19:25:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-faf6add5-5cdc-47be-8ecb-358e351e23d1
20/03/18 19:25:47 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 19:25:48 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 19:25:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/18 19:25:48 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/18 19:25:48 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
20/03/18 19:25:48 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
20/03/18 19:25:48 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
20/03/18 19:25:48 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
20/03/18 19:25:48 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
20/03/18 19:25:48 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
20/03/18 19:25:48 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.
20/03/18 19:25:48 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.
20/03/18 19:25:48 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.
20/03/18 19:25:48 INFO Utils: Successfully started service 'SparkUI' on port 4051.
20/03/18 19:25:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4051
20/03/18 19:25:48 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:45588/jars/CoolPocTest.jar with timestamp 1584530748415
20/03/18 19:25:49 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 19:25:49 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 19:25:49 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 19:25:49 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 19:25:49 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 19:25:49 INFO Client: Setting up container launch context for our AM
20/03/18 19:25:49 INFO Client: Setting up the launch environment for our AM container
20/03/18 19:25:49 INFO Client: Preparing resources for our AM container
20/03/18 19:25:49 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 19:25:49 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7065141 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 19:25:51 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 19:25:51 INFO metastore: Connected to metastore.
20/03/18 19:26:09 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ed 64 c5 65 8a 01 71 11 71 49 65 8e 03 02 8e 02 7a
20/03/18 19:26:09 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 19:26:09 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 19:26:09 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_71332/datanucleus-api-jdo-3.2.6.jar
20/03/18 19:26:09 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_71332/datanucleus-core-3.2.10.jar
20/03/18 19:26:09 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_71332/datanucleus-rdbms-3.2.9.jar
20/03/18 19:26:10 INFO Client: Uploading resource file:/tmp/spark-fa5e1e8c-ca89-4975-a1a2-c0ec76b4f800/__spark_conf__934465467195116270.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_71332/__spark_conf__.zip
20/03/18 19:26:10 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 19:26:10 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 19:26:10 INFO SecurityManager: Changing view acls groups to: 
20/03/18 19:26:10 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 19:26:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 19:26:10 INFO Client: Submitting application application_1583994958990_71332 to ResourceManager
20/03/18 19:26:10 INFO YarnClientImpl: Submitted application application_1583994958990_71332
20/03/18 19:26:10 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_71332 and attemptId None
20/03/18 19:26:11 INFO Client: Application report for application_1583994958990_71332 (state: ACCEPTED)
20/03/18 19:26:11 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584530770580
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_71332/
	 user: 1619795
20/03/18 19:26:12 INFO Client: Application report for application_1583994958990_71332 (state: ACCEPTED)
20/03/18 19:26:13 INFO Client: Application report for application_1583994958990_71332 (state: ACCEPTED)
20/03/18 19:26:14 INFO Client: Application report for application_1583994958990_71332 (state: ACCEPTED)
20/03/18 19:26:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_71332,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_71332), /proxy/application_1583994958990_71332
20/03/18 19:26:15 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 19:26:15 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 19:26:15 INFO Client: Application report for application_1583994958990_71332 (state: RUNNING)
20/03/18 19:26:15 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.142.66
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584530770580
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_71332/
	 user: 1619795
20/03/18 19:26:15 INFO YarnClientSchedulerBackend: Application application_1583994958990_71332 has started running.
20/03/18 19:26:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42840.
20/03/18 19:26:15 INFO NettyBlockTransferService: Server created on 10.20.174.137:42840
20/03/18 19:26:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 19:26:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 42840, None)
20/03/18 19:26:15 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:42840 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 42840, None)
20/03/18 19:26:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 42840, None)
20/03/18 19:26:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 42840, None)
20/03/18 19:26:15 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_71332
20/03/18 19:26:18 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584528918
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584528918
*************************************** CURRENT TIME |1584530778|************************
20/03/18 19:26:18 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/18 19:26:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/18 19:26:19 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/18 19:26:19 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/18 19:26:19 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 19:26:19 INFO metastore: Connected to metastore.
20/03/18 19:26:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.142.69:41254) with ID 1
20/03/18 19:26:21 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas013.global.standardchartered.com:46755 with 912.3 MB RAM, BlockManagerId(1, hklpathas013.global.standardchartered.com, 46755, None)
20/03/18 19:26:39 INFO SessionState: Created local directory: /tmp/6fb849f0-cd98-40a6-86e9-93e0d7f1017f_resources
20/03/18 19:26:39 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/6fb849f0-cd98-40a6-86e9-93e0d7f1017f
20/03/18 19:26:39 INFO SessionState: Created local directory: /tmp/1619795/6fb849f0-cd98-40a6-86e9-93e0d7f1017f
20/03/18 19:26:39 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/6fb849f0-cd98-40a6-86e9-93e0d7f1017f/_tmp_space.db
20/03/18 19:26:39 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 19:26:39 INFO SessionState: Created local directory: /tmp/b61bf817-c6ef-4325-880b-bc12dd4de0bd_resources
20/03/18 19:26:39 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/b61bf817-c6ef-4325-880b-bc12dd4de0bd
20/03/18 19:26:39 INFO SessionState: Created local directory: /tmp/1619795/b61bf817-c6ef-4325-880b-bc12dd4de0bd
20/03/18 19:26:39 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/b61bf817-c6ef-4325-880b-bc12dd4de0bd/_tmp_space.db
20/03/18 19:26:39 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 19:26:40 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/18 19:26:40 INFO CodeGenerator: Code generated in 173.3062 ms
20/03/18 19:26:41 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/18 19:26:41 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/18 19:26:41 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/18 19:26:41 INFO DAGScheduler: Parents of final stage: List()
20/03/18 19:26:41 INFO DAGScheduler: Missing parents: List()
20/03/18 19:26:41 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/18 19:26:41 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 19:26:41 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 19:26:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:42840 (size: 5.0 KB, free: 366.3 MB)
20/03/18 19:26:41 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/18 19:26:41 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 19:26:41 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/18 19:26:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 19:26:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas013.global.standardchartered.com:46755 (size: 5.0 KB, free: 912.3 MB)
20/03/18 19:26:42 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas013.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 19:26:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 849 ms on hklpathas013.global.standardchartered.com (executor 1) (1/4)
20/03/18 19:26:42 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas013.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 19:26:42 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 34 ms on hklpathas013.global.standardchartered.com (executor 1) (2/4)
20/03/18 19:26:42 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas013.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 46575 bytes)
20/03/18 19:26:42 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 24 ms on hklpathas013.global.standardchartered.com (executor 1) (3/4)
20/03/18 19:26:42 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 210 ms on hklpathas013.global.standardchartered.com (executor 1) (4/4)
20/03/18 19:26:42 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 19:26:42 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 1.105 s
20/03/18 19:26:42 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.351988 s
20/03/18 19:26:42 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/18 19:26:42 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/18 19:26:42 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/18 19:26:42 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/18 19:26:42 INFO DAGScheduler: Parents of final stage: List()
20/03/18 19:26:42 INFO DAGScheduler: Missing parents: List()
20/03/18 19:26:42 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/18 19:26:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 19:26:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 19:26:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:42840 (size: 5.0 KB, free: 366.3 MB)
20/03/18 19:26:42 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 19:26:42 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 19:26:42 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/18 19:26:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 19:26:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:42840 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 19:26:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas013.global.standardchartered.com:46755 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 19:26:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas013.global.standardchartered.com:46755 (size: 5.0 KB, free: 912.3 MB)
20/03/18 19:26:42 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas013.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 19:26:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 151 ms on hklpathas013.global.standardchartered.com (executor 1) (1/4)
20/03/18 19:26:42 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas013.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 19:26:42 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 9 ms on hklpathas013.global.standardchartered.com (executor 1) (2/4)
20/03/18 19:26:42 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas013.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 46575 bytes)
20/03/18 19:26:42 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 13 ms on hklpathas013.global.standardchartered.com (executor 1) (3/4)
20/03/18 19:26:42 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 23 ms on hklpathas013.global.standardchartered.com (executor 1) (4/4)
20/03/18 19:26:42 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/18 19:26:42 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.194 s
20/03/18 19:26:42 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.236104 s
20/03/18 19:26:43 INFO ContextCleaner: Cleaned accumulator 0
20/03/18 19:26:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:42840 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 19:26:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas013.global.standardchartered.com:46755 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 19:26:43 INFO ContextCleaner: Cleaned accumulator 51
20/03/18 19:26:43 INFO ContextCleaner: Cleaned accumulator 1
20/03/18 19:26:43 INFO ContextCleaner: Cleaned accumulator 52
20/03/18 19:26:43 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_19-26-43_875_3990351989797737756-1
20/03/18 19:26:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/18 19:26:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/18 19:26:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/18 19:26:44 INFO CodeGenerator: Code generated in 77.580506 ms
20/03/18 19:26:44 INFO CodeGenerator: Code generated in 45.55066 ms
20/03/18 19:26:44 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/18 19:26:44 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/18 19:26:44 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/18 19:26:44 INFO DAGScheduler: Parents of final stage: List()
20/03/18 19:26:44 INFO DAGScheduler: Missing parents: List()
20/03/18 19:26:44 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/18 19:26:44 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/18 19:26:44 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/18 19:26:44 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:42840 (size: 114.9 KB, free: 366.2 MB)
20/03/18 19:26:44 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/18 19:26:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/18 19:26:44 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/18 19:26:44 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 89051 bytes)
20/03/18 19:26:44 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas013.global.standardchartered.com:46755 (size: 114.9 KB, free: 912.2 MB)
20/03/18 19:26:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 9185 ms on hklpathas013.global.standardchartered.com (executor 1) (1/1)
20/03/18 19:26:53 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/18 19:26:53 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 9.186 s
20/03/18 19:26:53 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 9.266888 s
20/03/18 19:26:53 INFO FileFormatWriter: Job null committed.
20/03/18 19:26:53 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/18 19:26:53 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_19-26-43_875_3990351989797737756-1/-ext-10000/ods=2020-03-18/part-00000-71858e7d-0054-4a49-8fd5-7e580c7ed452.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18/part-00000-71858e7d-0054-4a49-8fd5-7e580c7ed452.c000, Status:true
20/03/18 19:26:54 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_19-26-43_875_3990351989797737756-1/-ext-10000/ods=2020-03-18 with partSpec {ods=2020-03-18}
20/03/18 19:26:54 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 19:26:54 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4051
20/03/18 19:26:54 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 19:26:54 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 19:26:54 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 19:26:54 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 19:26:54 INFO YarnClientSchedulerBackend: Stopped
20/03/18 19:26:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 19:26:54 INFO MemoryStore: MemoryStore cleared
20/03/18 19:26:54 INFO BlockManager: BlockManager stopped
20/03/18 19:26:54 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 19:26:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 19:26:54 INFO SparkContext: Successfully stopped SparkContext
20/03/18 19:26:54 INFO ShutdownHookManager: Shutdown hook called
20/03/18 19:26:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-fa5e1e8c-ca89-4975-a1a2-c0ec76b4f800
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584530814
+ DIFF_TIME=69
+ echo 'END_TIME: ' 1584530814
+ echo 'Total time taken: ' 69
+ attempt_num=5
+ sleep 30m
+ '[' 5 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584530778
++ date +%s
+ START_TIME=1584532614
+ echo 1584532614
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584530778 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 19:56:55 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 19:56:56 INFO SparkContext: Submitted application: CoolLogtest
20/03/18 19:56:56 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 19:56:56 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 19:56:56 INFO SecurityManager: Changing view acls groups to: 
20/03/18 19:56:56 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 19:56:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 19:56:56 INFO Utils: Successfully started service 'sparkDriver' on port 43266.
20/03/18 19:56:56 INFO SparkEnv: Registering MapOutputTracker
20/03/18 19:56:56 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 19:56:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 19:56:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 19:56:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cad80176-9670-408d-8c24-311788e37baa
20/03/18 19:56:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 19:56:57 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 19:56:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/18 19:56:57 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/18 19:56:57 INFO Utils: Successfully started service 'SparkUI' on port 4042.
20/03/18 19:56:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4042
20/03/18 19:56:57 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:43266/jars/CoolPocTest.jar with timestamp 1584532617386
20/03/18 19:56:58 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 19:56:58 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 19:56:58 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 19:56:58 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 19:56:58 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 19:56:58 INFO Client: Setting up container launch context for our AM
20/03/18 19:56:58 INFO Client: Setting up the launch environment for our AM container
20/03/18 19:56:58 INFO Client: Preparing resources for our AM container
20/03/18 19:56:58 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 19:56:58 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7065999 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 19:56:59 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 19:56:59 INFO metastore: Connected to metastore.
20/03/18 19:57:17 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ed 81 45 53 8a 01 71 11 8d c9 53 8e 03 64 8e 02 7a
20/03/18 19:57:17 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 19:57:17 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 19:57:17 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_71793/datanucleus-api-jdo-3.2.6.jar
20/03/18 19:57:17 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_71793/datanucleus-core-3.2.10.jar
20/03/18 19:57:17 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_71793/datanucleus-rdbms-3.2.9.jar
20/03/18 19:57:17 INFO Client: Uploading resource file:/tmp/spark-d73b8c91-caf9-4978-952e-9a9a9d04106a/__spark_conf__3426143406240666871.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_71793/__spark_conf__.zip
20/03/18 19:57:17 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 19:57:17 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 19:57:17 INFO SecurityManager: Changing view acls groups to: 
20/03/18 19:57:17 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 19:57:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 19:57:17 INFO Client: Submitting application application_1583994958990_71793 to ResourceManager
20/03/18 19:57:17 INFO YarnClientImpl: Submitted application application_1583994958990_71793
20/03/18 19:57:17 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_71793 and attemptId None
20/03/18 19:57:18 INFO Client: Application report for application_1583994958990_71793 (state: ACCEPTED)
20/03/18 19:57:18 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584532637706
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_71793/
	 user: 1619795
20/03/18 19:57:19 INFO Client: Application report for application_1583994958990_71793 (state: ACCEPTED)
20/03/18 19:57:20 INFO Client: Application report for application_1583994958990_71793 (state: ACCEPTED)
20/03/18 19:57:21 INFO Client: Application report for application_1583994958990_71793 (state: ACCEPTED)
20/03/18 19:57:22 INFO Client: Application report for application_1583994958990_71793 (state: ACCEPTED)
20/03/18 19:57:23 INFO Client: Application report for application_1583994958990_71793 (state: ACCEPTED)
20/03/18 19:57:24 INFO Client: Application report for application_1583994958990_71793 (state: ACCEPTED)
20/03/18 19:57:25 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_71793,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_71793), /proxy/application_1583994958990_71793
20/03/18 19:57:25 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 19:57:25 INFO Client: Application report for application_1583994958990_71793 (state: ACCEPTED)
20/03/18 19:57:25 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 19:57:26 INFO Client: Application report for application_1583994958990_71793 (state: RUNNING)
20/03/18 19:57:26 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.20.174.138
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584532637706
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_71793/
	 user: 1619795
20/03/18 19:57:26 INFO YarnClientSchedulerBackend: Application application_1583994958990_71793 has started running.
20/03/18 19:57:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43736.
20/03/18 19:57:26 INFO NettyBlockTransferService: Server created on 10.20.174.137:43736
20/03/18 19:57:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 19:57:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 43736, None)
20/03/18 19:57:26 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:43736 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 43736, None)
20/03/18 19:57:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 43736, None)
20/03/18 19:57:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 43736, None)
20/03/18 19:57:27 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_71793
20/03/18 19:57:27 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584530778
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584530778
*************************************** CURRENT TIME |1584532647|************************
20/03/18 19:57:27 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/18 19:57:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/18 19:57:27 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/18 19:57:28 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/18 19:57:28 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 19:57:28 INFO metastore: Connected to metastore.
20/03/18 19:57:31 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.142.96:44416) with ID 1
20/03/18 19:57:31 INFO BlockManagerMasterEndpoint: Registering block manager hklpadhaa012.global.standardchartered.com:40000 with 912.3 MB RAM, BlockManagerId(1, hklpadhaa012.global.standardchartered.com, 40000, None)
20/03/18 19:57:46 INFO SessionState: Created local directory: /tmp/39404ea1-f6ec-4ce5-8c8a-e1ab479f3ebc_resources
20/03/18 19:57:46 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/39404ea1-f6ec-4ce5-8c8a-e1ab479f3ebc
20/03/18 19:57:46 INFO SessionState: Created local directory: /tmp/1619795/39404ea1-f6ec-4ce5-8c8a-e1ab479f3ebc
20/03/18 19:57:46 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/39404ea1-f6ec-4ce5-8c8a-e1ab479f3ebc/_tmp_space.db
20/03/18 19:57:46 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 19:57:46 INFO SessionState: Created local directory: /tmp/ce0b24da-0ffd-4bc4-834d-b893fae4fe87_resources
20/03/18 19:57:46 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/ce0b24da-0ffd-4bc4-834d-b893fae4fe87
20/03/18 19:57:46 INFO SessionState: Created local directory: /tmp/1619795/ce0b24da-0ffd-4bc4-834d-b893fae4fe87
20/03/18 19:57:46 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/ce0b24da-0ffd-4bc4-834d-b893fae4fe87/_tmp_space.db
20/03/18 19:57:46 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 19:57:46 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/18 19:57:47 INFO CodeGenerator: Code generated in 170.479893 ms
20/03/18 19:57:47 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/18 19:57:47 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/18 19:57:47 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/18 19:57:47 INFO DAGScheduler: Parents of final stage: List()
20/03/18 19:57:47 INFO DAGScheduler: Missing parents: List()
20/03/18 19:57:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/18 19:57:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 19:57:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 19:57:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:43736 (size: 5.0 KB, free: 366.3 MB)
20/03/18 19:57:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/18 19:57:48 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 19:57:48 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/18 19:57:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpadhaa012.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 19:57:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpadhaa012.global.standardchartered.com:40000 (size: 5.0 KB, free: 912.3 MB)
20/03/18 19:57:48 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpadhaa012.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 19:57:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 785 ms on hklpadhaa012.global.standardchartered.com (executor 1) (1/4)
20/03/18 19:57:48 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpadhaa012.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 19:57:48 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 22 ms on hklpadhaa012.global.standardchartered.com (executor 1) (2/4)
20/03/18 19:57:48 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpadhaa012.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 48630 bytes)
20/03/18 19:57:48 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 13 ms on hklpadhaa012.global.standardchartered.com (executor 1) (3/4)
20/03/18 19:57:49 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 139 ms on hklpadhaa012.global.standardchartered.com (executor 1) (4/4)
20/03/18 19:57:49 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 19:57:49 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.956 s
20/03/18 19:57:49 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.200708 s
20/03/18 19:57:49 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/18 19:57:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:43736 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 19:57:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpadhaa012.global.standardchartered.com:40000 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 19:57:49 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/18 19:57:49 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/18 19:57:49 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/18 19:57:49 INFO DAGScheduler: Parents of final stage: List()
20/03/18 19:57:49 INFO DAGScheduler: Missing parents: List()
20/03/18 19:57:49 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/18 19:57:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 19:57:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 19:57:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:43736 (size: 5.0 KB, free: 366.3 MB)
20/03/18 19:57:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 19:57:49 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 19:57:49 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/18 19:57:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpadhaa012.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 19:57:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpadhaa012.global.standardchartered.com:40000 (size: 5.0 KB, free: 912.3 MB)
20/03/18 19:57:49 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpadhaa012.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 19:57:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 156 ms on hklpadhaa012.global.standardchartered.com (executor 1) (1/4)
20/03/18 19:57:49 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpadhaa012.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 19:57:49 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 7 ms on hklpadhaa012.global.standardchartered.com (executor 1) (2/4)
20/03/18 19:57:49 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpadhaa012.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 48630 bytes)
20/03/18 19:57:49 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpadhaa012.global.standardchartered.com (executor 1) (3/4)
20/03/18 19:57:49 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 15 ms on hklpadhaa012.global.standardchartered.com (executor 1) (4/4)
20/03/18 19:57:49 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/18 19:57:49 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.185 s
20/03/18 19:57:49 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.192397 s
20/03/18 19:57:50 INFO ContextCleaner: Cleaned accumulator 1
20/03/18 19:57:50 INFO ContextCleaner: Cleaned accumulator 0
20/03/18 19:57:50 INFO ContextCleaner: Cleaned accumulator 52
20/03/18 19:57:50 INFO ContextCleaner: Cleaned accumulator 51
20/03/18 19:57:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:43736 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 19:57:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpadhaa012.global.standardchartered.com:40000 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 19:57:50 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_19-57-50_324_4408442486852040050-1
20/03/18 19:57:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/18 19:57:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/18 19:57:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/18 19:57:50 INFO CodeGenerator: Code generated in 61.071541 ms
20/03/18 19:57:50 INFO CodeGenerator: Code generated in 27.106007 ms
20/03/18 19:57:50 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/18 19:57:50 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/18 19:57:50 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/18 19:57:50 INFO DAGScheduler: Parents of final stage: List()
20/03/18 19:57:50 INFO DAGScheduler: Missing parents: List()
20/03/18 19:57:50 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/18 19:57:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 309.2 KB, free 366.0 MB)
20/03/18 19:57:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 110.2 KB, free 365.9 MB)
20/03/18 19:57:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:43736 (size: 110.2 KB, free: 366.2 MB)
20/03/18 19:57:51 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/18 19:57:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/18 19:57:51 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/18 19:57:51 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpadhaa012.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 93161 bytes)
20/03/18 19:57:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpadhaa012.global.standardchartered.com:40000 (size: 110.2 KB, free: 912.2 MB)
20/03/18 19:57:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2004 ms on hklpadhaa012.global.standardchartered.com (executor 1) (1/1)
20/03/18 19:57:53 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/18 19:57:53 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.005 s
20/03/18 19:57:53 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.084305 s
20/03/18 19:57:53 INFO FileFormatWriter: Job null committed.
20/03/18 19:57:53 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/18 19:57:53 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_19-57-50_324_4408442486852040050-1/-ext-10000/ods=2020-03-18/part-00000-cee77f12-9514-409b-99dd-2eebfefcdda0.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18/part-00000-cee77f12-9514-409b-99dd-2eebfefcdda0.c000, Status:true
20/03/18 19:57:53 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_19-57-50_324_4408442486852040050-1/-ext-10000/ods=2020-03-18 with partSpec {ods=2020-03-18}
20/03/18 19:57:53 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 19:57:53 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4042
20/03/18 19:57:53 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 19:57:53 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 19:57:53 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 19:57:53 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 19:57:53 INFO YarnClientSchedulerBackend: Stopped
20/03/18 19:57:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 19:57:53 INFO MemoryStore: MemoryStore cleared
20/03/18 19:57:53 INFO BlockManager: BlockManager stopped
20/03/18 19:57:53 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 19:57:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 19:57:53 INFO SparkContext: Successfully stopped SparkContext
20/03/18 19:57:53 INFO ShutdownHookManager: Shutdown hook called
20/03/18 19:57:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-d73b8c91-caf9-4978-952e-9a9a9d04106a
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584532674
+ DIFF_TIME=60
+ echo 'END_TIME: ' 1584532674
+ echo 'Total time taken: ' 60
+ attempt_num=6
+ sleep 30m
+ '[' 6 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584532647
++ date +%s
+ START_TIME=1584534474
+ echo 1584534474
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584532647 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 20:27:55 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 20:27:55 INFO SparkContext: Submitted application: CoolLogtest
20/03/18 20:27:56 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 20:27:56 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 20:27:56 INFO SecurityManager: Changing view acls groups to: 
20/03/18 20:27:56 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 20:27:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 20:27:56 INFO Utils: Successfully started service 'sparkDriver' on port 38490.
20/03/18 20:27:56 INFO SparkEnv: Registering MapOutputTracker
20/03/18 20:27:56 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 20:27:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 20:27:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 20:27:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-65f25926-7a24-4f1f-95c9-de9c0ec71b19
20/03/18 20:27:56 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 20:27:56 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 20:27:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/18 20:27:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/18 20:27:56 INFO Utils: Successfully started service 'SparkUI' on port 4042.
20/03/18 20:27:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4042
20/03/18 20:27:56 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:38490/jars/CoolPocTest.jar with timestamp 1584534476786
20/03/18 20:27:57 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 20:27:57 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 20:27:57 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 20:27:58 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 20:27:58 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 20:27:58 INFO Client: Setting up container launch context for our AM
20/03/18 20:27:58 INFO Client: Setting up the launch environment for our AM container
20/03/18 20:27:58 INFO Client: Preparing resources for our AM container
20/03/18 20:27:58 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 20:27:58 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7067638 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 20:27:59 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 20:27:59 INFO metastore: Connected to metastore.
20/03/18 20:28:15 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ed 9d a0 bd 8a 01 71 11 aa 24 bd 8e 03 87 8e 02 7a
20/03/18 20:28:15 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 20:28:15 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 20:28:15 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_72087/datanucleus-api-jdo-3.2.6.jar
20/03/18 20:28:15 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_72087/datanucleus-core-3.2.10.jar
20/03/18 20:28:15 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_72087/datanucleus-rdbms-3.2.9.jar
20/03/18 20:28:16 INFO Client: Uploading resource file:/tmp/spark-62b71a7b-e7b3-4483-9481-3c33f1dd1901/__spark_conf__3692887352389371313.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_72087/__spark_conf__.zip
20/03/18 20:28:16 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 20:28:16 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 20:28:16 INFO SecurityManager: Changing view acls groups to: 
20/03/18 20:28:16 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 20:28:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 20:28:16 INFO Client: Submitting application application_1583994958990_72087 to ResourceManager
20/03/18 20:28:16 INFO YarnClientImpl: Submitted application application_1583994958990_72087
20/03/18 20:28:16 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_72087 and attemptId None
20/03/18 20:28:17 INFO Client: Application report for application_1583994958990_72087 (state: ACCEPTED)
20/03/18 20:28:17 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584534496274
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_72087/
	 user: 1619795
20/03/18 20:28:18 INFO Client: Application report for application_1583994958990_72087 (state: ACCEPTED)
20/03/18 20:28:19 INFO Client: Application report for application_1583994958990_72087 (state: ACCEPTED)
20/03/18 20:28:20 INFO Client: Application report for application_1583994958990_72087 (state: ACCEPTED)
20/03/18 20:28:21 INFO Client: Application report for application_1583994958990_72087 (state: ACCEPTED)
20/03/18 20:28:22 INFO Client: Application report for application_1583994958990_72087 (state: ACCEPTED)
20/03/18 20:28:23 INFO Client: Application report for application_1583994958990_72087 (state: ACCEPTED)
20/03/18 20:28:24 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_72087,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_72087), /proxy/application_1583994958990_72087
20/03/18 20:28:24 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 20:28:24 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 20:28:24 INFO Client: Application report for application_1583994958990_72087 (state: RUNNING)
20/03/18 20:28:24 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.20.174.138
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584534496274
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_72087/
	 user: 1619795
20/03/18 20:28:24 INFO YarnClientSchedulerBackend: Application application_1583994958990_72087 has started running.
20/03/18 20:28:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35405.
20/03/18 20:28:24 INFO NettyBlockTransferService: Server created on 10.20.174.137:35405
20/03/18 20:28:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 20:28:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 35405, None)
20/03/18 20:28:24 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:35405 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 35405, None)
20/03/18 20:28:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 35405, None)
20/03/18 20:28:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 35405, None)
20/03/18 20:28:24 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_72087
20/03/18 20:28:26 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584532647
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584532647
*************************************** CURRENT TIME |1584534507|************************
20/03/18 20:28:27 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/18 20:28:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/18 20:28:27 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/18 20:28:27 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/18 20:28:27 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 20:28:27 INFO metastore: Connected to metastore.
20/03/18 20:28:29 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.142.69:41634) with ID 1
20/03/18 20:28:29 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas013.global.standardchartered.com:35705 with 912.3 MB RAM, BlockManagerId(1, hklpathas013.global.standardchartered.com, 35705, None)
20/03/18 20:28:45 INFO SessionState: Created local directory: /tmp/f69c9d7d-5992-485f-8810-4100a0542026_resources
20/03/18 20:28:45 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/f69c9d7d-5992-485f-8810-4100a0542026
20/03/18 20:28:45 INFO SessionState: Created local directory: /tmp/1619795/f69c9d7d-5992-485f-8810-4100a0542026
20/03/18 20:28:45 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/f69c9d7d-5992-485f-8810-4100a0542026/_tmp_space.db
20/03/18 20:28:45 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 20:28:46 INFO SessionState: Created local directory: /tmp/80a5d49e-02e5-409c-9b99-5fe0e0331f1c_resources
20/03/18 20:28:46 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/80a5d49e-02e5-409c-9b99-5fe0e0331f1c
20/03/18 20:28:46 INFO SessionState: Created local directory: /tmp/1619795/80a5d49e-02e5-409c-9b99-5fe0e0331f1c
20/03/18 20:28:46 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/80a5d49e-02e5-409c-9b99-5fe0e0331f1c/_tmp_space.db
20/03/18 20:28:46 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 20:28:46 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/18 20:28:47 INFO CodeGenerator: Code generated in 169.250561 ms
20/03/18 20:28:47 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/18 20:28:47 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/18 20:28:47 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/18 20:28:47 INFO DAGScheduler: Parents of final stage: List()
20/03/18 20:28:47 INFO DAGScheduler: Missing parents: List()
20/03/18 20:28:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/18 20:28:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 20:28:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 20:28:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:35405 (size: 5.0 KB, free: 366.3 MB)
20/03/18 20:28:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/18 20:28:47 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 20:28:47 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/18 20:28:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 20:28:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas013.global.standardchartered.com:35705 (size: 5.0 KB, free: 912.3 MB)
20/03/18 20:28:48 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas013.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 20:28:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 617 ms on hklpathas013.global.standardchartered.com (executor 1) (1/4)
20/03/18 20:28:48 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas013.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 20:28:48 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 18 ms on hklpathas013.global.standardchartered.com (executor 1) (2/4)
20/03/18 20:28:48 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas013.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 51965 bytes)
20/03/18 20:28:48 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 12 ms on hklpathas013.global.standardchartered.com (executor 1) (3/4)
20/03/18 20:28:48 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 90 ms on hklpathas013.global.standardchartered.com (executor 1) (4/4)
20/03/18 20:28:48 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 20:28:48 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.732 s
20/03/18 20:28:48 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 0.906726 s
20/03/18 20:28:48 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/18 20:28:48 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/18 20:28:48 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/18 20:28:48 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/18 20:28:48 INFO DAGScheduler: Parents of final stage: List()
20/03/18 20:28:48 INFO DAGScheduler: Missing parents: List()
20/03/18 20:28:48 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/18 20:28:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 20:28:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 20:28:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:35405 (size: 5.0 KB, free: 366.3 MB)
20/03/18 20:28:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 20:28:48 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 20:28:48 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/18 20:28:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 20:28:48 INFO ContextCleaner: Cleaned accumulator 1
20/03/18 20:28:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:35405 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 20:28:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas013.global.standardchartered.com:35705 (size: 5.0 KB, free: 912.3 MB)
20/03/18 20:28:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas013.global.standardchartered.com:35705 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 20:28:48 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas013.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 20:28:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 162 ms on hklpathas013.global.standardchartered.com (executor 1) (1/4)
20/03/18 20:28:48 INFO ContextCleaner: Cleaned accumulator 0
20/03/18 20:28:48 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas013.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 20:28:48 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 11 ms on hklpathas013.global.standardchartered.com (executor 1) (2/4)
20/03/18 20:28:48 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas013.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 51965 bytes)
20/03/18 20:28:48 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpathas013.global.standardchartered.com (executor 1) (3/4)
20/03/18 20:28:48 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 14 ms on hklpathas013.global.standardchartered.com (executor 1) (4/4)
20/03/18 20:28:48 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/18 20:28:48 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.235 s
20/03/18 20:28:48 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.241829 s
20/03/18 20:28:49 INFO ContextCleaner: Cleaned accumulator 52
20/03/18 20:28:49 INFO ContextCleaner: Cleaned accumulator 51
20/03/18 20:28:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:35405 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 20:28:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas013.global.standardchartered.com:35705 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 20:28:49 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_20-28-49_363_1691719483255493805-1
20/03/18 20:28:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/18 20:28:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/18 20:28:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/18 20:28:49 INFO CodeGenerator: Code generated in 59.260754 ms
20/03/18 20:28:49 INFO CodeGenerator: Code generated in 28.398381 ms
20/03/18 20:28:49 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/18 20:28:49 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/18 20:28:49 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/18 20:28:49 INFO DAGScheduler: Parents of final stage: List()
20/03/18 20:28:49 INFO DAGScheduler: Missing parents: List()
20/03/18 20:28:49 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/18 20:28:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 309.2 KB, free 366.0 MB)
20/03/18 20:28:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 110.2 KB, free 365.9 MB)
20/03/18 20:28:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:35405 (size: 110.2 KB, free: 366.2 MB)
20/03/18 20:28:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/18 20:28:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/18 20:28:49 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/18 20:28:49 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 99831 bytes)
20/03/18 20:28:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas013.global.standardchartered.com:35705 (size: 110.2 KB, free: 912.2 MB)
20/03/18 20:28:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 1858 ms on hklpathas013.global.standardchartered.com (executor 1) (1/1)
20/03/18 20:28:51 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/18 20:28:51 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 1.859 s
20/03/18 20:28:51 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 1.934349 s
20/03/18 20:28:51 INFO FileFormatWriter: Job null committed.
20/03/18 20:28:51 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/18 20:28:51 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_20-28-49_363_1691719483255493805-1/-ext-10000/ods=2020-03-18/part-00000-d51162d0-823c-4fc7-9e19-eae997352ed6.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18/part-00000-d51162d0-823c-4fc7-9e19-eae997352ed6.c000, Status:true
20/03/18 20:28:52 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_20-28-49_363_1691719483255493805-1/-ext-10000/ods=2020-03-18 with partSpec {ods=2020-03-18}
20/03/18 20:28:52 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 20:28:52 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4042
20/03/18 20:28:52 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 20:28:52 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 20:28:52 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 20:28:52 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 20:28:52 INFO YarnClientSchedulerBackend: Stopped
20/03/18 20:28:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 20:28:52 INFO MemoryStore: MemoryStore cleared
20/03/18 20:28:52 INFO BlockManager: BlockManager stopped
20/03/18 20:28:52 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 20:28:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 20:28:52 INFO SparkContext: Successfully stopped SparkContext
20/03/18 20:28:52 INFO ShutdownHookManager: Shutdown hook called
20/03/18 20:28:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-62b71a7b-e7b3-4483-9481-3c33f1dd1901
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584534532
+ DIFF_TIME=58
+ echo 'END_TIME: ' 1584534532
+ echo 'Total time taken: ' 58
+ attempt_num=7
+ sleep 30m
+ '[' 7 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584534507
++ date +%s
+ START_TIME=1584536332
+ echo 1584536332
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584534507 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 20:58:53 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 20:58:54 INFO SparkContext: Submitted application: CoolLogtest
20/03/18 20:58:54 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 20:58:54 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 20:58:54 INFO SecurityManager: Changing view acls groups to: 
20/03/18 20:58:54 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 20:58:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 20:58:55 INFO Utils: Successfully started service 'sparkDriver' on port 42130.
20/03/18 20:58:55 INFO SparkEnv: Registering MapOutputTracker
20/03/18 20:58:55 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 20:58:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 20:58:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 20:58:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8b6b496d-8920-4f1b-b8cd-b14d15d993ea
20/03/18 20:58:55 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 20:58:55 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 20:58:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/18 20:58:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/18 20:58:55 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
20/03/18 20:58:55 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
20/03/18 20:58:55 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
20/03/18 20:58:55 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
20/03/18 20:58:55 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
20/03/18 20:58:55 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
20/03/18 20:58:55 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.
20/03/18 20:58:55 INFO Utils: Successfully started service 'SparkUI' on port 4049.
20/03/18 20:58:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4049
20/03/18 20:58:55 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:42130/jars/CoolPocTest.jar with timestamp 1584536335650
20/03/18 20:58:56 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 20:58:56 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 20:58:56 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 20:58:57 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 20:58:57 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 20:58:57 INFO Client: Setting up container launch context for our AM
20/03/18 20:58:57 INFO Client: Setting up the launch environment for our AM container
20/03/18 20:58:57 INFO Client: Preparing resources for our AM container
20/03/18 20:58:57 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 20:58:57 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7068539 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 20:58:58 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 20:58:58 INFO metastore: Connected to metastore.
20/03/18 20:59:15 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ed ba 02 19 8a 01 71 11 c6 86 19 8e 03 9c 8e 02 7a
20/03/18 20:59:15 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 20:59:15 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 20:59:15 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_72395/datanucleus-api-jdo-3.2.6.jar
20/03/18 20:59:15 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_72395/datanucleus-core-3.2.10.jar
20/03/18 20:59:15 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_72395/datanucleus-rdbms-3.2.9.jar
20/03/18 20:59:15 INFO Client: Uploading resource file:/tmp/spark-fb6e9b48-7932-4233-b52c-36699ba966d9/__spark_conf__8317064561209513089.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_72395/__spark_conf__.zip
20/03/18 20:59:16 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 20:59:16 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 20:59:16 INFO SecurityManager: Changing view acls groups to: 
20/03/18 20:59:16 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 20:59:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 20:59:16 INFO Client: Submitting application application_1583994958990_72395 to ResourceManager
20/03/18 20:59:16 INFO YarnClientImpl: Submitted application application_1583994958990_72395
20/03/18 20:59:16 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_72395 and attemptId None
20/03/18 20:59:17 INFO Client: Application report for application_1583994958990_72395 (state: ACCEPTED)
20/03/18 20:59:17 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584536356084
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_72395/
	 user: 1619795
20/03/18 20:59:18 INFO Client: Application report for application_1583994958990_72395 (state: ACCEPTED)
20/03/18 20:59:19 INFO Client: Application report for application_1583994958990_72395 (state: ACCEPTED)
20/03/18 20:59:20 INFO Client: Application report for application_1583994958990_72395 (state: ACCEPTED)
20/03/18 20:59:21 INFO Client: Application report for application_1583994958990_72395 (state: ACCEPTED)
20/03/18 20:59:22 INFO Client: Application report for application_1583994958990_72395 (state: ACCEPTED)
20/03/18 20:59:23 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_72395,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_72395), /proxy/application_1583994958990_72395
20/03/18 20:59:23 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 20:59:23 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 20:59:23 INFO Client: Application report for application_1583994958990_72395 (state: RUNNING)
20/03/18 20:59:23 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.142.96
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584536356084
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_72395/
	 user: 1619795
20/03/18 20:59:23 INFO YarnClientSchedulerBackend: Application application_1583994958990_72395 has started running.
20/03/18 20:59:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35397.
20/03/18 20:59:23 INFO NettyBlockTransferService: Server created on 10.20.174.137:35397
20/03/18 20:59:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 20:59:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 35397, None)
20/03/18 20:59:23 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:35397 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 35397, None)
20/03/18 20:59:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 35397, None)
20/03/18 20:59:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 35397, None)
20/03/18 20:59:23 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_72395
20/03/18 20:59:25 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584534507
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584534507
*************************************** CURRENT TIME |1584536366|************************
20/03/18 20:59:26 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/18 20:59:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/18 20:59:26 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/18 20:59:26 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/18 20:59:27 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 20:59:27 INFO metastore: Connected to metastore.
20/03/18 20:59:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.23:32898) with ID 1
20/03/18 20:59:28 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas016.global.standardchartered.com:41329 with 912.3 MB RAM, BlockManagerId(1, hklpathas016.global.standardchartered.com, 41329, None)
20/03/18 20:59:43 INFO SessionState: Created local directory: /tmp/fc56059b-a077-4e6d-8d74-6a85826c464e_resources
20/03/18 20:59:43 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/fc56059b-a077-4e6d-8d74-6a85826c464e
20/03/18 20:59:43 INFO SessionState: Created local directory: /tmp/1619795/fc56059b-a077-4e6d-8d74-6a85826c464e
20/03/18 20:59:43 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/fc56059b-a077-4e6d-8d74-6a85826c464e/_tmp_space.db
20/03/18 20:59:43 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 20:59:44 INFO SessionState: Created local directory: /tmp/066f2f6e-de3d-4653-ae3b-b183f0a4810d_resources
20/03/18 20:59:44 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/066f2f6e-de3d-4653-ae3b-b183f0a4810d
20/03/18 20:59:44 INFO SessionState: Created local directory: /tmp/1619795/066f2f6e-de3d-4653-ae3b-b183f0a4810d
20/03/18 20:59:44 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/066f2f6e-de3d-4653-ae3b-b183f0a4810d/_tmp_space.db
20/03/18 20:59:44 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 20:59:44 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/18 20:59:45 INFO CodeGenerator: Code generated in 160.388823 ms
20/03/18 20:59:45 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/18 20:59:45 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/18 20:59:45 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/18 20:59:45 INFO DAGScheduler: Parents of final stage: List()
20/03/18 20:59:45 INFO DAGScheduler: Missing parents: List()
20/03/18 20:59:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/18 20:59:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 20:59:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 20:59:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:35397 (size: 5.0 KB, free: 366.3 MB)
20/03/18 20:59:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/18 20:59:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 20:59:45 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/18 20:59:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas016.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 20:59:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas016.global.standardchartered.com:41329 (size: 5.0 KB, free: 912.3 MB)
20/03/18 20:59:46 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas016.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 20:59:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 647 ms on hklpathas016.global.standardchartered.com (executor 1) (1/4)
20/03/18 20:59:46 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas016.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 20:59:46 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 21 ms on hklpathas016.global.standardchartered.com (executor 1) (2/4)
20/03/18 20:59:46 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas016.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 58901 bytes)
20/03/18 20:59:46 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 14 ms on hklpathas016.global.standardchartered.com (executor 1) (3/4)
20/03/18 20:59:46 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 110 ms on hklpathas016.global.standardchartered.com (executor 1) (4/4)
20/03/18 20:59:46 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 20:59:46 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.786 s
20/03/18 20:59:46 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 0.954635 s
20/03/18 20:59:46 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/18 20:59:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:35397 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 20:59:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas016.global.standardchartered.com:41329 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 20:59:46 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/18 20:59:46 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/18 20:59:46 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/18 20:59:46 INFO DAGScheduler: Parents of final stage: List()
20/03/18 20:59:46 INFO DAGScheduler: Missing parents: List()
20/03/18 20:59:46 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/18 20:59:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 20:59:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 20:59:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:35397 (size: 5.0 KB, free: 366.3 MB)
20/03/18 20:59:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 20:59:46 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 20:59:46 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/18 20:59:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas016.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 20:59:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas016.global.standardchartered.com:41329 (size: 5.0 KB, free: 912.3 MB)
20/03/18 20:59:46 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas016.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 20:59:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 107 ms on hklpathas016.global.standardchartered.com (executor 1) (1/4)
20/03/18 20:59:46 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas016.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 20:59:46 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 9 ms on hklpathas016.global.standardchartered.com (executor 1) (2/4)
20/03/18 20:59:46 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas016.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 58901 bytes)
20/03/18 20:59:46 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 8 ms on hklpathas016.global.standardchartered.com (executor 1) (3/4)
20/03/18 20:59:46 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 19 ms on hklpathas016.global.standardchartered.com (executor 1) (4/4)
20/03/18 20:59:46 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/18 20:59:46 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.142 s
20/03/18 20:59:46 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.151012 s
20/03/18 20:59:47 INFO ContextCleaner: Cleaned accumulator 51
20/03/18 20:59:47 INFO ContextCleaner: Cleaned accumulator 0
20/03/18 20:59:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:35397 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 20:59:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas016.global.standardchartered.com:41329 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 20:59:47 INFO ContextCleaner: Cleaned accumulator 52
20/03/18 20:59:47 INFO ContextCleaner: Cleaned accumulator 1
20/03/18 20:59:47 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_20-59-47_579_7675556727462563486-1
20/03/18 20:59:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/18 20:59:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/18 20:59:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/18 20:59:47 INFO CodeGenerator: Code generated in 72.983344 ms
20/03/18 20:59:47 INFO CodeGenerator: Code generated in 45.599847 ms
20/03/18 20:59:48 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/18 20:59:48 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/18 20:59:48 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/18 20:59:48 INFO DAGScheduler: Parents of final stage: List()
20/03/18 20:59:48 INFO DAGScheduler: Missing parents: List()
20/03/18 20:59:48 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/18 20:59:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/18 20:59:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/18 20:59:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:35397 (size: 114.9 KB, free: 366.2 MB)
20/03/18 20:59:48 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/18 20:59:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/18 20:59:48 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/18 20:59:48 WARN TaskSetManager: Stage 2 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.
20/03/18 20:59:48 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas016.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 113703 bytes)
20/03/18 20:59:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas016.global.standardchartered.com:41329 (size: 114.9 KB, free: 912.2 MB)
20/03/18 20:59:50 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2393 ms on hklpathas016.global.standardchartered.com (executor 1) (1/1)
20/03/18 20:59:50 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/18 20:59:50 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.393 s
20/03/18 20:59:50 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.480678 s
20/03/18 20:59:50 INFO FileFormatWriter: Job null committed.
20/03/18 20:59:50 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/18 20:59:50 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_20-59-47_579_7675556727462563486-1/-ext-10000/ods=2020-03-18/part-00000-f1dbc1ba-815d-4d2a-8441-972f4a8be2b6.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18/part-00000-f1dbc1ba-815d-4d2a-8441-972f4a8be2b6.c000, Status:true
20/03/18 20:59:50 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_20-59-47_579_7675556727462563486-1/-ext-10000/ods=2020-03-18 with partSpec {ods=2020-03-18}
20/03/18 20:59:51 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 20:59:51 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4049
20/03/18 20:59:51 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 20:59:51 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 20:59:51 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 20:59:51 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 20:59:51 INFO YarnClientSchedulerBackend: Stopped
20/03/18 20:59:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 20:59:51 INFO MemoryStore: MemoryStore cleared
20/03/18 20:59:51 INFO BlockManager: BlockManager stopped
20/03/18 20:59:51 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 20:59:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 20:59:51 INFO SparkContext: Successfully stopped SparkContext
20/03/18 20:59:51 INFO ShutdownHookManager: Shutdown hook called
20/03/18 20:59:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-fb6e9b48-7932-4233-b52c-36699ba966d9
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584536391
+ DIFF_TIME=59
+ echo 'END_TIME: ' 1584536391
+ echo 'Total time taken: ' 59
+ attempt_num=8
+ sleep 30m
+ '[' 8 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584536366
++ date +%s
+ START_TIME=1584538191
+ echo 1584538191
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584536366 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 21:29:52 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 21:29:53 INFO SparkContext: Submitted application: CoolLogtest
20/03/18 21:29:53 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 21:29:53 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 21:29:53 INFO SecurityManager: Changing view acls groups to: 
20/03/18 21:29:53 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 21:29:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 21:29:53 INFO Utils: Successfully started service 'sparkDriver' on port 33747.
20/03/18 21:29:54 INFO SparkEnv: Registering MapOutputTracker
20/03/18 21:29:54 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 21:29:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 21:29:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 21:29:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-aa573e06-205d-4043-956a-2bd879abb7fe
20/03/18 21:29:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 21:29:54 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 21:29:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/18 21:29:54 INFO Utils: Successfully started service 'SparkUI' on port 4041.
20/03/18 21:29:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4041
20/03/18 21:29:54 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:33747/jars/CoolPocTest.jar with timestamp 1584538194518
20/03/18 21:29:55 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 21:29:55 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 21:29:55 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 21:29:55 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 21:29:55 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 21:29:55 INFO Client: Setting up container launch context for our AM
20/03/18 21:29:55 INFO Client: Setting up the launch environment for our AM container
20/03/18 21:29:55 INFO Client: Preparing resources for our AM container
20/03/18 21:29:55 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 21:29:56 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7069460 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 21:29:57 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 21:29:57 INFO metastore: Connected to metastore.
20/03/18 21:30:14 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ed d6 61 8c 8a 01 71 11 e2 e5 8c 8e 03 b7 8e 02 7a
20/03/18 21:30:14 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 21:30:14 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 21:30:14 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_72748/datanucleus-api-jdo-3.2.6.jar
20/03/18 21:30:15 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_72748/datanucleus-core-3.2.10.jar
20/03/18 21:30:15 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_72748/datanucleus-rdbms-3.2.9.jar
20/03/18 21:30:15 INFO Client: Uploading resource file:/tmp/spark-2aae37ac-a4ca-4111-a716-2191a472d92a/__spark_conf__1868207570341167976.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_72748/__spark_conf__.zip
20/03/18 21:30:15 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 21:30:15 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 21:30:15 INFO SecurityManager: Changing view acls groups to: 
20/03/18 21:30:15 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 21:30:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 21:30:15 INFO Client: Submitting application application_1583994958990_72748 to ResourceManager
20/03/18 21:30:15 INFO YarnClientImpl: Submitted application application_1583994958990_72748
20/03/18 21:30:15 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_72748 and attemptId None
20/03/18 21:30:16 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:16 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584538215611
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_72748/
	 user: 1619795
20/03/18 21:30:17 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:18 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:19 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:20 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:21 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:22 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:23 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:24 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:25 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:26 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:27 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:28 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:29 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:30 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:31 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:32 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:33 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:34 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:35 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:36 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:37 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:38 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:39 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:40 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:41 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:42 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:43 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:44 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:45 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:46 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:47 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:48 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:49 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:50 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:51 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:52 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:53 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:54 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:55 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:56 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:57 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:58 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:30:59 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:00 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:01 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:02 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:03 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:04 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:05 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:06 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:08 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:09 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:10 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:11 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:12 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:13 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:14 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:15 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:16 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:17 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:18 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:19 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:20 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:21 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:22 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:23 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:24 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:25 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:26 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:27 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:28 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:29 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:30 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:31 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:32 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:33 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:34 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:35 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:36 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:37 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:38 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:39 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:40 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:41 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:42 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:43 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:44 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:45 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:46 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:47 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:48 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:49 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:50 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:51 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:52 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:53 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:54 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:55 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:56 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:57 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:58 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:31:59 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:00 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:01 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:02 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:03 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:04 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:05 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:06 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:07 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:08 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:09 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:10 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:11 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:12 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:13 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:14 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:15 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:16 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:17 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:18 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:19 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:20 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:21 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:22 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:23 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:24 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:25 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:26 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:27 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:28 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:29 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:30 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:31 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:32 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:33 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:34 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:35 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:36 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:37 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:38 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:39 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:40 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:41 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:42 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:43 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:44 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:45 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:46 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:47 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:48 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:49 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:50 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:51 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:52 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:53 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:54 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:55 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:56 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:57 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:58 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:32:59 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:00 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:01 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:02 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:03 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:04 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:05 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:06 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:07 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:08 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:09 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:10 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:11 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:12 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:13 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:14 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:15 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:16 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:17 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:18 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:19 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:20 INFO Client: Application report for application_1583994958990_72748 (state: ACCEPTED)
20/03/18 21:33:20 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_72748,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_72748), /proxy/application_1583994958990_72748
20/03/18 21:33:20 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 21:33:20 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 21:33:21 INFO Client: Application report for application_1583994958990_72748 (state: RUNNING)
20/03/18 21:33:21 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.20.174.138
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584538215611
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_72748/
	 user: 1619795
20/03/18 21:33:21 INFO YarnClientSchedulerBackend: Application application_1583994958990_72748 has started running.
20/03/18 21:33:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44155.
20/03/18 21:33:21 INFO NettyBlockTransferService: Server created on 10.20.174.137:44155
20/03/18 21:33:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 21:33:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 44155, None)
20/03/18 21:33:21 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:44155 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 44155, None)
20/03/18 21:33:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 44155, None)
20/03/18 21:33:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 44155, None)
20/03/18 21:33:21 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_72748
20/03/18 21:33:21 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584536366
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584536366
*************************************** CURRENT TIME |1584538402|************************
20/03/18 21:33:22 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/18 21:33:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/18 21:33:22 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/18 21:33:22 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/18 21:33:23 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 21:33:23 INFO metastore: Connected to metastore.
20/03/18 21:33:25 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.48:39676) with ID 1
20/03/18 21:33:25 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas018.global.standardchartered.com:40290 with 912.3 MB RAM, BlockManagerId(1, hklpathas018.global.standardchartered.com, 40290, None)
20/03/18 21:33:41 INFO SessionState: Created local directory: /tmp/e9be8c52-a558-409c-8bc5-5166bcaae13a_resources
20/03/18 21:33:41 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/e9be8c52-a558-409c-8bc5-5166bcaae13a
20/03/18 21:33:41 INFO SessionState: Created local directory: /tmp/1619795/e9be8c52-a558-409c-8bc5-5166bcaae13a
20/03/18 21:33:41 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/e9be8c52-a558-409c-8bc5-5166bcaae13a/_tmp_space.db
20/03/18 21:33:41 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 21:33:41 INFO SessionState: Created local directory: /tmp/3efd1291-c4c2-413d-9d7f-c95ed972e858_resources
20/03/18 21:33:41 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/3efd1291-c4c2-413d-9d7f-c95ed972e858
20/03/18 21:33:41 INFO SessionState: Created local directory: /tmp/1619795/3efd1291-c4c2-413d-9d7f-c95ed972e858
20/03/18 21:33:41 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/3efd1291-c4c2-413d-9d7f-c95ed972e858/_tmp_space.db
20/03/18 21:33:41 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 21:33:41 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/18 21:33:42 INFO CodeGenerator: Code generated in 164.097338 ms
20/03/18 21:33:42 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/18 21:33:42 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/18 21:33:42 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/18 21:33:42 INFO DAGScheduler: Parents of final stage: List()
20/03/18 21:33:42 INFO DAGScheduler: Missing parents: List()
20/03/18 21:33:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/18 21:33:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 21:33:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 21:33:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:44155 (size: 5.0 KB, free: 366.3 MB)
20/03/18 21:33:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/18 21:33:42 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 21:33:42 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/18 21:33:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas018.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 21:33:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas018.global.standardchartered.com:40290 (size: 5.0 KB, free: 912.3 MB)
20/03/18 21:33:46 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas018.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 21:33:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3346 ms on hklpathas018.global.standardchartered.com (executor 1) (1/4)
20/03/18 21:33:46 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas018.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 21:33:46 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 20 ms on hklpathas018.global.standardchartered.com (executor 1) (2/4)
20/03/18 21:33:46 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas018.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 57590 bytes)
20/03/18 21:33:46 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 10 ms on hklpathas018.global.standardchartered.com (executor 1) (3/4)
20/03/18 21:33:46 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 211 ms on hklpathas018.global.standardchartered.com (executor 1) (4/4)
20/03/18 21:33:46 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 21:33:46 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 3.581 s
20/03/18 21:33:46 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 3.734906 s
20/03/18 21:33:46 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/18 21:33:46 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/18 21:33:46 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/18 21:33:46 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/18 21:33:46 INFO DAGScheduler: Parents of final stage: List()
20/03/18 21:33:46 INFO DAGScheduler: Missing parents: List()
20/03/18 21:33:46 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/18 21:33:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 21:33:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 21:33:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:44155 (size: 5.0 KB, free: 366.3 MB)
20/03/18 21:33:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 21:33:46 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 21:33:46 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/18 21:33:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas018.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 21:33:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas018.global.standardchartered.com:40290 (size: 5.0 KB, free: 912.3 MB)
20/03/18 21:33:46 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas018.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 21:33:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 26 ms on hklpathas018.global.standardchartered.com (executor 1) (1/4)
20/03/18 21:33:46 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas018.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 21:33:46 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 8 ms on hklpathas018.global.standardchartered.com (executor 1) (2/4)
20/03/18 21:33:46 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas018.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 57590 bytes)
20/03/18 21:33:46 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpathas018.global.standardchartered.com (executor 1) (3/4)
20/03/18 21:33:46 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 14 ms on hklpathas018.global.standardchartered.com (executor 1) (4/4)
20/03/18 21:33:46 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/18 21:33:46 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.054 s
20/03/18 21:33:46 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.068906 s
20/03/18 21:33:47 INFO ContextCleaner: Cleaned accumulator 51
20/03/18 21:33:47 INFO ContextCleaner: Cleaned accumulator 52
20/03/18 21:33:47 INFO ContextCleaner: Cleaned accumulator 1
20/03/18 21:33:47 INFO ContextCleaner: Cleaned accumulator 0
20/03/18 21:33:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:44155 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 21:33:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas018.global.standardchartered.com:40290 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 21:33:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:44155 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 21:33:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas018.global.standardchartered.com:40290 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 21:33:47 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_21-33-47_441_3027245482584072249-1
20/03/18 21:33:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/18 21:33:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/18 21:33:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/18 21:33:47 INFO CodeGenerator: Code generated in 61.893028 ms
20/03/18 21:33:47 INFO CodeGenerator: Code generated in 31.527928 ms
20/03/18 21:33:47 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/18 21:33:47 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/18 21:33:47 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/18 21:33:47 INFO DAGScheduler: Parents of final stage: List()
20/03/18 21:33:47 INFO DAGScheduler: Missing parents: List()
20/03/18 21:33:47 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/18 21:33:47 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 309.2 KB, free 366.0 MB)
20/03/18 21:33:47 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 110.2 KB, free 365.9 MB)
20/03/18 21:33:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:44155 (size: 110.2 KB, free: 366.2 MB)
20/03/18 21:33:47 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/18 21:33:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/18 21:33:47 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/18 21:33:47 WARN TaskSetManager: Stage 2 contains a task of very large size (108 KB). The maximum recommended task size is 100 KB.
20/03/18 21:33:47 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas018.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 111081 bytes)
20/03/18 21:33:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas018.global.standardchartered.com:40290 (size: 110.2 KB, free: 912.2 MB)
20/03/18 21:33:49 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 1994 ms on hklpathas018.global.standardchartered.com (executor 1) (1/1)
20/03/18 21:33:49 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/18 21:33:49 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 1.995 s
20/03/18 21:33:49 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.063790 s
20/03/18 21:33:50 INFO FileFormatWriter: Job null committed.
20/03/18 21:33:50 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/18 21:33:50 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_21-33-47_441_3027245482584072249-1/-ext-10000/ods=2020-03-18/part-00000-ec04cb0d-b16d-421f-a16c-55023b2219a0.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18/part-00000-ec04cb0d-b16d-421f-a16c-55023b2219a0.c000, Status:true
20/03/18 21:33:50 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_21-33-47_441_3027245482584072249-1/-ext-10000/ods=2020-03-18 with partSpec {ods=2020-03-18}
20/03/18 21:33:50 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 21:33:50 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4041
20/03/18 21:33:50 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 21:33:50 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 21:33:50 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 21:33:50 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 21:33:50 INFO YarnClientSchedulerBackend: Stopped
20/03/18 21:33:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 21:33:50 INFO MemoryStore: MemoryStore cleared
20/03/18 21:33:50 INFO BlockManager: BlockManager stopped
20/03/18 21:33:50 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 21:33:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 21:33:50 INFO SparkContext: Successfully stopped SparkContext
20/03/18 21:33:50 INFO ShutdownHookManager: Shutdown hook called
20/03/18 21:33:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-2aae37ac-a4ca-4111-a716-2191a472d92a
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584538430
+ DIFF_TIME=239
+ echo 'END_TIME: ' 1584538430
+ echo 'Total time taken: ' 239
+ attempt_num=9
+ sleep 30m
+ '[' 9 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584538402
++ date +%s
+ START_TIME=1584540230
+ echo 1584540230
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584538402 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 22:03:51 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 22:03:52 INFO SparkContext: Submitted application: CoolLogtest
20/03/18 22:03:52 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 22:03:52 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 22:03:52 INFO SecurityManager: Changing view acls groups to: 
20/03/18 22:03:52 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 22:03:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 22:03:53 INFO Utils: Successfully started service 'sparkDriver' on port 42935.
20/03/18 22:03:53 INFO SparkEnv: Registering MapOutputTracker
20/03/18 22:03:53 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 22:03:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 22:03:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 22:03:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e9c4d33d-c3b9-43c5-b064-6ea008d54b03
20/03/18 22:03:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 22:03:53 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 22:03:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/18 22:03:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/18 22:03:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
20/03/18 22:03:53 INFO Utils: Successfully started service 'SparkUI' on port 4043.
20/03/18 22:03:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4043
20/03/18 22:03:53 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:42935/jars/CoolPocTest.jar with timestamp 1584540233761
20/03/18 22:03:54 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 22:03:55 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 22:03:55 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 22:03:55 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 22:03:55 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 22:03:55 INFO Client: Setting up container launch context for our AM
20/03/18 22:03:55 INFO Client: Setting up the launch environment for our AM container
20/03/18 22:03:55 INFO Client: Preparing resources for our AM container
20/03/18 22:03:55 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 22:03:55 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7073351 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 22:03:56 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 22:03:56 INFO metastore: Connected to metastore.
20/03/18 22:04:16 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ed f5 8a 85 8a 01 71 12 02 0e 85 8e 03 f0 8e 02 7a
20/03/18 22:04:17 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 22:04:17 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 22:04:17 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73126/datanucleus-api-jdo-3.2.6.jar
20/03/18 22:04:17 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73126/datanucleus-core-3.2.10.jar
20/03/18 22:04:17 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73126/datanucleus-rdbms-3.2.9.jar
20/03/18 22:04:17 INFO Client: Uploading resource file:/tmp/spark-fae652f5-1c72-43b5-a4ae-60cde98711e2/__spark_conf__3271815694489145393.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73126/__spark_conf__.zip
20/03/18 22:04:17 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 22:04:17 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 22:04:17 INFO SecurityManager: Changing view acls groups to: 
20/03/18 22:04:17 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 22:04:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 22:04:17 INFO Client: Submitting application application_1583994958990_73126 to ResourceManager
20/03/18 22:04:18 INFO YarnClientImpl: Submitted application application_1583994958990_73126
20/03/18 22:04:18 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_73126 and attemptId None
20/03/18 22:04:19 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:19 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584540257983
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73126/
	 user: 1619795
20/03/18 22:04:20 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:21 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:22 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:23 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:24 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:25 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:26 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:27 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:28 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:29 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:30 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:31 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:32 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:33 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:34 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:35 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:36 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:37 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:38 INFO Client: Application report for application_1583994958990_73126 (state: ACCEPTED)
20/03/18 22:04:38 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_73126,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73126), /proxy/application_1583994958990_73126
20/03/18 22:04:38 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 22:04:38 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 22:04:39 INFO Client: Application report for application_1583994958990_73126 (state: RUNNING)
20/03/18 22:04:39 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.142.44
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584540257983
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73126/
	 user: 1619795
20/03/18 22:04:39 INFO YarnClientSchedulerBackend: Application application_1583994958990_73126 has started running.
20/03/18 22:04:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39347.
20/03/18 22:04:39 INFO NettyBlockTransferService: Server created on 10.20.174.137:39347
20/03/18 22:04:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 22:04:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 39347, None)
20/03/18 22:04:39 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:39347 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 39347, None)
20/03/18 22:04:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 39347, None)
20/03/18 22:04:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 39347, None)
20/03/18 22:04:39 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_73126
20/03/18 22:04:39 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584538402
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584538402
*************************************** CURRENT TIME |1584540279|************************
20/03/18 22:04:40 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/18 22:04:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/18 22:04:40 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/18 22:04:40 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/18 22:04:40 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 22:04:40 INFO metastore: Connected to metastore.
20/03/18 22:04:45 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.142.46:44808) with ID 1
20/03/18 22:04:45 INFO BlockManagerMasterEndpoint: Registering block manager hklpadhaa006.global.standardchartered.com:33150 with 912.3 MB RAM, BlockManagerId(1, hklpadhaa006.global.standardchartered.com, 33150, None)
20/03/18 22:04:58 INFO SessionState: Created local directory: /tmp/0dc88533-c2e7-48f1-b20f-85366bb17c06_resources
20/03/18 22:04:58 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/0dc88533-c2e7-48f1-b20f-85366bb17c06
20/03/18 22:04:58 INFO SessionState: Created local directory: /tmp/1619795/0dc88533-c2e7-48f1-b20f-85366bb17c06
20/03/18 22:04:58 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/0dc88533-c2e7-48f1-b20f-85366bb17c06/_tmp_space.db
20/03/18 22:04:58 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 22:04:59 INFO SessionState: Created local directory: /tmp/8304d9e1-5f1e-43ea-b4db-e3e933c3fe94_resources
20/03/18 22:04:59 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/8304d9e1-5f1e-43ea-b4db-e3e933c3fe94
20/03/18 22:04:59 INFO SessionState: Created local directory: /tmp/1619795/8304d9e1-5f1e-43ea-b4db-e3e933c3fe94
20/03/18 22:04:59 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/8304d9e1-5f1e-43ea-b4db-e3e933c3fe94/_tmp_space.db
20/03/18 22:04:59 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 22:04:59 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/18 22:05:00 INFO CodeGenerator: Code generated in 164.266068 ms
20/03/18 22:05:00 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/18 22:05:00 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/18 22:05:00 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/18 22:05:00 INFO DAGScheduler: Parents of final stage: List()
20/03/18 22:05:00 INFO DAGScheduler: Missing parents: List()
20/03/18 22:05:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/18 22:05:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 22:05:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 22:05:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:39347 (size: 5.0 KB, free: 366.3 MB)
20/03/18 22:05:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/18 22:05:00 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 22:05:00 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/18 22:05:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpadhaa006.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 22:05:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpadhaa006.global.standardchartered.com:33150 (size: 5.0 KB, free: 912.3 MB)
20/03/18 22:05:01 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpadhaa006.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 22:05:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1059 ms on hklpadhaa006.global.standardchartered.com (executor 1) (1/4)
20/03/18 22:05:01 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpadhaa006.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 22:05:01 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 31 ms on hklpadhaa006.global.standardchartered.com (executor 1) (2/4)
20/03/18 22:05:01 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpadhaa006.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 58691 bytes)
20/03/18 22:05:01 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 16 ms on hklpadhaa006.global.standardchartered.com (executor 1) (3/4)
20/03/18 22:05:01 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 214 ms on hklpadhaa006.global.standardchartered.com (executor 1) (4/4)
20/03/18 22:05:01 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 22:05:01 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 1.311 s
20/03/18 22:05:01 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.489438 s
20/03/18 22:05:02 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/18 22:05:02 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/18 22:05:02 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/18 22:05:02 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/18 22:05:02 INFO DAGScheduler: Parents of final stage: List()
20/03/18 22:05:02 INFO DAGScheduler: Missing parents: List()
20/03/18 22:05:02 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/18 22:05:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 22:05:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 22:05:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:39347 (size: 5.0 KB, free: 366.3 MB)
20/03/18 22:05:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 22:05:02 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 22:05:02 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/18 22:05:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpadhaa006.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 22:05:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpadhaa006.global.standardchartered.com:33150 (size: 5.0 KB, free: 912.3 MB)
20/03/18 22:05:02 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpadhaa006.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 22:05:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 42 ms on hklpadhaa006.global.standardchartered.com (executor 1) (1/4)
20/03/18 22:05:02 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpadhaa006.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 22:05:02 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 11 ms on hklpadhaa006.global.standardchartered.com (executor 1) (2/4)
20/03/18 22:05:02 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpadhaa006.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 58691 bytes)
20/03/18 22:05:02 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 147 ms on hklpadhaa006.global.standardchartered.com (executor 1) (3/4)
20/03/18 22:05:02 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 30 ms on hklpadhaa006.global.standardchartered.com (executor 1) (4/4)
20/03/18 22:05:02 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/18 22:05:02 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.230 s
20/03/18 22:05:02 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.238448 s
20/03/18 22:05:02 INFO ContextCleaner: Cleaned accumulator 0
20/03/18 22:05:02 INFO ContextCleaner: Cleaned accumulator 1
20/03/18 22:05:02 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:39347 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 22:05:02 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpadhaa006.global.standardchartered.com:33150 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 22:05:02 INFO ContextCleaner: Cleaned accumulator 52
20/03/18 22:05:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:39347 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 22:05:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpadhaa006.global.standardchartered.com:33150 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 22:05:02 INFO ContextCleaner: Cleaned accumulator 51
20/03/18 22:05:03 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_22-05-03_222_8733733629460421458-1
20/03/18 22:05:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/18 22:05:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/18 22:05:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/18 22:05:03 INFO CodeGenerator: Code generated in 61.26473 ms
20/03/18 22:05:03 INFO CodeGenerator: Code generated in 29.803848 ms
20/03/18 22:05:03 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/18 22:05:03 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/18 22:05:03 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/18 22:05:03 INFO DAGScheduler: Parents of final stage: List()
20/03/18 22:05:03 INFO DAGScheduler: Missing parents: List()
20/03/18 22:05:03 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/18 22:05:03 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 309.2 KB, free 366.0 MB)
20/03/18 22:05:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 110.2 KB, free 365.9 MB)
20/03/18 22:05:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:39347 (size: 110.2 KB, free: 366.2 MB)
20/03/18 22:05:03 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/18 22:05:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/18 22:05:03 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/18 22:05:03 WARN TaskSetManager: Stage 2 contains a task of very large size (110 KB). The maximum recommended task size is 100 KB.
20/03/18 22:05:03 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpadhaa006.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 113283 bytes)
20/03/18 22:05:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpadhaa006.global.standardchartered.com:33150 (size: 110.2 KB, free: 912.2 MB)
20/03/18 22:05:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2745 ms on hklpadhaa006.global.standardchartered.com (executor 1) (1/1)
20/03/18 22:05:06 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/18 22:05:06 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.745 s
20/03/18 22:05:06 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.813989 s
20/03/18 22:05:06 INFO FileFormatWriter: Job null committed.
20/03/18 22:05:06 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/18 22:05:06 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_22-05-03_222_8733733629460421458-1/-ext-10000/ods=2020-03-18/part-00000-e8b5f531-d03d-4272-be7e-d5aefc0a5f1a.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18/part-00000-e8b5f531-d03d-4272-be7e-d5aefc0a5f1a.c000, Status:true
20/03/18 22:05:06 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_22-05-03_222_8733733629460421458-1/-ext-10000/ods=2020-03-18 with partSpec {ods=2020-03-18}
20/03/18 22:05:06 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 22:05:06 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4043
20/03/18 22:05:06 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 22:05:06 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 22:05:06 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 22:05:06 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 22:05:07 INFO YarnClientSchedulerBackend: Stopped
20/03/18 22:05:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 22:05:07 INFO MemoryStore: MemoryStore cleared
20/03/18 22:05:07 INFO BlockManager: BlockManager stopped
20/03/18 22:05:07 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 22:05:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 22:05:07 INFO SparkContext: Successfully stopped SparkContext
20/03/18 22:05:07 INFO ShutdownHookManager: Shutdown hook called
20/03/18 22:05:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-fae652f5-1c72-43b5-a4ae-60cde98711e2
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584540307
+ DIFF_TIME=77
+ echo 'END_TIME: ' 1584540307
+ echo 'Total time taken: ' 77
+ attempt_num=10
+ sleep 30m
+ '[' 10 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584540279
++ date +%s
+ START_TIME=1584542107
+ echo 1584542107
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584540279 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 22:35:08 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 22:35:09 INFO SparkContext: Submitted application: CoolLogtest
20/03/18 22:35:09 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 22:35:09 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 22:35:09 INFO SecurityManager: Changing view acls groups to: 
20/03/18 22:35:09 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 22:35:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 22:35:09 INFO Utils: Successfully started service 'sparkDriver' on port 45048.
20/03/18 22:35:09 INFO SparkEnv: Registering MapOutputTracker
20/03/18 22:35:09 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 22:35:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 22:35:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 22:35:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-62fe8aae-cefa-41be-b993-0fdf1f412fc3
20/03/18 22:35:09 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 22:35:10 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 22:35:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/18 22:35:10 INFO Utils: Successfully started service 'SparkUI' on port 4041.
20/03/18 22:35:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4041
20/03/18 22:35:10 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:45048/jars/CoolPocTest.jar with timestamp 1584542110289
20/03/18 22:35:11 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 22:35:11 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 22:35:11 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 22:35:11 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 22:35:11 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 22:35:11 INFO Client: Setting up container launch context for our AM
20/03/18 22:35:11 INFO Client: Setting up the launch environment for our AM container
20/03/18 22:35:11 INFO Client: Preparing resources for our AM container
20/03/18 22:35:11 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 22:35:11 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7075825 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 22:35:12 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 22:35:12 INFO metastore: Connected to metastore.
20/03/18 22:35:29 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ee 12 1c 39 8a 01 71 12 1e a0 39 8e 03 fb 8e 02 7a
20/03/18 22:35:29 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 22:35:29 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 22:35:29 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73324/datanucleus-api-jdo-3.2.6.jar
20/03/18 22:35:29 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73324/datanucleus-core-3.2.10.jar
20/03/18 22:35:29 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73324/datanucleus-rdbms-3.2.9.jar
20/03/18 22:35:29 INFO Client: Uploading resource file:/tmp/spark-4ec9d116-aad5-4902-b253-59ea2249b7d1/__spark_conf__3390256428081094344.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73324/__spark_conf__.zip
20/03/18 22:35:29 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 22:35:29 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 22:35:29 INFO SecurityManager: Changing view acls groups to: 
20/03/18 22:35:29 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 22:35:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 22:35:29 INFO Client: Submitting application application_1583994958990_73324 to ResourceManager
20/03/18 22:35:30 INFO YarnClientImpl: Submitted application application_1583994958990_73324
20/03/18 22:35:30 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_73324 and attemptId None
20/03/18 22:35:31 INFO Client: Application report for application_1583994958990_73324 (state: ACCEPTED)
20/03/18 22:35:31 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584542129899
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73324/
	 user: 1619795
20/03/18 22:35:32 INFO Client: Application report for application_1583994958990_73324 (state: ACCEPTED)
20/03/18 22:35:33 INFO Client: Application report for application_1583994958990_73324 (state: ACCEPTED)
20/03/18 22:35:34 INFO Client: Application report for application_1583994958990_73324 (state: ACCEPTED)
20/03/18 22:35:34 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_73324,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73324), /proxy/application_1583994958990_73324
20/03/18 22:35:34 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 22:35:34 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 22:35:35 INFO Client: Application report for application_1583994958990_73324 (state: RUNNING)
20/03/18 22:35:35 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.47
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584542129899
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73324/
	 user: 1619795
20/03/18 22:35:35 INFO YarnClientSchedulerBackend: Application application_1583994958990_73324 has started running.
20/03/18 22:35:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38264.
20/03/18 22:35:35 INFO NettyBlockTransferService: Server created on 10.20.174.137:38264
20/03/18 22:35:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 22:35:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 38264, None)
20/03/18 22:35:35 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:38264 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 38264, None)
20/03/18 22:35:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 38264, None)
20/03/18 22:35:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 38264, None)
20/03/18 22:35:35 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_73324
20/03/18 22:35:37 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.47:55532) with ID 1
20/03/18 22:35:37 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/03/18 22:35:37 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas017.global.standardchartered.com:41075 with 912.3 MB RAM, BlockManagerId(1, hklpathas017.global.standardchartered.com, 41075, None)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584540279
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584540279
*************************************** CURRENT TIME |1584542137|************************
20/03/18 22:35:38 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/18 22:35:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/18 22:35:38 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/18 22:35:38 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/18 22:35:38 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 22:35:38 INFO metastore: Connected to metastore.
20/03/18 22:35:56 INFO SessionState: Created local directory: /tmp/ffc92d36-c993-45b7-b1e3-d43c97bcf93e_resources
20/03/18 22:35:56 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/ffc92d36-c993-45b7-b1e3-d43c97bcf93e
20/03/18 22:35:56 INFO SessionState: Created local directory: /tmp/1619795/ffc92d36-c993-45b7-b1e3-d43c97bcf93e
20/03/18 22:35:56 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/ffc92d36-c993-45b7-b1e3-d43c97bcf93e/_tmp_space.db
20/03/18 22:35:56 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 22:35:56 INFO SessionState: Created local directory: /tmp/0365eb77-45e8-4793-821a-12ad0b4f0dff_resources
20/03/18 22:35:56 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/0365eb77-45e8-4793-821a-12ad0b4f0dff
20/03/18 22:35:56 INFO SessionState: Created local directory: /tmp/1619795/0365eb77-45e8-4793-821a-12ad0b4f0dff
20/03/18 22:35:56 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/0365eb77-45e8-4793-821a-12ad0b4f0dff/_tmp_space.db
20/03/18 22:35:56 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 22:35:56 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/18 22:35:57 INFO CodeGenerator: Code generated in 153.692321 ms
20/03/18 22:35:57 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/18 22:35:57 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/18 22:35:57 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/18 22:35:57 INFO DAGScheduler: Parents of final stage: List()
20/03/18 22:35:57 INFO DAGScheduler: Missing parents: List()
20/03/18 22:35:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/18 22:35:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 22:35:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 22:35:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:38264 (size: 5.0 KB, free: 366.3 MB)
20/03/18 22:35:57 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/18 22:35:57 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 22:35:57 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/18 22:35:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas017.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 22:35:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas017.global.standardchartered.com:41075 (size: 5.0 KB, free: 912.3 MB)
20/03/18 22:35:58 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas017.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 22:35:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 596 ms on hklpathas017.global.standardchartered.com (executor 1) (1/4)
20/03/18 22:35:58 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas017.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 22:35:58 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 22 ms on hklpathas017.global.standardchartered.com (executor 1) (2/4)
20/03/18 22:35:58 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas017.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 63017 bytes)
20/03/18 22:35:58 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 11 ms on hklpathas017.global.standardchartered.com (executor 1) (3/4)
20/03/18 22:35:58 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 207 ms on hklpathas017.global.standardchartered.com (executor 1) (4/4)
20/03/18 22:35:58 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 22:35:58 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.831 s
20/03/18 22:35:58 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.016011 s
20/03/18 22:35:58 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/18 22:35:58 INFO ContextCleaner: Cleaned accumulator 0
20/03/18 22:35:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:38264 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 22:35:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas017.global.standardchartered.com:41075 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 22:35:58 INFO ContextCleaner: Cleaned accumulator 1
20/03/18 22:35:58 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/18 22:35:58 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/18 22:35:58 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/18 22:35:58 INFO DAGScheduler: Parents of final stage: List()
20/03/18 22:35:58 INFO DAGScheduler: Missing parents: List()
20/03/18 22:35:58 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/18 22:35:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 22:35:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 22:35:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:38264 (size: 5.0 KB, free: 366.3 MB)
20/03/18 22:35:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 22:35:58 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 22:35:58 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/18 22:35:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas017.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 22:35:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas017.global.standardchartered.com:41075 (size: 5.0 KB, free: 912.3 MB)
20/03/18 22:35:58 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas017.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 22:35:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 22 ms on hklpathas017.global.standardchartered.com (executor 1) (1/4)
20/03/18 22:35:58 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas017.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 22:35:58 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 8 ms on hklpathas017.global.standardchartered.com (executor 1) (2/4)
20/03/18 22:35:58 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas017.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 63017 bytes)
20/03/18 22:35:58 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpathas017.global.standardchartered.com (executor 1) (3/4)
20/03/18 22:35:58 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 15 ms on hklpathas017.global.standardchartered.com (executor 1) (4/4)
20/03/18 22:35:58 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/18 22:35:58 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.049 s
20/03/18 22:35:58 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.056987 s
20/03/18 22:35:59 INFO ContextCleaner: Cleaned accumulator 51
20/03/18 22:35:59 INFO ContextCleaner: Cleaned accumulator 52
20/03/18 22:35:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:38264 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 22:35:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas017.global.standardchartered.com:41075 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 22:35:59 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_22-35-59_722_1257443058153540666-1
20/03/18 22:35:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/18 22:35:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/18 22:35:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/18 22:36:00 INFO CodeGenerator: Code generated in 85.516411 ms
20/03/18 22:36:00 INFO CodeGenerator: Code generated in 59.663216 ms
20/03/18 22:36:00 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/18 22:36:00 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/18 22:36:00 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/18 22:36:00 INFO DAGScheduler: Parents of final stage: List()
20/03/18 22:36:00 INFO DAGScheduler: Missing parents: List()
20/03/18 22:36:00 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/18 22:36:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/18 22:36:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/18 22:36:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:38264 (size: 114.9 KB, free: 366.2 MB)
20/03/18 22:36:00 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/18 22:36:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/18 22:36:00 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/18 22:36:00 WARN TaskSetManager: Stage 2 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.
20/03/18 22:36:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas017.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 121935 bytes)
20/03/18 22:36:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas017.global.standardchartered.com:41075 (size: 114.9 KB, free: 912.2 MB)
20/03/18 22:36:02 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2530 ms on hklpathas017.global.standardchartered.com (executor 1) (1/1)
20/03/18 22:36:02 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/18 22:36:02 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.531 s
20/03/18 22:36:02 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.601776 s
20/03/18 22:36:02 INFO FileFormatWriter: Job null committed.
20/03/18 22:36:03 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/18 22:36:03 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_22-35-59_722_1257443058153540666-1/-ext-10000/ods=2020-03-18/part-00000-bbf9252a-7f7a-4165-934f-682b42f750ec.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18/part-00000-bbf9252a-7f7a-4165-934f-682b42f750ec.c000, Status:true
20/03/18 22:36:03 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_22-35-59_722_1257443058153540666-1/-ext-10000/ods=2020-03-18 with partSpec {ods=2020-03-18}
20/03/18 22:36:03 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 22:36:03 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4041
20/03/18 22:36:03 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 22:36:03 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 22:36:03 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 22:36:03 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 22:36:03 INFO YarnClientSchedulerBackend: Stopped
20/03/18 22:36:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 22:36:03 INFO MemoryStore: MemoryStore cleared
20/03/18 22:36:03 INFO BlockManager: BlockManager stopped
20/03/18 22:36:03 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 22:36:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 22:36:03 INFO SparkContext: Successfully stopped SparkContext
20/03/18 22:36:03 INFO ShutdownHookManager: Shutdown hook called
20/03/18 22:36:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ec9d116-aad5-4902-b253-59ea2249b7d1
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584542163
+ DIFF_TIME=56
+ echo 'END_TIME: ' 1584542163
+ echo 'Total time taken: ' 56
+ attempt_num=11
+ sleep 30m
+ '[' 11 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584542137
++ date +%s
+ START_TIME=1584543963
+ echo 1584543963
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584542137 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 23:06:04 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 23:06:05 INFO SparkContext: Submitted application: CoolLogtest
20/03/18 23:06:05 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 23:06:05 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 23:06:05 INFO SecurityManager: Changing view acls groups to: 
20/03/18 23:06:05 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 23:06:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 23:06:06 INFO Utils: Successfully started service 'sparkDriver' on port 42213.
20/03/18 23:06:06 INFO SparkEnv: Registering MapOutputTracker
20/03/18 23:06:06 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 23:06:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 23:06:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 23:06:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d9f14f0b-8c60-4460-9ca5-5c09ba5c275b
20/03/18 23:06:06 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 23:06:06 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 23:06:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/18 23:06:06 INFO Utils: Successfully started service 'SparkUI' on port 4041.
20/03/18 23:06:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4041
20/03/18 23:06:06 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:42213/jars/CoolPocTest.jar with timestamp 1584543966525
20/03/18 23:06:07 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 23:06:07 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 23:06:07 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 23:06:07 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 23:06:07 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 23:06:07 INFO Client: Setting up container launch context for our AM
20/03/18 23:06:07 INFO Client: Setting up the launch environment for our AM container
20/03/18 23:06:07 INFO Client: Preparing resources for our AM container
20/03/18 23:06:07 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 23:06:07 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7079967 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 23:06:09 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 23:06:09 INFO metastore: Connected to metastore.
20/03/18 23:06:25 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ee 2e 70 17 8a 01 71 12 3a f4 17 8e 04 00 8e 02 7a
20/03/18 23:06:25 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 23:06:25 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 23:06:25 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73515/datanucleus-api-jdo-3.2.6.jar
20/03/18 23:06:26 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73515/datanucleus-core-3.2.10.jar
20/03/18 23:06:26 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73515/datanucleus-rdbms-3.2.9.jar
20/03/18 23:06:26 INFO Client: Uploading resource file:/tmp/spark-86f8f5ab-9cf0-4751-b418-45a8d215ad55/__spark_conf__8266696511115728462.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73515/__spark_conf__.zip
20/03/18 23:06:26 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 23:06:26 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 23:06:26 INFO SecurityManager: Changing view acls groups to: 
20/03/18 23:06:26 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 23:06:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 23:06:26 INFO Client: Submitting application application_1583994958990_73515 to ResourceManager
20/03/18 23:06:26 INFO YarnClientImpl: Submitted application application_1583994958990_73515
20/03/18 23:06:26 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_73515 and attemptId None
20/03/18 23:06:27 INFO Client: Application report for application_1583994958990_73515 (state: ACCEPTED)
20/03/18 23:06:27 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584543986435
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73515/
	 user: 1619795
20/03/18 23:06:28 INFO Client: Application report for application_1583994958990_73515 (state: ACCEPTED)
20/03/18 23:06:29 INFO Client: Application report for application_1583994958990_73515 (state: ACCEPTED)
20/03/18 23:06:30 INFO Client: Application report for application_1583994958990_73515 (state: ACCEPTED)
20/03/18 23:06:31 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_73515,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73515), /proxy/application_1583994958990_73515
20/03/18 23:06:31 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 23:06:31 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 23:06:31 INFO Client: Application report for application_1583994958990_73515 (state: RUNNING)
20/03/18 23:06:31 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.142.68
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584543986435
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73515/
	 user: 1619795
20/03/18 23:06:31 INFO YarnClientSchedulerBackend: Application application_1583994958990_73515 has started running.
20/03/18 23:06:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40148.
20/03/18 23:06:31 INFO NettyBlockTransferService: Server created on 10.20.174.137:40148
20/03/18 23:06:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 23:06:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 40148, None)
20/03/18 23:06:31 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:40148 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 40148, None)
20/03/18 23:06:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 40148, None)
20/03/18 23:06:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 40148, None)
20/03/18 23:06:31 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_73515
20/03/18 23:06:36 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
20/03/18 23:06:36 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.22:52152) with ID 1
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584542137
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584542137
20/03/18 23:06:36 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas015.global.standardchartered.com:39019 with 912.3 MB RAM, BlockManagerId(1, hklpathas015.global.standardchartered.com, 39019, None)
*************************************** CURRENT TIME |1584543996|************************
20/03/18 23:06:37 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/18 23:06:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/18 23:06:37 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/18 23:06:37 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/18 23:06:37 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 23:06:37 INFO metastore: Connected to metastore.
20/03/18 23:06:54 INFO SessionState: Created local directory: /tmp/6de39e85-c662-4f8c-82cd-cb5f7679b2ec_resources
20/03/18 23:06:54 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/6de39e85-c662-4f8c-82cd-cb5f7679b2ec
20/03/18 23:06:54 INFO SessionState: Created local directory: /tmp/1619795/6de39e85-c662-4f8c-82cd-cb5f7679b2ec
20/03/18 23:06:54 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/6de39e85-c662-4f8c-82cd-cb5f7679b2ec/_tmp_space.db
20/03/18 23:06:54 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 23:06:54 INFO SessionState: Created local directory: /tmp/9435629f-724d-4271-ae61-e82972acf470_resources
20/03/18 23:06:54 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/9435629f-724d-4271-ae61-e82972acf470
20/03/18 23:06:54 INFO SessionState: Created local directory: /tmp/1619795/9435629f-724d-4271-ae61-e82972acf470
20/03/18 23:06:54 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/9435629f-724d-4271-ae61-e82972acf470/_tmp_space.db
20/03/18 23:06:54 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 23:06:54 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/18 23:06:55 INFO CodeGenerator: Code generated in 165.074308 ms
20/03/18 23:06:55 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/18 23:06:55 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/18 23:06:55 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/18 23:06:55 INFO DAGScheduler: Parents of final stage: List()
20/03/18 23:06:55 INFO DAGScheduler: Missing parents: List()
20/03/18 23:06:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/18 23:06:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 23:06:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 23:06:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:40148 (size: 5.0 KB, free: 366.3 MB)
20/03/18 23:06:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/18 23:06:55 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 23:06:55 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/18 23:06:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas015.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 23:06:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas015.global.standardchartered.com:39019 (size: 5.0 KB, free: 912.3 MB)
20/03/18 23:06:56 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas015.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 23:06:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 766 ms on hklpathas015.global.standardchartered.com (executor 1) (1/4)
20/03/18 23:06:56 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas015.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 23:06:56 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 28 ms on hklpathas015.global.standardchartered.com (executor 1) (2/4)
20/03/18 23:06:56 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas015.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 64613 bytes)
20/03/18 23:06:56 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 15 ms on hklpathas015.global.standardchartered.com (executor 1) (3/4)
20/03/18 23:06:56 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 140 ms on hklpathas015.global.standardchartered.com (executor 1) (4/4)
20/03/18 23:06:56 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 23:06:56 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.943 s
20/03/18 23:06:56 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.135795 s
20/03/18 23:06:56 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/18 23:06:56 INFO ContextCleaner: Cleaned accumulator 0
20/03/18 23:06:56 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:40148 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 23:06:56 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/18 23:06:56 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/18 23:06:56 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/18 23:06:56 INFO DAGScheduler: Parents of final stage: List()
20/03/18 23:06:56 INFO DAGScheduler: Missing parents: List()
20/03/18 23:06:56 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/18 23:06:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 23:06:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 23:06:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:40148 (size: 5.0 KB, free: 366.3 MB)
20/03/18 23:06:56 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 23:06:56 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 23:06:56 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/18 23:06:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas015.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 23:06:56 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas015.global.standardchartered.com:39019 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 23:06:56 INFO ContextCleaner: Cleaned accumulator 1
20/03/18 23:06:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas015.global.standardchartered.com:39019 (size: 5.0 KB, free: 912.3 MB)
20/03/18 23:06:56 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas015.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 23:06:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 25 ms on hklpathas015.global.standardchartered.com (executor 1) (1/4)
20/03/18 23:06:56 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas015.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 23:06:56 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 8 ms on hklpathas015.global.standardchartered.com (executor 1) (2/4)
20/03/18 23:06:56 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas015.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 64613 bytes)
20/03/18 23:06:56 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 9 ms on hklpathas015.global.standardchartered.com (executor 1) (3/4)
20/03/18 23:06:56 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 17 ms on hklpathas015.global.standardchartered.com (executor 1) (4/4)
20/03/18 23:06:56 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/18 23:06:56 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.059 s
20/03/18 23:06:56 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.068510 s
20/03/18 23:06:57 INFO ContextCleaner: Cleaned accumulator 51
20/03/18 23:06:57 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:40148 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 23:06:57 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas015.global.standardchartered.com:39019 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 23:06:57 INFO ContextCleaner: Cleaned accumulator 52
20/03/18 23:06:57 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_23-06-57_636_8227342874135816798-1
20/03/18 23:06:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/18 23:06:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/18 23:06:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/18 23:06:57 INFO CodeGenerator: Code generated in 76.164893 ms
20/03/18 23:06:58 INFO CodeGenerator: Code generated in 49.73565 ms
20/03/18 23:06:58 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/18 23:06:58 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/18 23:06:58 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/18 23:06:58 INFO DAGScheduler: Parents of final stage: List()
20/03/18 23:06:58 INFO DAGScheduler: Missing parents: List()
20/03/18 23:06:58 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/18 23:06:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/18 23:06:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/18 23:06:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:40148 (size: 114.9 KB, free: 366.2 MB)
20/03/18 23:06:58 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/18 23:06:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/18 23:06:58 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/18 23:06:58 WARN TaskSetManager: Stage 2 contains a task of very large size (122 KB). The maximum recommended task size is 100 KB.
20/03/18 23:06:58 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas015.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 125127 bytes)
20/03/18 23:06:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas015.global.standardchartered.com:39019 (size: 114.9 KB, free: 912.2 MB)
20/03/18 23:07:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2112 ms on hklpathas015.global.standardchartered.com (executor 1) (1/1)
20/03/18 23:07:00 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/18 23:07:00 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.113 s
20/03/18 23:07:00 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.191828 s
20/03/18 23:07:00 INFO FileFormatWriter: Job null committed.
20/03/18 23:07:00 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/18 23:07:00 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_23-06-57_636_8227342874135816798-1/-ext-10000/ods=2020-03-18/part-00000-fefa3bc5-2db7-4695-bfbe-52b232773e9c.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18/part-00000-fefa3bc5-2db7-4695-bfbe-52b232773e9c.c000, Status:true
20/03/18 23:07:00 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_23-06-57_636_8227342874135816798-1/-ext-10000/ods=2020-03-18 with partSpec {ods=2020-03-18}
20/03/18 23:07:00 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 23:07:00 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4041
20/03/18 23:07:00 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 23:07:00 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 23:07:00 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 23:07:00 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 23:07:00 INFO YarnClientSchedulerBackend: Stopped
20/03/18 23:07:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 23:07:00 INFO MemoryStore: MemoryStore cleared
20/03/18 23:07:00 INFO BlockManager: BlockManager stopped
20/03/18 23:07:00 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 23:07:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 23:07:00 INFO SparkContext: Successfully stopped SparkContext
20/03/18 23:07:00 INFO ShutdownHookManager: Shutdown hook called
20/03/18 23:07:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-86f8f5ab-9cf0-4751-b418-45a8d215ad55
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584544021
+ DIFF_TIME=58
+ echo 'END_TIME: ' 1584544021
+ echo 'Total time taken: ' 58
+ attempt_num=12
+ sleep 30m
+ '[' 12 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584543996
++ date +%s
+ START_TIME=1584545821
+ echo 1584545821
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584543996 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/18 23:37:02 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/18 23:37:03 INFO SparkContext: Submitted application: CoolLogtest
20/03/18 23:37:03 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 23:37:03 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 23:37:03 INFO SecurityManager: Changing view acls groups to: 
20/03/18 23:37:03 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 23:37:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 23:37:03 INFO Utils: Successfully started service 'sparkDriver' on port 44190.
20/03/18 23:37:03 INFO SparkEnv: Registering MapOutputTracker
20/03/18 23:37:03 INFO SparkEnv: Registering BlockManagerMaster
20/03/18 23:37:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/18 23:37:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/18 23:37:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5e48a38c-c528-44d8-a087-a7d378010c01
20/03/18 23:37:03 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/18 23:37:03 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/18 23:37:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/18 23:37:04 INFO Utils: Successfully started service 'SparkUI' on port 4041.
20/03/18 23:37:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4041
20/03/18 23:37:04 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:44190/jars/CoolPocTest.jar with timestamp 1584545824082
20/03/18 23:37:05 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/18 23:37:05 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/18 23:37:05 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/18 23:37:05 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/18 23:37:05 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/18 23:37:05 INFO Client: Setting up container launch context for our AM
20/03/18 23:37:05 INFO Client: Setting up the launch environment for our AM container
20/03/18 23:37:05 INFO Client: Preparing resources for our AM container
20/03/18 23:37:05 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/18 23:37:05 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7084104 for 1619795 on ha-hdfs:nnscbhaastest
20/03/18 23:37:06 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 23:37:06 INFO metastore: Connected to metastore.
20/03/18 23:37:23 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ee 4a c7 1b 8a 01 71 12 57 4b 1b 8e 04 05 8e 02 7a
20/03/18 23:37:23 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 23:37:23 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/18 23:37:23 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73718/datanucleus-api-jdo-3.2.6.jar
20/03/18 23:37:23 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73718/datanucleus-core-3.2.10.jar
20/03/18 23:37:23 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73718/datanucleus-rdbms-3.2.9.jar
20/03/18 23:37:23 INFO Client: Uploading resource file:/tmp/spark-0cda775d-18fe-4329-ac50-3523320aa5ee/__spark_conf__6599244224876716766.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73718/__spark_conf__.zip
20/03/18 23:37:23 INFO SecurityManager: Changing view acls to: 1619795
20/03/18 23:37:23 INFO SecurityManager: Changing modify acls to: 1619795
20/03/18 23:37:23 INFO SecurityManager: Changing view acls groups to: 
20/03/18 23:37:23 INFO SecurityManager: Changing modify acls groups to: 
20/03/18 23:37:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/18 23:37:23 INFO Client: Submitting application application_1583994958990_73718 to ResourceManager
20/03/18 23:37:23 INFO YarnClientImpl: Submitted application application_1583994958990_73718
20/03/18 23:37:23 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_73718 and attemptId None
20/03/18 23:37:24 INFO Client: Application report for application_1583994958990_73718 (state: ACCEPTED)
20/03/18 23:37:24 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584545843644
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73718/
	 user: 1619795
20/03/18 23:37:25 INFO Client: Application report for application_1583994958990_73718 (state: ACCEPTED)
20/03/18 23:37:26 INFO Client: Application report for application_1583994958990_73718 (state: ACCEPTED)
20/03/18 23:37:27 INFO Client: Application report for application_1583994958990_73718 (state: ACCEPTED)
20/03/18 23:37:28 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_73718,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73718), /proxy/application_1583994958990_73718
20/03/18 23:37:28 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/18 23:37:28 INFO Client: Application report for application_1583994958990_73718 (state: ACCEPTED)
20/03/18 23:37:29 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/18 23:37:29 INFO Client: Application report for application_1583994958990_73718 (state: RUNNING)
20/03/18 23:37:29 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.20.174.138
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584545843644
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73718/
	 user: 1619795
20/03/18 23:37:29 INFO YarnClientSchedulerBackend: Application application_1583994958990_73718 has started running.
20/03/18 23:37:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46153.
20/03/18 23:37:29 INFO NettyBlockTransferService: Server created on 10.20.174.137:46153
20/03/18 23:37:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/18 23:37:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 46153, None)
20/03/18 23:37:29 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:46153 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 46153, None)
20/03/18 23:37:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 46153, None)
20/03/18 23:37:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 46153, None)
20/03/18 23:37:30 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_73718
20/03/18 23:37:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.49:48214) with ID 1
20/03/18 23:37:34 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/03/18 23:37:34 INFO BlockManagerMasterEndpoint: Registering block manager hklpadhas013.global.standardchartered.com:43392 with 912.3 MB RAM, BlockManagerId(1, hklpadhas013.global.standardchartered.com, 43392, None)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584543996
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584543996
*************************************** CURRENT TIME |1584545854|************************
20/03/18 23:37:34 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/18 23:37:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/18 23:37:34 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/18 23:37:34 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/18 23:37:35 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/18 23:37:35 INFO metastore: Connected to metastore.
20/03/18 23:37:52 INFO SessionState: Created local directory: /tmp/7b5a41f6-a93b-448b-8999-83974a80ad5c_resources
20/03/18 23:37:52 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/7b5a41f6-a93b-448b-8999-83974a80ad5c
20/03/18 23:37:52 INFO SessionState: Created local directory: /tmp/1619795/7b5a41f6-a93b-448b-8999-83974a80ad5c
20/03/18 23:37:52 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/7b5a41f6-a93b-448b-8999-83974a80ad5c/_tmp_space.db
20/03/18 23:37:52 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 23:37:53 INFO SessionState: Created local directory: /tmp/766ffda1-6026-4f80-b35e-a647b5da688e_resources
20/03/18 23:37:53 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/766ffda1-6026-4f80-b35e-a647b5da688e
20/03/18 23:37:53 INFO SessionState: Created local directory: /tmp/1619795/766ffda1-6026-4f80-b35e-a647b5da688e
20/03/18 23:37:53 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/766ffda1-6026-4f80-b35e-a647b5da688e/_tmp_space.db
20/03/18 23:37:53 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/18 23:37:53 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/18 23:37:53 INFO CodeGenerator: Code generated in 158.425212 ms
20/03/18 23:37:54 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/18 23:37:54 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/18 23:37:54 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/18 23:37:54 INFO DAGScheduler: Parents of final stage: List()
20/03/18 23:37:54 INFO DAGScheduler: Missing parents: List()
20/03/18 23:37:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/18 23:37:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 23:37:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 23:37:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:46153 (size: 5.0 KB, free: 366.3 MB)
20/03/18 23:37:54 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/18 23:37:54 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 23:37:54 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/18 23:37:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 23:37:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpadhas013.global.standardchartered.com:43392 (size: 5.0 KB, free: 912.3 MB)
20/03/18 23:37:54 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpadhas013.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 23:37:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 609 ms on hklpadhas013.global.standardchartered.com (executor 1) (1/4)
20/03/18 23:37:54 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpadhas013.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 23:37:54 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 21 ms on hklpadhas013.global.standardchartered.com (executor 1) (2/4)
20/03/18 23:37:54 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpadhas013.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 70288 bytes)
20/03/18 23:37:54 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 13 ms on hklpadhas013.global.standardchartered.com (executor 1) (3/4)
20/03/18 23:37:55 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 238 ms on hklpadhas013.global.standardchartered.com (executor 1) (4/4)
20/03/18 23:37:55 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/18 23:37:55 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.876 s
20/03/18 23:37:55 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.051439 s
20/03/18 23:37:55 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/18 23:37:55 INFO ContextCleaner: Cleaned accumulator 0
20/03/18 23:37:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:46153 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 23:37:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpadhas013.global.standardchartered.com:43392 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 23:37:55 INFO ContextCleaner: Cleaned accumulator 1
20/03/18 23:37:55 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/18 23:37:55 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/18 23:37:55 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/18 23:37:55 INFO DAGScheduler: Parents of final stage: List()
20/03/18 23:37:55 INFO DAGScheduler: Missing parents: List()
20/03/18 23:37:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/18 23:37:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/18 23:37:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/18 23:37:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:46153 (size: 5.0 KB, free: 366.3 MB)
20/03/18 23:37:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/18 23:37:55 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/18 23:37:55 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/18 23:37:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/18 23:37:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpadhas013.global.standardchartered.com:43392 (size: 5.0 KB, free: 912.3 MB)
20/03/18 23:37:55 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpadhas013.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/18 23:37:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 23 ms on hklpadhas013.global.standardchartered.com (executor 1) (1/4)
20/03/18 23:37:55 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpadhas013.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/18 23:37:55 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 8 ms on hklpadhas013.global.standardchartered.com (executor 1) (2/4)
20/03/18 23:37:55 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpadhas013.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 70288 bytes)
20/03/18 23:37:55 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpadhas013.global.standardchartered.com (executor 1) (3/4)
20/03/18 23:37:55 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 20 ms on hklpadhas013.global.standardchartered.com (executor 1) (4/4)
20/03/18 23:37:55 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/18 23:37:55 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.058 s
20/03/18 23:37:55 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.066269 s
20/03/18 23:37:55 INFO ContextCleaner: Cleaned accumulator 52
20/03/18 23:37:55 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:46153 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/18 23:37:55 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpadhas013.global.standardchartered.com:43392 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/18 23:37:55 INFO ContextCleaner: Cleaned accumulator 51
20/03/18 23:37:56 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_23-37-56_248_5648470597548918015-1
20/03/18 23:37:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/18 23:37:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/18 23:37:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/18 23:37:56 INFO CodeGenerator: Code generated in 60.629849 ms
20/03/18 23:37:56 INFO CodeGenerator: Code generated in 39.026808 ms
20/03/18 23:37:56 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/18 23:37:56 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/18 23:37:56 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/18 23:37:56 INFO DAGScheduler: Parents of final stage: List()
20/03/18 23:37:56 INFO DAGScheduler: Missing parents: List()
20/03/18 23:37:56 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/18 23:37:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/18 23:37:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/18 23:37:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:46153 (size: 114.9 KB, free: 366.2 MB)
20/03/18 23:37:56 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/18 23:37:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/18 23:37:56 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/18 23:37:56 WARN TaskSetManager: Stage 2 contains a task of very large size (133 KB). The maximum recommended task size is 100 KB.
20/03/18 23:37:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 136477 bytes)
20/03/18 23:37:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpadhas013.global.standardchartered.com:43392 (size: 114.9 KB, free: 912.2 MB)
20/03/18 23:37:58 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2136 ms on hklpadhas013.global.standardchartered.com (executor 1) (1/1)
20/03/18 23:37:58 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/18 23:37:58 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.137 s
20/03/18 23:37:58 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.214227 s
20/03/18 23:37:58 INFO FileFormatWriter: Job null committed.
20/03/18 23:37:59 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/18 23:37:59 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_23-37-56_248_5648470597548918015-1/-ext-10000/ods=2020-03-18/part-00000-bc25ba71-c823-4614-a686-f766589a565c.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-18/part-00000-bc25ba71-c823-4614-a686-f766589a565c.c000, Status:true
20/03/18 23:37:59 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-18_23-37-56_248_5648470597548918015-1/-ext-10000/ods=2020-03-18 with partSpec {ods=2020-03-18}
20/03/18 23:37:59 INFO SparkContext: Invoking stop() from shutdown hook
20/03/18 23:37:59 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4041
20/03/18 23:37:59 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/18 23:37:59 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/18 23:37:59 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/18 23:37:59 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/18 23:37:59 INFO YarnClientSchedulerBackend: Stopped
20/03/18 23:37:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/18 23:37:59 INFO MemoryStore: MemoryStore cleared
20/03/18 23:37:59 INFO BlockManager: BlockManager stopped
20/03/18 23:37:59 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/18 23:37:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/18 23:37:59 INFO SparkContext: Successfully stopped SparkContext
20/03/18 23:37:59 INFO ShutdownHookManager: Shutdown hook called
20/03/18 23:37:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-0cda775d-18fe-4329-ac50-3523320aa5ee
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584545879
+ DIFF_TIME=58
+ echo 'END_TIME: ' 1584545879
+ echo 'Total time taken: ' 58
+ attempt_num=13
+ sleep 30m
+ '[' 13 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584545854
++ date +%s
+ START_TIME=1584547679
+ echo 1584547679
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584545854 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 00:08:00 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 00:08:01 INFO SparkContext: Submitted application: CoolLogtest
20/03/19 00:08:01 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 00:08:01 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 00:08:01 INFO SecurityManager: Changing view acls groups to: 
20/03/19 00:08:01 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 00:08:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 00:08:02 INFO Utils: Successfully started service 'sparkDriver' on port 42611.
20/03/19 00:08:02 INFO SparkEnv: Registering MapOutputTracker
20/03/19 00:08:02 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 00:08:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 00:08:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 00:08:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1a4a4134-02af-4ece-8f10-d00daad4779b
20/03/19 00:08:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 00:08:02 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 00:08:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 00:08:02 INFO Utils: Successfully started service 'SparkUI' on port 4041.
20/03/19 00:08:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4041
20/03/19 00:08:02 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:42611/jars/CoolPocTest.jar with timestamp 1584547682563
20/03/19 00:08:03 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 00:08:03 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 00:08:03 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 00:08:03 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 00:08:03 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 00:08:03 INFO Client: Setting up container launch context for our AM
20/03/19 00:08:03 INFO Client: Setting up the launch environment for our AM container
20/03/19 00:08:03 INFO Client: Preparing resources for our AM container
20/03/19 00:08:03 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 00:08:03 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7085769 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 00:08:05 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 00:08:05 INFO metastore: Connected to metastore.
20/03/19 00:08:20 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ee 67 1f 47 8a 01 71 12 73 a3 47 8e 04 09 8e 02 7a
20/03/19 00:08:20 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 00:08:20 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 00:08:21 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73879/datanucleus-api-jdo-3.2.6.jar
20/03/19 00:08:21 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73879/datanucleus-core-3.2.10.jar
20/03/19 00:08:50 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73879/datanucleus-rdbms-3.2.9.jar
20/03/19 00:08:51 INFO Client: Uploading resource file:/tmp/spark-a45991ff-c5e9-4304-9c71-eb53d2df8f2b/__spark_conf__1798837664935456105.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_73879/__spark_conf__.zip
20/03/19 00:08:51 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 00:08:51 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 00:08:51 INFO SecurityManager: Changing view acls groups to: 
20/03/19 00:08:51 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 00:08:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 00:08:51 INFO Client: Submitting application application_1583994958990_73879 to ResourceManager
20/03/19 00:08:51 INFO YarnClientImpl: Submitted application application_1583994958990_73879
20/03/19 00:08:51 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_73879 and attemptId None
20/03/19 00:08:52 INFO Client: Application report for application_1583994958990_73879 (state: ACCEPTED)
20/03/19 00:08:52 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584547731288
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73879/
	 user: 1619795
20/03/19 00:08:53 INFO Client: Application report for application_1583994958990_73879 (state: ACCEPTED)
20/03/19 00:08:54 INFO Client: Application report for application_1583994958990_73879 (state: ACCEPTED)
20/03/19 00:08:55 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_73879,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73879), /proxy/application_1583994958990_73879
20/03/19 00:08:55 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 00:08:55 INFO Client: Application report for application_1583994958990_73879 (state: ACCEPTED)
20/03/19 00:08:55 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 00:08:56 INFO Client: Application report for application_1583994958990_73879 (state: RUNNING)
20/03/19 00:08:56 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.59
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584547731288
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_73879/
	 user: 1619795
20/03/19 00:08:56 INFO YarnClientSchedulerBackend: Application application_1583994958990_73879 has started running.
20/03/19 00:08:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38356.
20/03/19 00:08:56 INFO NettyBlockTransferService: Server created on 10.20.174.137:38356
20/03/19 00:08:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 00:08:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 38356, None)
20/03/19 00:08:56 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:38356 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 38356, None)
20/03/19 00:08:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 38356, None)
20/03/19 00:08:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 38356, None)
20/03/19 00:08:56 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_73879
20/03/19 00:08:56 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584545854
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584545854
*************************************** CURRENT TIME |1584547737|************************
20/03/19 00:08:57 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/19 00:08:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/19 00:08:57 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/19 00:08:57 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/19 00:08:57 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 00:08:57 INFO metastore: Connected to metastore.
20/03/19 00:09:00 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.22:45568) with ID 1
20/03/19 00:09:00 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas015.global.standardchartered.com:45773 with 912.3 MB RAM, BlockManagerId(1, hklpathas015.global.standardchartered.com, 45773, None)
20/03/19 00:09:15 INFO SessionState: Created local directory: /tmp/8894fa92-e09a-4719-ae6f-1279170e5707_resources
20/03/19 00:09:15 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/8894fa92-e09a-4719-ae6f-1279170e5707
20/03/19 00:09:15 INFO SessionState: Created local directory: /tmp/1619795/8894fa92-e09a-4719-ae6f-1279170e5707
20/03/19 00:09:15 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/8894fa92-e09a-4719-ae6f-1279170e5707/_tmp_space.db
20/03/19 00:09:15 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 00:09:15 INFO SessionState: Created local directory: /tmp/55fba089-97c6-468e-bdcf-b884f6bf91f7_resources
20/03/19 00:09:15 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/55fba089-97c6-468e-bdcf-b884f6bf91f7
20/03/19 00:09:15 INFO SessionState: Created local directory: /tmp/1619795/55fba089-97c6-468e-bdcf-b884f6bf91f7
20/03/19 00:09:15 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/55fba089-97c6-468e-bdcf-b884f6bf91f7/_tmp_space.db
20/03/19 00:09:15 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 00:09:15 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/19 00:09:16 INFO CodeGenerator: Code generated in 141.817709 ms
20/03/19 00:09:16 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/19 00:09:16 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/19 00:09:16 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/19 00:09:16 INFO DAGScheduler: Parents of final stage: List()
20/03/19 00:09:16 INFO DAGScheduler: Missing parents: List()
20/03/19 00:09:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/19 00:09:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 00:09:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 00:09:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:38356 (size: 5.0 KB, free: 366.3 MB)
20/03/19 00:09:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/19 00:09:16 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 00:09:16 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/19 00:09:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas015.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 00:09:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas015.global.standardchartered.com:45773 (size: 5.0 KB, free: 912.3 MB)
20/03/19 00:09:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas015.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 00:09:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 697 ms on hklpathas015.global.standardchartered.com (executor 1) (1/4)
20/03/19 00:09:17 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas015.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 00:09:17 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 23 ms on hklpathas015.global.standardchartered.com (executor 1) (2/4)
20/03/19 00:09:17 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas015.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71812 bytes)
20/03/19 00:09:17 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 13 ms on hklpathas015.global.standardchartered.com (executor 1) (3/4)
20/03/19 00:09:17 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 126 ms on hklpathas015.global.standardchartered.com (executor 1) (4/4)
20/03/19 00:09:17 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 00:09:17 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.853 s
20/03/19 00:09:17 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.008998 s
20/03/19 00:09:17 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/19 00:09:17 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/19 00:09:17 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/19 00:09:17 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/19 00:09:17 INFO DAGScheduler: Parents of final stage: List()
20/03/19 00:09:17 INFO DAGScheduler: Missing parents: List()
20/03/19 00:09:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/19 00:09:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 00:09:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 00:09:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:38356 (size: 5.0 KB, free: 366.3 MB)
20/03/19 00:09:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 00:09:17 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 00:09:17 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/19 00:09:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas015.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 00:09:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas015.global.standardchartered.com:45773 (size: 5.0 KB, free: 912.3 MB)
20/03/19 00:09:17 INFO ContextCleaner: Cleaned accumulator 0
20/03/19 00:09:17 INFO ContextCleaner: Cleaned accumulator 1
20/03/19 00:09:17 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas015.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 00:09:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 164 ms on hklpathas015.global.standardchartered.com (executor 1) (1/4)
20/03/19 00:09:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:38356 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 00:09:17 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas015.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 00:09:17 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 16 ms on hklpathas015.global.standardchartered.com (executor 1) (2/4)
20/03/19 00:09:17 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas015.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71812 bytes)
20/03/19 00:09:17 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 8 ms on hklpathas015.global.standardchartered.com (executor 1) (3/4)
20/03/19 00:09:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas015.global.standardchartered.com:45773 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 00:09:17 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 18 ms on hklpathas015.global.standardchartered.com (executor 1) (4/4)
20/03/19 00:09:17 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/19 00:09:17 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.202 s
20/03/19 00:09:17 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.210291 s
20/03/19 00:09:18 INFO ContextCleaner: Cleaned accumulator 52
20/03/19 00:09:18 INFO ContextCleaner: Cleaned accumulator 51
20/03/19 00:09:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:38356 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 00:09:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas015.global.standardchartered.com:45773 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 00:09:18 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_00-09-18_511_8209382031821491133-1
20/03/19 00:09:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/19 00:09:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/19 00:09:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/19 00:09:18 INFO CodeGenerator: Code generated in 58.926966 ms
20/03/19 00:09:18 INFO CodeGenerator: Code generated in 39.934705 ms
20/03/19 00:09:18 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/19 00:09:18 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/19 00:09:18 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/19 00:09:18 INFO DAGScheduler: Parents of final stage: List()
20/03/19 00:09:18 INFO DAGScheduler: Missing parents: List()
20/03/19 00:09:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/19 00:09:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/19 00:09:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 115.0 KB, free 365.9 MB)
20/03/19 00:09:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:38356 (size: 115.0 KB, free: 366.2 MB)
20/03/19 00:09:19 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/19 00:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/19 00:09:19 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/19 00:09:19 WARN TaskSetManager: Stage 2 contains a task of very large size (136 KB). The maximum recommended task size is 100 KB.
20/03/19 00:09:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas015.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 139525 bytes)
20/03/19 00:09:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas015.global.standardchartered.com:45773 (size: 115.0 KB, free: 912.2 MB)
20/03/19 00:09:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 1988 ms on hklpathas015.global.standardchartered.com (executor 1) (1/1)
20/03/19 00:09:21 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/19 00:09:21 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 1.989 s
20/03/19 00:09:21 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.086853 s
20/03/19 00:09:21 INFO FileFormatWriter: Job null committed.
20/03/19 00:09:21 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19
20/03/19 00:09:21 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/19 00:09:21 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_00-09-18_511_8209382031821491133-1/-ext-10000/ods=2020-03-19/part-00000-00814583-8ef9-47dc-b4a4-9c7cd1df1ea3.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19/part-00000-00814583-8ef9-47dc-b4a4-9c7cd1df1ea3.c000, Status:true
20/03/19 00:09:21 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_00-09-18_511_8209382031821491133-1/-ext-10000/ods=2020-03-19 with partSpec {ods=2020-03-19}
20/03/19 00:09:21 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 00:09:21 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4041
20/03/19 00:09:21 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 00:09:21 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 00:09:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 00:09:21 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 00:09:21 INFO YarnClientSchedulerBackend: Stopped
20/03/19 00:09:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 00:09:21 INFO MemoryStore: MemoryStore cleared
20/03/19 00:09:21 INFO BlockManager: BlockManager stopped
20/03/19 00:09:21 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 00:09:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 00:09:21 INFO SparkContext: Successfully stopped SparkContext
20/03/19 00:09:21 INFO ShutdownHookManager: Shutdown hook called
20/03/19 00:09:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-a45991ff-c5e9-4304-9c71-eb53d2df8f2b
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584547762
+ DIFF_TIME=83
+ echo 'END_TIME: ' 1584547762
+ echo 'Total time taken: ' 83
+ attempt_num=14
+ sleep 30m
+ '[' 14 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584547737
++ date +%s
+ START_TIME=1584549562
+ echo 1584549562
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584547737 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 00:39:22 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 00:39:23 INFO SparkContext: Submitted application: CoolLogtest
20/03/19 00:39:23 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 00:39:23 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 00:39:23 INFO SecurityManager: Changing view acls groups to: 
20/03/19 00:39:23 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 00:39:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 00:39:24 INFO Utils: Successfully started service 'sparkDriver' on port 44328.
20/03/19 00:39:24 INFO SparkEnv: Registering MapOutputTracker
20/03/19 00:39:24 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 00:39:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 00:39:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 00:39:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-08a4aca0-5798-48f7-b2a4-d41df92fbefe
20/03/19 00:39:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 00:39:24 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 00:39:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 00:39:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/19 00:39:24 INFO Utils: Successfully started service 'SparkUI' on port 4042.
20/03/19 00:39:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4042
20/03/19 00:39:24 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:44328/jars/CoolPocTest.jar with timestamp 1584549564650
20/03/19 00:39:25 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 00:39:25 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 00:39:25 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 00:39:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 00:39:25 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 00:39:25 INFO Client: Setting up container launch context for our AM
20/03/19 00:39:25 INFO Client: Setting up the launch environment for our AM container
20/03/19 00:39:25 INFO Client: Preparing resources for our AM container
20/03/19 00:39:25 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 00:39:25 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7087232 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 00:39:27 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 00:39:27 INFO metastore: Connected to metastore.
20/03/19 00:39:44 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ee 83 dd da 8a 01 71 12 90 61 da 8e 04 2d 8e 02 7a
20/03/19 00:39:44 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 00:39:44 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 00:39:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74311/datanucleus-api-jdo-3.2.6.jar
20/03/19 00:39:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74311/datanucleus-core-3.2.10.jar
20/03/19 00:39:44 WARN DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-2100300935-10.20.174.135-1476178117288:blk_1610564026_544171149
java.io.IOException: Bad response ERROR for block BP-2100300935-10.20.174.135-1476178117288:blk_1610564026_544171149 from datanode DatanodeInfoWithStorage[10.23.142.46:1019,DS-3493ed14-1472-4ba0-9b41-c560ac8db6ef,DISK]
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:887)
20/03/19 00:39:44 WARN DFSClient: Error Recovery for block BP-2100300935-10.20.174.135-1476178117288:blk_1610564026_544171149 in pipeline DatanodeInfoWithStorage[10.23.225.59:1019,DS-2d99e644-76e4-40c5-9dd9-984c05606af3,DISK], DatanodeInfoWithStorage[10.23.225.23:1019,DS-434b13a1-f15d-4ae8-a0b7-8c59e9d051ea,DISK], DatanodeInfoWithStorage[10.23.142.46:1019,DS-3493ed14-1472-4ba0-9b41-c560ac8db6ef,DISK]: bad datanode DatanodeInfoWithStorage[10.23.142.46:1019,DS-3493ed14-1472-4ba0-9b41-c560ac8db6ef,DISK]
20/03/19 00:39:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74311/datanucleus-rdbms-3.2.9.jar
20/03/19 00:39:45 INFO Client: Uploading resource file:/tmp/spark-e2e9572b-268e-4cbe-8497-56bf65d65961/__spark_conf__4877687790534042971.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74311/__spark_conf__.zip
20/03/19 00:39:45 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 00:39:45 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 00:39:45 INFO SecurityManager: Changing view acls groups to: 
20/03/19 00:39:45 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 00:39:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 00:39:45 INFO Client: Submitting application application_1583994958990_74311 to ResourceManager
20/03/19 00:39:45 INFO YarnClientImpl: Submitted application application_1583994958990_74311
20/03/19 00:39:45 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_74311 and attemptId None
20/03/19 00:39:46 INFO Client: Application report for application_1583994958990_74311 (state: ACCEPTED)
20/03/19 00:39:46 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584549585102
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74311/
	 user: 1619795
20/03/19 00:39:47 INFO Client: Application report for application_1583994958990_74311 (state: ACCEPTED)
20/03/19 00:39:48 INFO Client: Application report for application_1583994958990_74311 (state: ACCEPTED)
20/03/19 00:39:49 INFO Client: Application report for application_1583994958990_74311 (state: ACCEPTED)
20/03/19 00:39:50 INFO Client: Application report for application_1583994958990_74311 (state: ACCEPTED)
20/03/19 00:39:50 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_74311,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74311), /proxy/application_1583994958990_74311
20/03/19 00:39:50 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 00:39:50 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 00:39:51 INFO Client: Application report for application_1583994958990_74311 (state: RUNNING)
20/03/19 00:39:51 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.23
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584549585102
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74311/
	 user: 1619795
20/03/19 00:39:51 INFO YarnClientSchedulerBackend: Application application_1583994958990_74311 has started running.
20/03/19 00:39:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44764.
20/03/19 00:39:51 INFO NettyBlockTransferService: Server created on 10.20.174.137:44764
20/03/19 00:39:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 00:39:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 44764, None)
20/03/19 00:39:51 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:44764 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 44764, None)
20/03/19 00:39:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 44764, None)
20/03/19 00:39:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 44764, None)
20/03/19 00:39:51 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_74311
20/03/19 00:39:54 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584547737
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584547737
*************************************** CURRENT TIME |1584549595|************************
20/03/19 00:39:55 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/19 00:39:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/19 00:39:55 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/19 00:39:55 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/19 00:39:56 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 00:39:56 INFO metastore: Connected to metastore.
20/03/19 00:39:56 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.49:56302) with ID 1
20/03/19 00:39:56 INFO BlockManagerMasterEndpoint: Registering block manager hklpadhas013.global.standardchartered.com:46421 with 912.3 MB RAM, BlockManagerId(1, hklpadhas013.global.standardchartered.com, 46421, None)
20/03/19 00:40:13 INFO SessionState: Created local directory: /tmp/00db8634-09fa-4b5a-b43a-6a6b1c33fcd9_resources
20/03/19 00:40:13 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/00db8634-09fa-4b5a-b43a-6a6b1c33fcd9
20/03/19 00:40:13 INFO SessionState: Created local directory: /tmp/1619795/00db8634-09fa-4b5a-b43a-6a6b1c33fcd9
20/03/19 00:40:13 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/00db8634-09fa-4b5a-b43a-6a6b1c33fcd9/_tmp_space.db
20/03/19 00:40:13 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 00:40:14 INFO SessionState: Created local directory: /tmp/ee212a9f-5216-4036-b491-a8672331b320_resources
20/03/19 00:40:14 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/ee212a9f-5216-4036-b491-a8672331b320
20/03/19 00:40:14 INFO SessionState: Created local directory: /tmp/1619795/ee212a9f-5216-4036-b491-a8672331b320
20/03/19 00:40:14 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/ee212a9f-5216-4036-b491-a8672331b320/_tmp_space.db
20/03/19 00:40:14 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 00:40:14 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/19 00:40:15 INFO CodeGenerator: Code generated in 158.616882 ms
20/03/19 00:40:15 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/19 00:40:15 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/19 00:40:15 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/19 00:40:15 INFO DAGScheduler: Parents of final stage: List()
20/03/19 00:40:15 INFO DAGScheduler: Missing parents: List()
20/03/19 00:40:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/19 00:40:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 00:40:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 00:40:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:44764 (size: 5.0 KB, free: 366.3 MB)
20/03/19 00:40:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/19 00:40:15 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 00:40:15 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/19 00:40:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 00:40:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpadhas013.global.standardchartered.com:46421 (size: 5.0 KB, free: 912.3 MB)
20/03/19 00:40:16 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpadhas013.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 00:40:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 707 ms on hklpadhas013.global.standardchartered.com (executor 1) (1/4)
20/03/19 00:40:16 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpadhas013.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 00:40:16 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 22 ms on hklpadhas013.global.standardchartered.com (executor 1) (2/4)
20/03/19 00:40:16 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpadhas013.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71813 bytes)
20/03/19 00:40:16 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 17 ms on hklpadhas013.global.standardchartered.com (executor 1) (3/4)
20/03/19 00:40:16 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 209 ms on hklpadhas013.global.standardchartered.com (executor 1) (4/4)
20/03/19 00:40:16 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 00:40:16 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.949 s
20/03/19 00:40:16 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.141080 s
20/03/19 00:40:16 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/19 00:40:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:44764 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 00:40:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpadhas013.global.standardchartered.com:46421 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 00:40:16 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/19 00:40:16 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/19 00:40:16 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/19 00:40:16 INFO DAGScheduler: Parents of final stage: List()
20/03/19 00:40:16 INFO DAGScheduler: Missing parents: List()
20/03/19 00:40:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/19 00:40:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 00:40:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 00:40:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:44764 (size: 5.0 KB, free: 366.3 MB)
20/03/19 00:40:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 00:40:16 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 00:40:16 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/19 00:40:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 00:40:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpadhas013.global.standardchartered.com:46421 (size: 5.0 KB, free: 912.3 MB)
20/03/19 00:40:16 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpadhas013.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 00:40:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 25 ms on hklpadhas013.global.standardchartered.com (executor 1) (1/4)
20/03/19 00:40:16 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpadhas013.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 00:40:16 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 10 ms on hklpadhas013.global.standardchartered.com (executor 1) (2/4)
20/03/19 00:40:16 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpadhas013.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71813 bytes)
20/03/19 00:40:16 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpadhas013.global.standardchartered.com (executor 1) (3/4)
20/03/19 00:40:16 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 18 ms on hklpadhas013.global.standardchartered.com (executor 1) (4/4)
20/03/19 00:40:16 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/19 00:40:16 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.060 s
20/03/19 00:40:16 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.068448 s
20/03/19 00:40:17 INFO ContextCleaner: Cleaned accumulator 1
20/03/19 00:40:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:44764 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 00:40:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpadhas013.global.standardchartered.com:46421 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 00:40:17 INFO ContextCleaner: Cleaned accumulator 51
20/03/19 00:40:17 INFO ContextCleaner: Cleaned accumulator 52
20/03/19 00:40:17 INFO ContextCleaner: Cleaned accumulator 0
20/03/19 00:40:17 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_00-40-17_543_5819196957025595281-1
20/03/19 00:40:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/19 00:40:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/19 00:40:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/19 00:40:17 INFO CodeGenerator: Code generated in 65.706695 ms
20/03/19 00:40:17 INFO CodeGenerator: Code generated in 41.769966 ms
20/03/19 00:40:18 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/19 00:40:18 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/19 00:40:18 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/19 00:40:18 INFO DAGScheduler: Parents of final stage: List()
20/03/19 00:40:18 INFO DAGScheduler: Missing parents: List()
20/03/19 00:40:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/19 00:40:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/19 00:40:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/19 00:40:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:44764 (size: 114.9 KB, free: 366.2 MB)
20/03/19 00:40:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/19 00:40:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/19 00:40:18 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/19 00:40:18 WARN TaskSetManager: Stage 2 contains a task of very large size (136 KB). The maximum recommended task size is 100 KB.
20/03/19 00:40:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 139527 bytes)
20/03/19 00:40:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpadhas013.global.standardchartered.com:46421 (size: 114.9 KB, free: 912.2 MB)
20/03/19 00:40:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2729 ms on hklpadhas013.global.standardchartered.com (executor 1) (1/1)
20/03/19 00:40:20 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/19 00:40:20 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.731 s
20/03/19 00:40:20 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.809752 s
20/03/19 00:40:20 INFO FileFormatWriter: Job null committed.
20/03/19 00:40:21 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/19 00:40:21 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_00-40-17_543_5819196957025595281-1/-ext-10000/ods=2020-03-19/part-00000-8bf11f74-315a-40ac-9a71-ea4e6e24ae90.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19/part-00000-8bf11f74-315a-40ac-9a71-ea4e6e24ae90.c000, Status:true
20/03/19 00:40:21 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_00-40-17_543_5819196957025595281-1/-ext-10000/ods=2020-03-19 with partSpec {ods=2020-03-19}
20/03/19 00:40:21 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 00:40:21 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4042
20/03/19 00:40:21 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 00:40:21 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 00:40:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 00:40:21 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 00:40:21 INFO YarnClientSchedulerBackend: Stopped
20/03/19 00:40:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 00:40:21 INFO MemoryStore: MemoryStore cleared
20/03/19 00:40:21 INFO BlockManager: BlockManager stopped
20/03/19 00:40:21 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 00:40:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 00:40:21 INFO SparkContext: Successfully stopped SparkContext
20/03/19 00:40:21 INFO ShutdownHookManager: Shutdown hook called
20/03/19 00:40:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-e2e9572b-268e-4cbe-8497-56bf65d65961
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584549621
+ DIFF_TIME=59
+ echo 'END_TIME: ' 1584549621
+ echo 'Total time taken: ' 59
+ attempt_num=15
+ sleep 30m
+ '[' 15 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584549595
++ date +%s
+ START_TIME=1584551421
+ echo 1584551421
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584549595 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 01:10:22 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 01:10:23 INFO SparkContext: Submitted application: CoolLogtest
20/03/19 01:10:23 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 01:10:23 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 01:10:23 INFO SecurityManager: Changing view acls groups to: 
20/03/19 01:10:23 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 01:10:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 01:10:24 INFO Utils: Successfully started service 'sparkDriver' on port 35442.
20/03/19 01:10:24 INFO SparkEnv: Registering MapOutputTracker
20/03/19 01:10:24 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 01:10:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 01:10:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 01:10:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-44a2e6cf-77ad-4cbf-b352-fc9f7f25e9cb
20/03/19 01:10:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 01:10:24 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 01:10:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 01:10:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/19 01:10:24 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
20/03/19 01:10:24 INFO Utils: Successfully started service 'SparkUI' on port 4043.
20/03/19 01:10:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4043
20/03/19 01:10:24 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:35442/jars/CoolPocTest.jar with timestamp 1584551424623
20/03/19 01:10:25 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 01:10:25 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 01:10:25 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 01:10:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 01:10:25 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 01:10:25 INFO Client: Setting up container launch context for our AM
20/03/19 01:10:25 INFO Client: Setting up the launch environment for our AM container
20/03/19 01:10:25 INFO Client: Preparing resources for our AM container
20/03/19 01:10:25 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 01:10:25 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7090214 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 01:10:27 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 01:10:27 INFO metastore: Connected to metastore.
20/03/19 01:10:44 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ee a0 3f e6 8a 01 71 12 ac c3 e6 8e 04 4d 8e 02 7a
20/03/19 01:10:44 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 01:10:44 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 01:10:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74507/datanucleus-api-jdo-3.2.6.jar
20/03/19 01:10:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74507/datanucleus-core-3.2.10.jar
20/03/19 01:10:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74507/datanucleus-rdbms-3.2.9.jar
20/03/19 01:10:45 INFO Client: Uploading resource file:/tmp/spark-1c1caa08-20fc-4651-aa01-9cfa6bc5e211/__spark_conf__4227746484001373496.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74507/__spark_conf__.zip
20/03/19 01:10:45 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 01:10:45 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 01:10:45 INFO SecurityManager: Changing view acls groups to: 
20/03/19 01:10:45 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 01:10:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 01:10:45 INFO Client: Submitting application application_1583994958990_74507 to ResourceManager
20/03/19 01:10:45 INFO YarnClientImpl: Submitted application application_1583994958990_74507
20/03/19 01:10:45 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_74507 and attemptId None
20/03/19 01:10:46 INFO Client: Application report for application_1583994958990_74507 (state: ACCEPTED)
20/03/19 01:10:46 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584551445107
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74507/
	 user: 1619795
20/03/19 01:10:47 INFO Client: Application report for application_1583994958990_74507 (state: ACCEPTED)
20/03/19 01:10:48 INFO Client: Application report for application_1583994958990_74507 (state: ACCEPTED)
20/03/19 01:10:49 INFO Client: Application report for application_1583994958990_74507 (state: ACCEPTED)
20/03/19 01:10:50 INFO Client: Application report for application_1583994958990_74507 (state: ACCEPTED)
20/03/19 01:10:50 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_74507,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74507), /proxy/application_1583994958990_74507
20/03/19 01:10:50 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 01:10:50 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 01:10:51 INFO Client: Application report for application_1583994958990_74507 (state: RUNNING)
20/03/19 01:10:51 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.47
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584551445107
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74507/
	 user: 1619795
20/03/19 01:10:51 INFO YarnClientSchedulerBackend: Application application_1583994958990_74507 has started running.
20/03/19 01:10:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42849.
20/03/19 01:10:51 INFO NettyBlockTransferService: Server created on 10.20.174.137:42849
20/03/19 01:10:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 01:10:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 42849, None)
20/03/19 01:10:51 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:42849 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 42849, None)
20/03/19 01:10:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 42849, None)
20/03/19 01:10:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 42849, None)
20/03/19 01:10:51 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_74507
20/03/19 01:10:54 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584549595
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584549595
*************************************** CURRENT TIME |1584551455|************************
20/03/19 01:10:55 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/19 01:10:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/19 01:10:55 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/19 01:10:55 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/19 01:10:55 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 01:10:55 INFO metastore: Connected to metastore.
20/03/19 01:10:56 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.142.46:40494) with ID 1
20/03/19 01:10:57 INFO BlockManagerMasterEndpoint: Registering block manager hklpadhaa006.global.standardchartered.com:37401 with 912.3 MB RAM, BlockManagerId(1, hklpadhaa006.global.standardchartered.com, 37401, None)
20/03/19 01:11:13 INFO SessionState: Created local directory: /tmp/3124c00d-8d72-47d8-8974-5ad2bb43aadb_resources
20/03/19 01:11:13 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/3124c00d-8d72-47d8-8974-5ad2bb43aadb
20/03/19 01:11:13 INFO SessionState: Created local directory: /tmp/1619795/3124c00d-8d72-47d8-8974-5ad2bb43aadb
20/03/19 01:11:13 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/3124c00d-8d72-47d8-8974-5ad2bb43aadb/_tmp_space.db
20/03/19 01:11:13 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 01:11:14 INFO SessionState: Created local directory: /tmp/1f446d5d-ee21-4920-82be-187f57216113_resources
20/03/19 01:11:14 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/1f446d5d-ee21-4920-82be-187f57216113
20/03/19 01:11:14 INFO SessionState: Created local directory: /tmp/1619795/1f446d5d-ee21-4920-82be-187f57216113
20/03/19 01:11:14 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/1f446d5d-ee21-4920-82be-187f57216113/_tmp_space.db
20/03/19 01:11:14 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 01:11:14 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/19 01:11:15 INFO CodeGenerator: Code generated in 150.107676 ms
20/03/19 01:11:15 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/19 01:11:15 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/19 01:11:15 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/19 01:11:15 INFO DAGScheduler: Parents of final stage: List()
20/03/19 01:11:15 INFO DAGScheduler: Missing parents: List()
20/03/19 01:11:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/19 01:11:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 01:11:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 01:11:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:42849 (size: 5.0 KB, free: 366.3 MB)
20/03/19 01:11:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/19 01:11:15 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 01:11:15 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/19 01:11:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpadhaa006.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 01:11:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpadhaa006.global.standardchartered.com:37401 (size: 5.0 KB, free: 912.3 MB)
20/03/19 01:11:16 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpadhaa006.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 01:11:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 951 ms on hklpadhaa006.global.standardchartered.com (executor 1) (1/4)
20/03/19 01:11:16 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpadhaa006.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 01:11:16 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 27 ms on hklpadhaa006.global.standardchartered.com (executor 1) (2/4)
20/03/19 01:11:16 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpadhaa006.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71814 bytes)
20/03/19 01:11:16 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 14 ms on hklpadhaa006.global.standardchartered.com (executor 1) (3/4)
20/03/19 01:11:16 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 278 ms on hklpadhaa006.global.standardchartered.com (executor 1) (4/4)
20/03/19 01:11:16 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 01:11:16 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 1.266 s
20/03/19 01:11:16 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.474728 s
20/03/19 01:11:16 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/19 01:11:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:42849 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 01:11:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpadhaa006.global.standardchartered.com:37401 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 01:11:17 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/19 01:11:17 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/19 01:11:17 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/19 01:11:17 INFO DAGScheduler: Parents of final stage: List()
20/03/19 01:11:17 INFO DAGScheduler: Missing parents: List()
20/03/19 01:11:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/19 01:11:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 01:11:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 01:11:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:42849 (size: 5.0 KB, free: 366.3 MB)
20/03/19 01:11:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 01:11:17 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 01:11:17 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/19 01:11:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpadhaa006.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 01:11:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpadhaa006.global.standardchartered.com:37401 (size: 5.0 KB, free: 912.3 MB)
20/03/19 01:11:17 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpadhaa006.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 01:11:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 26 ms on hklpadhaa006.global.standardchartered.com (executor 1) (1/4)
20/03/19 01:11:17 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpadhaa006.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 01:11:17 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 11 ms on hklpadhaa006.global.standardchartered.com (executor 1) (2/4)
20/03/19 01:11:17 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpadhaa006.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71814 bytes)
20/03/19 01:11:17 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 10 ms on hklpadhaa006.global.standardchartered.com (executor 1) (3/4)
20/03/19 01:11:17 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 23 ms on hklpadhaa006.global.standardchartered.com (executor 1) (4/4)
20/03/19 01:11:17 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/19 01:11:17 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.068 s
20/03/19 01:11:17 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.076979 s
20/03/19 01:11:17 INFO ContextCleaner: Cleaned accumulator 1
20/03/19 01:11:17 INFO ContextCleaner: Cleaned accumulator 52
20/03/19 01:11:17 INFO ContextCleaner: Cleaned accumulator 0
20/03/19 01:11:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:42849 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 01:11:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpadhaa006.global.standardchartered.com:37401 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 01:11:17 INFO ContextCleaner: Cleaned accumulator 51
20/03/19 01:11:17 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_01-11-17_740_5700686275290304412-1
20/03/19 01:11:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/19 01:11:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/19 01:11:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/19 01:11:18 INFO CodeGenerator: Code generated in 78.100225 ms
20/03/19 01:11:18 INFO CodeGenerator: Code generated in 46.304279 ms
20/03/19 01:11:18 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/19 01:11:18 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/19 01:11:18 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/19 01:11:18 INFO DAGScheduler: Parents of final stage: List()
20/03/19 01:11:18 INFO DAGScheduler: Missing parents: List()
20/03/19 01:11:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/19 01:11:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/19 01:11:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/19 01:11:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:42849 (size: 114.9 KB, free: 366.2 MB)
20/03/19 01:11:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/19 01:11:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/19 01:11:18 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/19 01:11:18 WARN TaskSetManager: Stage 2 contains a task of very large size (136 KB). The maximum recommended task size is 100 KB.
20/03/19 01:11:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpadhaa006.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 139529 bytes)
20/03/19 01:11:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpadhaa006.global.standardchartered.com:37401 (size: 114.9 KB, free: 912.2 MB)
20/03/19 01:11:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2754 ms on hklpadhaa006.global.standardchartered.com (executor 1) (1/1)
20/03/19 01:11:21 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/19 01:11:21 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.756 s
20/03/19 01:11:21 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.842390 s
20/03/19 01:11:21 INFO FileFormatWriter: Job null committed.
20/03/19 01:11:21 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/19 01:11:21 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_01-11-17_740_5700686275290304412-1/-ext-10000/ods=2020-03-19/part-00000-fe0f126e-24e2-42f3-b16c-77b68e819173.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19/part-00000-fe0f126e-24e2-42f3-b16c-77b68e819173.c000, Status:true
20/03/19 01:11:21 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_01-11-17_740_5700686275290304412-1/-ext-10000/ods=2020-03-19 with partSpec {ods=2020-03-19}
20/03/19 01:11:21 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 01:11:21 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4043
20/03/19 01:11:21 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 01:11:21 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 01:11:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 01:11:21 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 01:11:21 INFO YarnClientSchedulerBackend: Stopped
20/03/19 01:11:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 01:11:21 INFO MemoryStore: MemoryStore cleared
20/03/19 01:11:21 INFO BlockManager: BlockManager stopped
20/03/19 01:11:21 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 01:11:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 01:11:21 INFO SparkContext: Successfully stopped SparkContext
20/03/19 01:11:21 INFO ShutdownHookManager: Shutdown hook called
20/03/19 01:11:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-1c1caa08-20fc-4651-aa01-9cfa6bc5e211
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584551482
+ DIFF_TIME=61
+ echo 'END_TIME: ' 1584551482
+ echo 'Total time taken: ' 61
+ attempt_num=16
+ sleep 30m
+ '[' 16 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584551455
++ date +%s
+ START_TIME=1584553282
+ echo 1584553282
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584551455 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 01:41:23 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 01:41:24 INFO SparkContext: Submitted application: CoolLogtest
20/03/19 01:41:24 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 01:41:24 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 01:41:24 INFO SecurityManager: Changing view acls groups to: 
20/03/19 01:41:24 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 01:41:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 01:41:24 INFO Utils: Successfully started service 'sparkDriver' on port 35300.
20/03/19 01:41:24 INFO SparkEnv: Registering MapOutputTracker
20/03/19 01:41:24 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 01:41:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 01:41:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 01:41:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b3899734-624e-4177-ab93-f53d56ba85e1
20/03/19 01:41:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 01:41:24 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 01:41:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 01:41:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/19 01:41:24 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
20/03/19 01:41:24 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
20/03/19 01:41:24 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
20/03/19 01:41:24 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
20/03/19 01:41:24 INFO Utils: Successfully started service 'SparkUI' on port 4046.
20/03/19 01:41:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4046
20/03/19 01:41:24 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:35300/jars/CoolPocTest.jar with timestamp 1584553284892
20/03/19 01:41:25 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 01:41:26 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 01:41:26 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 01:41:26 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 01:41:26 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 01:41:26 INFO Client: Setting up container launch context for our AM
20/03/19 01:41:26 INFO Client: Setting up the launch environment for our AM container
20/03/19 01:41:26 INFO Client: Preparing resources for our AM container
20/03/19 01:41:26 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 01:41:26 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7090998 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 01:41:27 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 01:41:27 INFO metastore: Connected to metastore.
20/03/19 01:41:44 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ee bc a1 f2 8a 01 71 12 c9 25 f2 8e 04 5f 8e 02 7a
20/03/19 01:41:44 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 01:41:44 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 01:41:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74647/datanucleus-api-jdo-3.2.6.jar
20/03/19 01:41:44 WARN DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-2100300935-10.20.174.135-1476178117288:blk_1610595174_544206988
java.io.IOException: Bad response ERROR for block BP-2100300935-10.20.174.135-1476178117288:blk_1610595174_544206988 from datanode DatanodeInfoWithStorage[10.23.142.44:1019,DS-f26302e9-3580-48a9-9ced-e9c548f8cf38,DISK]
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:887)
20/03/19 01:41:44 WARN DFSClient: Error Recovery for block BP-2100300935-10.20.174.135-1476178117288:blk_1610595174_544206988 in pipeline DatanodeInfoWithStorage[10.23.142.67:1019,DS-5de67fcc-6d39-49a5-a330-980d05a58c41,DISK], DatanodeInfoWithStorage[10.23.142.44:1019,DS-f26302e9-3580-48a9-9ced-e9c548f8cf38,DISK], DatanodeInfoWithStorage[10.23.225.49:1019,DS-983a91f4-3117-4e5e-88f9-ba1a8b37319d,DISK]: bad datanode DatanodeInfoWithStorage[10.23.142.44:1019,DS-f26302e9-3580-48a9-9ced-e9c548f8cf38,DISK]
20/03/19 01:41:45 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74647/datanucleus-core-3.2.10.jar
20/03/19 01:41:45 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74647/datanucleus-rdbms-3.2.9.jar
20/03/19 01:41:45 INFO Client: Uploading resource file:/tmp/spark-6c817d6e-d1a3-4e83-8489-3b7433900de3/__spark_conf__2818018729929866577.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74647/__spark_conf__.zip
20/03/19 01:41:45 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 01:41:45 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 01:41:45 INFO SecurityManager: Changing view acls groups to: 
20/03/19 01:41:45 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 01:41:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 01:41:45 INFO Client: Submitting application application_1583994958990_74647 to ResourceManager
20/03/19 01:41:45 INFO YarnClientImpl: Submitted application application_1583994958990_74647
20/03/19 01:41:45 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_74647 and attemptId None
20/03/19 01:41:46 INFO Client: Application report for application_1583994958990_74647 (state: ACCEPTED)
20/03/19 01:41:46 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584553305287
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74647/
	 user: 1619795
20/03/19 01:41:47 INFO Client: Application report for application_1583994958990_74647 (state: ACCEPTED)
20/03/19 01:41:48 INFO Client: Application report for application_1583994958990_74647 (state: ACCEPTED)
20/03/19 01:41:49 INFO Client: Application report for application_1583994958990_74647 (state: ACCEPTED)
20/03/19 01:41:50 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_74647,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74647), /proxy/application_1583994958990_74647
20/03/19 01:41:50 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 01:41:50 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 01:41:50 INFO Client: Application report for application_1583994958990_74647 (state: RUNNING)
20/03/19 01:41:50 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.23
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584553305287
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74647/
	 user: 1619795
20/03/19 01:41:50 INFO YarnClientSchedulerBackend: Application application_1583994958990_74647 has started running.
20/03/19 01:41:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46777.
20/03/19 01:41:50 INFO NettyBlockTransferService: Server created on 10.20.174.137:46777
20/03/19 01:41:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 01:41:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 46777, None)
20/03/19 01:41:50 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:46777 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 46777, None)
20/03/19 01:41:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 46777, None)
20/03/19 01:41:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 46777, None)
20/03/19 01:41:50 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_74647
20/03/19 01:41:55 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584551455
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584551455
20/03/19 01:41:55 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.142.96:47546) with ID 1
20/03/19 01:41:55 INFO BlockManagerMasterEndpoint: Registering block manager hklpadhaa012.global.standardchartered.com:41223 with 912.3 MB RAM, BlockManagerId(1, hklpadhaa012.global.standardchartered.com, 41223, None)
*************************************** CURRENT TIME |1584553315|************************
20/03/19 01:41:55 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/19 01:41:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/19 01:41:55 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/19 01:41:55 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/19 01:41:56 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 01:41:56 INFO metastore: Connected to metastore.
20/03/19 01:42:13 INFO SessionState: Created local directory: /tmp/e238430a-2dac-46a9-a9c1-158b442db387_resources
20/03/19 01:42:13 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/e238430a-2dac-46a9-a9c1-158b442db387
20/03/19 01:42:13 INFO SessionState: Created local directory: /tmp/1619795/e238430a-2dac-46a9-a9c1-158b442db387
20/03/19 01:42:13 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/e238430a-2dac-46a9-a9c1-158b442db387/_tmp_space.db
20/03/19 01:42:13 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 01:42:14 INFO SessionState: Created local directory: /tmp/104d0ea9-7f39-4dad-bda2-9961f9db52b1_resources
20/03/19 01:42:14 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/104d0ea9-7f39-4dad-bda2-9961f9db52b1
20/03/19 01:42:14 INFO SessionState: Created local directory: /tmp/1619795/104d0ea9-7f39-4dad-bda2-9961f9db52b1
20/03/19 01:42:14 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/104d0ea9-7f39-4dad-bda2-9961f9db52b1/_tmp_space.db
20/03/19 01:42:14 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 01:42:14 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/19 01:42:15 INFO CodeGenerator: Code generated in 145.060284 ms
20/03/19 01:42:15 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/19 01:42:15 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/19 01:42:15 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/19 01:42:15 INFO DAGScheduler: Parents of final stage: List()
20/03/19 01:42:15 INFO DAGScheduler: Missing parents: List()
20/03/19 01:42:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/19 01:42:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 01:42:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 01:42:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:46777 (size: 5.0 KB, free: 366.3 MB)
20/03/19 01:42:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/19 01:42:15 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 01:42:15 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/19 01:42:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpadhaa012.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 01:42:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpadhaa012.global.standardchartered.com:41223 (size: 5.0 KB, free: 912.3 MB)
20/03/19 01:42:16 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpadhaa012.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 01:42:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 717 ms on hklpadhaa012.global.standardchartered.com (executor 1) (1/4)
20/03/19 01:42:16 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpadhaa012.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 01:42:16 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 22 ms on hklpadhaa012.global.standardchartered.com (executor 1) (2/4)
20/03/19 01:42:16 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpadhaa012.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71814 bytes)
20/03/19 01:42:16 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 14 ms on hklpadhaa012.global.standardchartered.com (executor 1) (3/4)
20/03/19 01:42:16 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 122 ms on hklpadhaa012.global.standardchartered.com (executor 1) (4/4)
20/03/19 01:42:16 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 01:42:16 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.868 s
20/03/19 01:42:16 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.031804 s
20/03/19 01:42:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:46777 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 01:42:16 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/19 01:42:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpadhaa012.global.standardchartered.com:41223 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 01:42:16 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/19 01:42:16 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/19 01:42:16 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/19 01:42:16 INFO DAGScheduler: Parents of final stage: List()
20/03/19 01:42:16 INFO DAGScheduler: Missing parents: List()
20/03/19 01:42:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/19 01:42:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 01:42:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 01:42:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:46777 (size: 5.0 KB, free: 366.3 MB)
20/03/19 01:42:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 01:42:16 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 01:42:16 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/19 01:42:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpadhaa012.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 01:42:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpadhaa012.global.standardchartered.com:41223 (size: 5.0 KB, free: 912.3 MB)
20/03/19 01:42:16 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpadhaa012.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 01:42:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 125 ms on hklpadhaa012.global.standardchartered.com (executor 1) (1/4)
20/03/19 01:42:16 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpadhaa012.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 01:42:16 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 8 ms on hklpadhaa012.global.standardchartered.com (executor 1) (2/4)
20/03/19 01:42:16 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpadhaa012.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71814 bytes)
20/03/19 01:42:16 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpadhaa012.global.standardchartered.com (executor 1) (3/4)
20/03/19 01:42:16 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 20 ms on hklpadhaa012.global.standardchartered.com (executor 1) (4/4)
20/03/19 01:42:16 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/19 01:42:16 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.158 s
20/03/19 01:42:16 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.166860 s
20/03/19 01:42:17 INFO ContextCleaner: Cleaned accumulator 0
20/03/19 01:42:17 INFO ContextCleaner: Cleaned accumulator 1
20/03/19 01:42:17 INFO ContextCleaner: Cleaned accumulator 52
20/03/19 01:42:17 INFO ContextCleaner: Cleaned accumulator 51
20/03/19 01:42:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:46777 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 01:42:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpadhaa012.global.standardchartered.com:41223 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 01:42:17 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_01-42-17_413_2747912584689851523-1
20/03/19 01:42:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/19 01:42:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/19 01:42:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/19 01:42:17 INFO CodeGenerator: Code generated in 59.102678 ms
20/03/19 01:42:17 INFO CodeGenerator: Code generated in 37.083243 ms
20/03/19 01:42:17 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/19 01:42:17 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/19 01:42:17 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/19 01:42:17 INFO DAGScheduler: Parents of final stage: List()
20/03/19 01:42:17 INFO DAGScheduler: Missing parents: List()
20/03/19 01:42:17 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/19 01:42:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/19 01:42:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/19 01:42:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:46777 (size: 114.9 KB, free: 366.2 MB)
20/03/19 01:42:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/19 01:42:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/19 01:42:17 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/19 01:42:17 WARN TaskSetManager: Stage 2 contains a task of very large size (136 KB). The maximum recommended task size is 100 KB.
20/03/19 01:42:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpadhaa012.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 139529 bytes)
20/03/19 01:42:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpadhaa012.global.standardchartered.com:41223 (size: 114.9 KB, free: 912.2 MB)
20/03/19 01:42:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2388 ms on hklpadhaa012.global.standardchartered.com (executor 1) (1/1)
20/03/19 01:42:20 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/19 01:42:20 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.389 s
20/03/19 01:42:20 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.471238 s
20/03/19 01:42:20 INFO FileFormatWriter: Job null committed.
20/03/19 01:42:20 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/19 01:42:20 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_01-42-17_413_2747912584689851523-1/-ext-10000/ods=2020-03-19/part-00000-60819287-7e72-45e3-b0d9-a56000972802.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19/part-00000-60819287-7e72-45e3-b0d9-a56000972802.c000, Status:true
20/03/19 01:42:20 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_01-42-17_413_2747912584689851523-1/-ext-10000/ods=2020-03-19 with partSpec {ods=2020-03-19}
20/03/19 01:42:20 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 01:42:20 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4046
20/03/19 01:42:20 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 01:42:20 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 01:42:20 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 01:42:20 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 01:42:20 INFO YarnClientSchedulerBackend: Stopped
20/03/19 01:42:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 01:42:20 INFO MemoryStore: MemoryStore cleared
20/03/19 01:42:20 INFO BlockManager: BlockManager stopped
20/03/19 01:42:20 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 01:42:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 01:42:20 INFO SparkContext: Successfully stopped SparkContext
20/03/19 01:42:20 INFO ShutdownHookManager: Shutdown hook called
20/03/19 01:42:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-6c817d6e-d1a3-4e83-8489-3b7433900de3
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584553341
+ DIFF_TIME=59
+ echo 'END_TIME: ' 1584553341
+ echo 'Total time taken: ' 59
+ attempt_num=17
+ sleep 30m
+ '[' 17 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584553315
++ date +%s
+ START_TIME=1584555141
+ echo 1584555141
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584553315 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 02:12:22 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 02:12:23 INFO SparkContext: Submitted application: CoolLogtest
20/03/19 02:12:23 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 02:12:23 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 02:12:23 INFO SecurityManager: Changing view acls groups to: 
20/03/19 02:12:23 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 02:12:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 02:12:23 INFO Utils: Successfully started service 'sparkDriver' on port 33229.
20/03/19 02:12:23 INFO SparkEnv: Registering MapOutputTracker
20/03/19 02:12:23 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 02:12:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 02:12:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 02:12:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5d3e69e9-7cfe-4c55-b824-a21484babf41
20/03/19 02:12:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 02:12:23 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 02:12:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 02:12:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/19 02:12:23 INFO Utils: Successfully started service 'SparkUI' on port 4042.
20/03/19 02:12:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4042
20/03/19 02:12:24 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:33229/jars/CoolPocTest.jar with timestamp 1584555144022
20/03/19 02:12:25 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 02:12:25 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 02:12:25 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 02:12:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 02:12:25 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 02:12:25 INFO Client: Setting up container launch context for our AM
20/03/19 02:12:25 INFO Client: Setting up the launch environment for our AM container
20/03/19 02:12:25 INFO Client: Preparing resources for our AM container
20/03/19 02:12:25 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 02:12:25 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7091454 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 02:12:26 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 02:12:26 INFO metastore: Connected to metastore.
20/03/19 02:12:44 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ee d9 01 4b 8a 01 71 12 e5 85 4b 8e 04 7b 8e 02 7a
20/03/19 02:12:44 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 02:12:44 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 02:12:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74799/datanucleus-api-jdo-3.2.6.jar
20/03/19 02:12:44 WARN DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-2100300935-10.20.174.135-1476178117288:blk_1610611959_544226099
java.io.IOException: Bad response ERROR for block BP-2100300935-10.20.174.135-1476178117288:blk_1610611959_544226099 from datanode DatanodeInfoWithStorage[10.23.142.46:1019,DS-3493ed14-1472-4ba0-9b41-c560ac8db6ef,DISK]
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:887)
20/03/19 02:12:44 WARN DFSClient: Error Recovery for block BP-2100300935-10.20.174.135-1476178117288:blk_1610611959_544226099 in pipeline DatanodeInfoWithStorage[10.23.225.22:1019,DS-58901a96-bf12-45a1-a19a-df489f1def68,DISK], DatanodeInfoWithStorage[10.23.142.45:1019,DS-e1a61962-be4b-4bf0-b8a5-c02fae9dada7,DISK], DatanodeInfoWithStorage[10.23.142.46:1019,DS-3493ed14-1472-4ba0-9b41-c560ac8db6ef,DISK]: bad datanode DatanodeInfoWithStorage[10.23.142.46:1019,DS-3493ed14-1472-4ba0-9b41-c560ac8db6ef,DISK]
20/03/19 02:12:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74799/datanucleus-core-3.2.10.jar
20/03/19 02:12:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74799/datanucleus-rdbms-3.2.9.jar
20/03/19 02:12:45 WARN DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-2100300935-10.20.174.135-1476178117288:blk_1610611971_544226113
java.io.IOException: Bad response ERROR for block BP-2100300935-10.20.174.135-1476178117288:blk_1610611971_544226113 from datanode DatanodeInfoWithStorage[10.23.142.44:1019,DS-f26302e9-3580-48a9-9ced-e9c548f8cf38,DISK]
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:887)
20/03/19 02:12:45 WARN DFSClient: Error Recovery for block BP-2100300935-10.20.174.135-1476178117288:blk_1610611971_544226113 in pipeline DatanodeInfoWithStorage[10.23.225.23:1019,DS-434b13a1-f15d-4ae8-a0b7-8c59e9d051ea,DISK], DatanodeInfoWithStorage[10.23.142.45:1019,DS-e1a61962-be4b-4bf0-b8a5-c02fae9dada7,DISK], DatanodeInfoWithStorage[10.23.142.44:1019,DS-f26302e9-3580-48a9-9ced-e9c548f8cf38,DISK]: bad datanode DatanodeInfoWithStorage[10.23.142.44:1019,DS-f26302e9-3580-48a9-9ced-e9c548f8cf38,DISK]
20/03/19 02:12:45 INFO Client: Uploading resource file:/tmp/spark-5d2d34bd-c994-4367-bba5-53e3eb46a090/__spark_conf__7579389143133479745.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74799/__spark_conf__.zip
20/03/19 02:12:45 WARN DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-2100300935-10.20.174.135-1476178117288:blk_1610611975_544226118
java.io.IOException: Bad response ERROR for block BP-2100300935-10.20.174.135-1476178117288:blk_1610611975_544226118 from datanode DatanodeInfoWithStorage[10.23.142.45:1019,DS-e1a61962-be4b-4bf0-b8a5-c02fae9dada7,DISK]
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:887)
20/03/19 02:12:45 WARN DFSClient: Error Recovery for block BP-2100300935-10.20.174.135-1476178117288:blk_1610611975_544226118 in pipeline DatanodeInfoWithStorage[10.23.142.69:1019,DS-66e4d25d-a83e-4a4c-bc4d-a92bb4228491,DISK], DatanodeInfoWithStorage[10.23.142.45:1019,DS-e1a61962-be4b-4bf0-b8a5-c02fae9dada7,DISK], DatanodeInfoWithStorage[10.23.142.66:1019,DS-fa34d852-c0c3-4964-98fc-de810a12b338,DISK]: bad datanode DatanodeInfoWithStorage[10.23.142.45:1019,DS-e1a61962-be4b-4bf0-b8a5-c02fae9dada7,DISK]
20/03/19 02:12:45 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 02:12:45 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 02:12:45 INFO SecurityManager: Changing view acls groups to: 
20/03/19 02:12:45 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 02:12:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 02:12:45 INFO Client: Submitting application application_1583994958990_74799 to ResourceManager
20/03/19 02:12:45 INFO YarnClientImpl: Submitted application application_1583994958990_74799
20/03/19 02:12:45 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_74799 and attemptId None
20/03/19 02:12:46 INFO Client: Application report for application_1583994958990_74799 (state: ACCEPTED)
20/03/19 02:12:46 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584555165152
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74799/
	 user: 1619795
20/03/19 02:12:47 INFO Client: Application report for application_1583994958990_74799 (state: ACCEPTED)
20/03/19 02:12:48 INFO Client: Application report for application_1583994958990_74799 (state: ACCEPTED)
20/03/19 02:12:49 INFO Client: Application report for application_1583994958990_74799 (state: ACCEPTED)
20/03/19 02:12:50 INFO Client: Application report for application_1583994958990_74799 (state: ACCEPTED)
20/03/19 02:12:50 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_74799,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74799), /proxy/application_1583994958990_74799
20/03/19 02:12:50 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 02:12:50 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 02:12:51 INFO Client: Application report for application_1583994958990_74799 (state: RUNNING)
20/03/19 02:12:51 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.142.66
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584555165152
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74799/
	 user: 1619795
20/03/19 02:12:51 INFO YarnClientSchedulerBackend: Application application_1583994958990_74799 has started running.
20/03/19 02:12:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43414.
20/03/19 02:12:51 INFO NettyBlockTransferService: Server created on 10.20.174.137:43414
20/03/19 02:12:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 02:12:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 43414, None)
20/03/19 02:12:51 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:43414 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 43414, None)
20/03/19 02:12:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 43414, None)
20/03/19 02:12:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 43414, None)
20/03/19 02:12:51 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_74799
20/03/19 02:12:54 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584553315
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584553315
*************************************** CURRENT TIME |1584555174|************************
20/03/19 02:12:54 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/19 02:12:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/19 02:12:54 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/19 02:12:54 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/19 02:12:55 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 02:12:55 INFO metastore: Connected to metastore.
20/03/19 02:12:57 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.142.45:42290) with ID 1
20/03/19 02:12:57 INFO BlockManagerMasterEndpoint: Registering block manager hklpadhaa005.global.standardchartered.com:33645 with 912.3 MB RAM, BlockManagerId(1, hklpadhaa005.global.standardchartered.com, 33645, None)
20/03/19 02:13:11 INFO SessionState: Created local directory: /tmp/d53743c7-3963-4a3c-8d8a-ba14ef13e88e_resources
20/03/19 02:13:11 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/d53743c7-3963-4a3c-8d8a-ba14ef13e88e
20/03/19 02:13:11 INFO SessionState: Created local directory: /tmp/1619795/d53743c7-3963-4a3c-8d8a-ba14ef13e88e
20/03/19 02:13:11 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/d53743c7-3963-4a3c-8d8a-ba14ef13e88e/_tmp_space.db
20/03/19 02:13:11 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 02:13:12 INFO SessionState: Created local directory: /tmp/f57cf0a3-2b4a-473e-803e-76bb0f1c3449_resources
20/03/19 02:13:12 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/f57cf0a3-2b4a-473e-803e-76bb0f1c3449
20/03/19 02:13:12 INFO SessionState: Created local directory: /tmp/1619795/f57cf0a3-2b4a-473e-803e-76bb0f1c3449
20/03/19 02:13:12 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/f57cf0a3-2b4a-473e-803e-76bb0f1c3449/_tmp_space.db
20/03/19 02:13:12 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 02:13:12 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/19 02:13:13 INFO CodeGenerator: Code generated in 158.683439 ms
20/03/19 02:13:13 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/19 02:13:13 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/19 02:13:13 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/19 02:13:13 INFO DAGScheduler: Parents of final stage: List()
20/03/19 02:13:13 INFO DAGScheduler: Missing parents: List()
20/03/19 02:13:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/19 02:13:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 02:13:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 02:13:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:43414 (size: 5.0 KB, free: 366.3 MB)
20/03/19 02:13:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/19 02:13:13 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 02:13:13 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/19 02:13:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpadhaa005.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 02:13:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpadhaa005.global.standardchartered.com:33645 (size: 5.0 KB, free: 912.3 MB)
20/03/19 02:13:14 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpadhaa005.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 02:13:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1038 ms on hklpadhaa005.global.standardchartered.com (executor 1) (1/4)
20/03/19 02:13:14 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpadhaa005.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 02:13:14 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 23 ms on hklpadhaa005.global.standardchartered.com (executor 1) (2/4)
20/03/19 02:13:14 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpadhaa005.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 02:13:14 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 16 ms on hklpadhaa005.global.standardchartered.com (executor 1) (3/4)
20/03/19 02:13:14 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 221 ms on hklpadhaa005.global.standardchartered.com (executor 1) (4/4)
20/03/19 02:13:14 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 02:13:14 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 1.294 s
20/03/19 02:13:14 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.502153 s
20/03/19 02:13:14 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/19 02:13:15 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/19 02:13:15 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/19 02:13:15 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/19 02:13:15 INFO DAGScheduler: Parents of final stage: List()
20/03/19 02:13:15 INFO DAGScheduler: Missing parents: List()
20/03/19 02:13:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/19 02:13:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 02:13:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 02:13:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:43414 (size: 5.0 KB, free: 366.3 MB)
20/03/19 02:13:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 02:13:15 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 02:13:15 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/19 02:13:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpadhaa005.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 02:13:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:43414 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 02:13:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpadhaa005.global.standardchartered.com:33645 (size: 5.0 KB, free: 912.3 MB)
20/03/19 02:13:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpadhaa005.global.standardchartered.com:33645 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 02:13:15 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpadhaa005.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 02:13:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 140 ms on hklpadhaa005.global.standardchartered.com (executor 1) (1/4)
20/03/19 02:13:15 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpadhaa005.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 02:13:15 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 10 ms on hklpadhaa005.global.standardchartered.com (executor 1) (2/4)
20/03/19 02:13:15 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpadhaa005.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 02:13:15 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 10 ms on hklpadhaa005.global.standardchartered.com (executor 1) (3/4)
20/03/19 02:13:15 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 24 ms on hklpadhaa005.global.standardchartered.com (executor 1) (4/4)
20/03/19 02:13:15 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/19 02:13:15 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.219 s
20/03/19 02:13:15 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.228974 s
20/03/19 02:13:15 INFO ContextCleaner: Cleaned accumulator 1
20/03/19 02:13:15 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:43414 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 02:13:15 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpadhaa005.global.standardchartered.com:33645 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 02:13:15 INFO ContextCleaner: Cleaned accumulator 0
20/03/19 02:13:15 INFO ContextCleaner: Cleaned accumulator 51
20/03/19 02:13:15 INFO ContextCleaner: Cleaned accumulator 52
20/03/19 02:13:15 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_02-13-15_949_4129137976894850144-1
20/03/19 02:13:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/19 02:13:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/19 02:13:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/19 02:13:16 INFO CodeGenerator: Code generated in 60.025004 ms
20/03/19 02:13:16 INFO CodeGenerator: Code generated in 36.478551 ms
20/03/19 02:13:16 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/19 02:13:16 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/19 02:13:16 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/19 02:13:16 INFO DAGScheduler: Parents of final stage: List()
20/03/19 02:13:16 INFO DAGScheduler: Missing parents: List()
20/03/19 02:13:16 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/19 02:13:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/19 02:13:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/19 02:13:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:43414 (size: 114.9 KB, free: 366.2 MB)
20/03/19 02:13:16 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/19 02:13:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/19 02:13:16 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/19 02:13:16 WARN TaskSetManager: Stage 2 contains a task of very large size (136 KB). The maximum recommended task size is 100 KB.
20/03/19 02:13:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpadhaa005.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 139531 bytes)
20/03/19 02:13:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpadhaa005.global.standardchartered.com:33645 (size: 114.9 KB, free: 912.2 MB)
20/03/19 02:13:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 3135 ms on hklpadhaa005.global.standardchartered.com (executor 1) (1/1)
20/03/19 02:13:19 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/19 02:13:19 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 3.136 s
20/03/19 02:13:19 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 3.219398 s
20/03/19 02:13:19 INFO FileFormatWriter: Job null committed.
20/03/19 02:13:19 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/19 02:13:19 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_02-13-15_949_4129137976894850144-1/-ext-10000/ods=2020-03-19/part-00000-bc52d172-05a8-4917-a2d6-d1db8ed144c5.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19/part-00000-bc52d172-05a8-4917-a2d6-d1db8ed144c5.c000, Status:true
20/03/19 02:13:19 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_02-13-15_949_4129137976894850144-1/-ext-10000/ods=2020-03-19 with partSpec {ods=2020-03-19}
20/03/19 02:13:20 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 02:13:20 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4042
20/03/19 02:13:20 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 02:13:20 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 02:13:20 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 02:13:20 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 02:13:20 INFO YarnClientSchedulerBackend: Stopped
20/03/19 02:13:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 02:13:20 INFO MemoryStore: MemoryStore cleared
20/03/19 02:13:20 INFO BlockManager: BlockManager stopped
20/03/19 02:13:20 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 02:13:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 02:13:20 INFO SparkContext: Successfully stopped SparkContext
20/03/19 02:13:20 INFO ShutdownHookManager: Shutdown hook called
20/03/19 02:13:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d2d34bd-c994-4367-bba5-53e3eb46a090
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584555200
+ DIFF_TIME=59
+ echo 'END_TIME: ' 1584555200
+ echo 'Total time taken: ' 59
+ attempt_num=18
+ sleep 30m
+ '[' 18 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584555174
++ date +%s
+ START_TIME=1584557000
+ echo 1584557000
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584555174 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 02:43:21 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 02:43:22 INFO SparkContext: Submitted application: CoolLogtest
20/03/19 02:43:22 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 02:43:22 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 02:43:22 INFO SecurityManager: Changing view acls groups to: 
20/03/19 02:43:22 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 02:43:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 02:43:22 INFO Utils: Successfully started service 'sparkDriver' on port 38856.
20/03/19 02:43:22 INFO SparkEnv: Registering MapOutputTracker
20/03/19 02:43:22 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 02:43:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 02:43:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 02:43:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9ae39bc1-9862-4c17-bae9-c5a3c5585b67
20/03/19 02:43:22 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 02:43:23 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 02:43:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 02:43:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/19 02:43:23 INFO Utils: Successfully started service 'SparkUI' on port 4042.
20/03/19 02:43:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4042
20/03/19 02:43:23 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:38856/jars/CoolPocTest.jar with timestamp 1584557003234
20/03/19 02:43:24 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 02:43:24 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 02:43:24 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 02:43:24 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 02:43:24 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 02:43:24 INFO Client: Setting up container launch context for our AM
20/03/19 02:43:24 INFO Client: Setting up the launch environment for our AM container
20/03/19 02:43:24 INFO Client: Preparing resources for our AM container
20/03/19 02:43:24 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 02:43:24 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7091696 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 02:43:26 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 02:43:26 INFO metastore: Connected to metastore.
20/03/19 02:43:42 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ee f5 5e 43 8a 01 71 13 01 e2 43 8e 04 86 8e 02 7a
20/03/19 02:43:42 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 02:43:42 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 02:43:42 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74914/datanucleus-api-jdo-3.2.6.jar
20/03/19 02:43:43 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74914/datanucleus-core-3.2.10.jar
20/03/19 02:43:43 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74914/datanucleus-rdbms-3.2.9.jar
20/03/19 02:43:43 INFO Client: Uploading resource file:/tmp/spark-de93b652-a5d0-4c86-be83-0f7bb207c920/__spark_conf__3596905806779793413.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_74914/__spark_conf__.zip
20/03/19 02:43:43 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 02:43:43 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 02:43:43 INFO SecurityManager: Changing view acls groups to: 
20/03/19 02:43:43 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 02:43:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 02:43:43 INFO Client: Submitting application application_1583994958990_74914 to ResourceManager
20/03/19 02:43:43 INFO YarnClientImpl: Submitted application application_1583994958990_74914
20/03/19 02:43:43 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_74914 and attemptId None
20/03/19 02:43:44 INFO Client: Application report for application_1583994958990_74914 (state: ACCEPTED)
20/03/19 02:43:44 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584557023489
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74914/
	 user: 1619795
20/03/19 02:43:45 INFO Client: Application report for application_1583994958990_74914 (state: ACCEPTED)
20/03/19 02:43:46 INFO Client: Application report for application_1583994958990_74914 (state: ACCEPTED)
20/03/19 02:43:47 INFO Client: Application report for application_1583994958990_74914 (state: ACCEPTED)
20/03/19 02:43:48 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_74914,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74914), /proxy/application_1583994958990_74914
20/03/19 02:43:48 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 02:43:48 INFO Client: Application report for application_1583994958990_74914 (state: ACCEPTED)
20/03/19 02:43:48 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 02:43:49 INFO Client: Application report for application_1583994958990_74914 (state: RUNNING)
20/03/19 02:43:49 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.48
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584557023489
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_74914/
	 user: 1619795
20/03/19 02:43:49 INFO YarnClientSchedulerBackend: Application application_1583994958990_74914 has started running.
20/03/19 02:43:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45227.
20/03/19 02:43:49 INFO NettyBlockTransferService: Server created on 10.20.174.137:45227
20/03/19 02:43:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 02:43:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 45227, None)
20/03/19 02:43:49 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:45227 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 45227, None)
20/03/19 02:43:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 45227, None)
20/03/19 02:43:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 45227, None)
20/03/19 02:43:50 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_74914
20/03/19 02:43:53 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584555174
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584555174
*************************************** CURRENT TIME |1584557033|************************
20/03/19 02:43:53 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/19 02:43:53 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/19 02:43:53 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/19 02:43:54 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/19 02:43:54 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 02:43:54 INFO metastore: Connected to metastore.
20/03/19 02:43:55 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.47:45826) with ID 1
20/03/19 02:43:55 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas017.global.standardchartered.com:43541 with 912.3 MB RAM, BlockManagerId(1, hklpathas017.global.standardchartered.com, 43541, None)
20/03/19 02:44:11 INFO SessionState: Created local directory: /tmp/bdbacc3a-c9b9-4ccd-b4e9-976389d916e1_resources
20/03/19 02:44:11 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/bdbacc3a-c9b9-4ccd-b4e9-976389d916e1
20/03/19 02:44:11 INFO SessionState: Created local directory: /tmp/1619795/bdbacc3a-c9b9-4ccd-b4e9-976389d916e1
20/03/19 02:44:11 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/bdbacc3a-c9b9-4ccd-b4e9-976389d916e1/_tmp_space.db
20/03/19 02:44:11 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 02:44:12 INFO SessionState: Created local directory: /tmp/e105d841-0c1b-41dd-b3f1-23b48698bdcd_resources
20/03/19 02:44:12 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/e105d841-0c1b-41dd-b3f1-23b48698bdcd
20/03/19 02:44:12 INFO SessionState: Created local directory: /tmp/1619795/e105d841-0c1b-41dd-b3f1-23b48698bdcd
20/03/19 02:44:12 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/e105d841-0c1b-41dd-b3f1-23b48698bdcd/_tmp_space.db
20/03/19 02:44:12 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 02:44:12 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/19 02:44:13 INFO CodeGenerator: Code generated in 162.535593 ms
20/03/19 02:44:13 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/19 02:44:13 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/19 02:44:13 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/19 02:44:13 INFO DAGScheduler: Parents of final stage: List()
20/03/19 02:44:13 INFO DAGScheduler: Missing parents: List()
20/03/19 02:44:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/19 02:44:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 02:44:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 02:44:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:45227 (size: 5.0 KB, free: 366.3 MB)
20/03/19 02:44:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/19 02:44:13 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 02:44:13 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/19 02:44:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas017.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 02:44:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas017.global.standardchartered.com:43541 (size: 5.0 KB, free: 912.3 MB)
20/03/19 02:44:14 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas017.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 02:44:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 821 ms on hklpathas017.global.standardchartered.com (executor 1) (1/4)
20/03/19 02:44:14 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas017.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 02:44:14 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 21 ms on hklpathas017.global.standardchartered.com (executor 1) (2/4)
20/03/19 02:44:14 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas017.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 02:44:14 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 12 ms on hklpathas017.global.standardchartered.com (executor 1) (3/4)
20/03/19 02:44:14 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 307 ms on hklpathas017.global.standardchartered.com (executor 1) (4/4)
20/03/19 02:44:14 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 02:44:14 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 1.154 s
20/03/19 02:44:14 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.345104 s
20/03/19 02:44:14 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/19 02:44:14 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/19 02:44:14 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/19 02:44:14 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/19 02:44:14 INFO DAGScheduler: Parents of final stage: List()
20/03/19 02:44:14 INFO DAGScheduler: Missing parents: List()
20/03/19 02:44:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/19 02:44:14 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 02:44:14 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 02:44:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:45227 (size: 5.0 KB, free: 366.3 MB)
20/03/19 02:44:14 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 02:44:14 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 02:44:14 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/19 02:44:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas017.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 02:44:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas017.global.standardchartered.com:43541 (size: 5.0 KB, free: 912.3 MB)
20/03/19 02:44:14 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas017.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 02:44:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 23 ms on hklpathas017.global.standardchartered.com (executor 1) (1/4)
20/03/19 02:44:14 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas017.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 02:44:14 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 8 ms on hklpathas017.global.standardchartered.com (executor 1) (2/4)
20/03/19 02:44:14 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas017.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 02:44:14 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpathas017.global.standardchartered.com (executor 1) (3/4)
20/03/19 02:44:14 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 16 ms on hklpathas017.global.standardchartered.com (executor 1) (4/4)
20/03/19 02:44:14 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/19 02:44:14 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.052 s
20/03/19 02:44:14 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.060715 s
20/03/19 02:44:15 INFO ContextCleaner: Cleaned accumulator 1
20/03/19 02:44:15 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:45227 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 02:44:15 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas017.global.standardchartered.com:43541 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 02:44:15 INFO ContextCleaner: Cleaned accumulator 51
20/03/19 02:44:15 INFO ContextCleaner: Cleaned accumulator 0
20/03/19 02:44:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:45227 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 02:44:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas017.global.standardchartered.com:43541 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 02:44:15 INFO ContextCleaner: Cleaned accumulator 52
20/03/19 02:44:15 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_02-44-15_479_3793516090225619814-1
20/03/19 02:44:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/19 02:44:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/19 02:44:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/19 02:44:15 INFO CodeGenerator: Code generated in 57.397608 ms
20/03/19 02:44:15 INFO CodeGenerator: Code generated in 36.408811 ms
20/03/19 02:44:15 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/19 02:44:15 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/19 02:44:15 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/19 02:44:15 INFO DAGScheduler: Parents of final stage: List()
20/03/19 02:44:15 INFO DAGScheduler: Missing parents: List()
20/03/19 02:44:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/19 02:44:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/19 02:44:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/19 02:44:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:45227 (size: 114.9 KB, free: 366.2 MB)
20/03/19 02:44:15 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/19 02:44:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/19 02:44:15 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/19 02:44:15 WARN TaskSetManager: Stage 2 contains a task of very large size (136 KB). The maximum recommended task size is 100 KB.
20/03/19 02:44:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas017.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 139531 bytes)
20/03/19 02:44:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas017.global.standardchartered.com:43541 (size: 114.9 KB, free: 912.2 MB)
20/03/19 02:44:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2142 ms on hklpathas017.global.standardchartered.com (executor 1) (1/1)
20/03/19 02:44:18 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/19 02:44:18 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.143 s
20/03/19 02:44:18 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.217422 s
20/03/19 02:44:18 INFO FileFormatWriter: Job null committed.
20/03/19 02:44:18 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/19 02:44:18 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_02-44-15_479_3793516090225619814-1/-ext-10000/ods=2020-03-19/part-00000-942aeb9f-06b9-4f97-b9b5-621cf83301d7.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19/part-00000-942aeb9f-06b9-4f97-b9b5-621cf83301d7.c000, Status:true
20/03/19 02:44:18 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_02-44-15_479_3793516090225619814-1/-ext-10000/ods=2020-03-19 with partSpec {ods=2020-03-19}
20/03/19 02:44:18 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 02:44:18 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4042
20/03/19 02:44:18 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 02:44:18 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 02:44:18 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 02:44:18 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 02:44:18 INFO YarnClientSchedulerBackend: Stopped
20/03/19 02:44:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 02:44:18 INFO MemoryStore: MemoryStore cleared
20/03/19 02:44:18 INFO BlockManager: BlockManager stopped
20/03/19 02:44:18 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 02:44:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 02:44:18 INFO SparkContext: Successfully stopped SparkContext
20/03/19 02:44:18 INFO ShutdownHookManager: Shutdown hook called
20/03/19 02:44:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-de93b652-a5d0-4c86-be83-0f7bb207c920
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584557059
+ DIFF_TIME=59
+ echo 'END_TIME: ' 1584557059
+ echo 'Total time taken: ' 59
+ attempt_num=19
+ sleep 30m
+ '[' 19 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584557033
++ date +%s
+ START_TIME=1584558859
+ echo 1584558859
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584557033 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 03:14:20 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 03:14:21 INFO SparkContext: Submitted application: CoolLogtest
20/03/19 03:14:21 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 03:14:21 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 03:14:21 INFO SecurityManager: Changing view acls groups to: 
20/03/19 03:14:21 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 03:14:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 03:14:21 INFO Utils: Successfully started service 'sparkDriver' on port 40239.
20/03/19 03:14:21 INFO SparkEnv: Registering MapOutputTracker
20/03/19 03:14:21 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 03:14:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 03:14:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 03:14:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0d55fd78-1d6e-48da-bb7e-26d80162f08a
20/03/19 03:14:21 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 03:14:21 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 03:14:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 03:14:21 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/19 03:14:21 INFO Utils: Successfully started service 'SparkUI' on port 4042.
20/03/19 03:14:21 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4042
20/03/19 03:14:21 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:40239/jars/CoolPocTest.jar with timestamp 1584558861845
20/03/19 03:14:22 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 03:14:23 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 03:14:23 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 03:14:23 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 03:14:23 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 03:14:23 INFO Client: Setting up container launch context for our AM
20/03/19 03:14:23 INFO Client: Setting up the launch environment for our AM container
20/03/19 03:14:23 INFO Client: Preparing resources for our AM container
20/03/19 03:14:23 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 03:14:23 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7093951 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 03:14:24 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 03:14:24 INFO metastore: Connected to metastore.
20/03/19 03:14:39 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ef 11 b4 d1 8a 01 71 13 1e 38 d1 8e 04 96 8e 02 7a
20/03/19 03:14:40 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 03:14:40 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 03:14:40 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75122/datanucleus-api-jdo-3.2.6.jar
20/03/19 03:14:40 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75122/datanucleus-core-3.2.10.jar
20/03/19 03:14:40 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75122/datanucleus-rdbms-3.2.9.jar
20/03/19 03:14:40 INFO Client: Uploading resource file:/tmp/spark-0310c59d-6705-4d20-af5f-375639c84ffe/__spark_conf__7649233814168521789.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75122/__spark_conf__.zip
20/03/19 03:14:40 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 03:14:40 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 03:14:40 INFO SecurityManager: Changing view acls groups to: 
20/03/19 03:14:40 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 03:14:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 03:14:40 INFO Client: Submitting application application_1583994958990_75122 to ResourceManager
20/03/19 03:14:40 INFO YarnClientImpl: Submitted application application_1583994958990_75122
20/03/19 03:14:40 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_75122 and attemptId None
20/03/19 03:14:41 INFO Client: Application report for application_1583994958990_75122 (state: ACCEPTED)
20/03/19 03:14:41 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584558880638
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75122/
	 user: 1619795
20/03/19 03:14:42 INFO Client: Application report for application_1583994958990_75122 (state: ACCEPTED)
20/03/19 03:14:43 INFO Client: Application report for application_1583994958990_75122 (state: ACCEPTED)
20/03/19 03:14:44 INFO Client: Application report for application_1583994958990_75122 (state: ACCEPTED)
20/03/19 03:14:45 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_75122,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75122), /proxy/application_1583994958990_75122
20/03/19 03:14:45 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 03:14:45 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 03:14:45 INFO Client: Application report for application_1583994958990_75122 (state: RUNNING)
20/03/19 03:14:45 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.142.96
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584558880638
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75122/
	 user: 1619795
20/03/19 03:14:45 INFO YarnClientSchedulerBackend: Application application_1583994958990_75122 has started running.
20/03/19 03:14:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38894.
20/03/19 03:14:45 INFO NettyBlockTransferService: Server created on 10.20.174.137:38894
20/03/19 03:14:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 03:14:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 38894, None)
20/03/19 03:14:45 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:38894 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 38894, None)
20/03/19 03:14:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 38894, None)
20/03/19 03:14:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 38894, None)
20/03/19 03:14:46 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_75122
20/03/19 03:14:50 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.48:52196) with ID 1
20/03/19 03:14:50 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/03/19 03:14:50 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas018.global.standardchartered.com:33189 with 912.3 MB RAM, BlockManagerId(1, hklpathas018.global.standardchartered.com, 33189, None)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584557033
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584557033
*************************************** CURRENT TIME |1584558891|************************
20/03/19 03:14:51 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/19 03:14:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/19 03:14:51 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/19 03:14:51 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/19 03:14:51 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 03:14:51 INFO metastore: Connected to metastore.
20/03/19 03:15:09 INFO SessionState: Created local directory: /tmp/5dcf162c-e293-4460-bf49-84e934c5490c_resources
20/03/19 03:15:09 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/5dcf162c-e293-4460-bf49-84e934c5490c
20/03/19 03:15:09 INFO SessionState: Created local directory: /tmp/1619795/5dcf162c-e293-4460-bf49-84e934c5490c
20/03/19 03:15:09 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/5dcf162c-e293-4460-bf49-84e934c5490c/_tmp_space.db
20/03/19 03:15:09 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 03:15:09 INFO SessionState: Created local directory: /tmp/ad5f8f96-7010-4983-a8eb-b99b57260a45_resources
20/03/19 03:15:09 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/ad5f8f96-7010-4983-a8eb-b99b57260a45
20/03/19 03:15:09 INFO SessionState: Created local directory: /tmp/1619795/ad5f8f96-7010-4983-a8eb-b99b57260a45
20/03/19 03:15:09 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/ad5f8f96-7010-4983-a8eb-b99b57260a45/_tmp_space.db
20/03/19 03:15:09 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 03:15:09 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/19 03:15:10 INFO CodeGenerator: Code generated in 177.139884 ms
20/03/19 03:15:10 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/19 03:15:10 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/19 03:15:10 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/19 03:15:10 INFO DAGScheduler: Parents of final stage: List()
20/03/19 03:15:10 INFO DAGScheduler: Missing parents: List()
20/03/19 03:15:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/19 03:15:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 03:15:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 03:15:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:38894 (size: 5.0 KB, free: 366.3 MB)
20/03/19 03:15:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/19 03:15:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 03:15:11 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/19 03:15:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas018.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 03:15:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas018.global.standardchartered.com:33189 (size: 5.0 KB, free: 912.3 MB)
20/03/19 03:15:11 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas018.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 03:15:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 741 ms on hklpathas018.global.standardchartered.com (executor 1) (1/4)
20/03/19 03:15:11 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas018.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 03:15:11 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 21 ms on hklpathas018.global.standardchartered.com (executor 1) (2/4)
20/03/19 03:15:11 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas018.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 03:15:11 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 32 ms on hklpathas018.global.standardchartered.com (executor 1) (3/4)
20/03/19 03:15:12 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 232 ms on hklpathas018.global.standardchartered.com (executor 1) (4/4)
20/03/19 03:15:12 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 03:15:12 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 1.020 s
20/03/19 03:15:12 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.195736 s
20/03/19 03:15:12 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/19 03:15:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:38894 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 03:15:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas018.global.standardchartered.com:33189 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 03:15:12 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/19 03:15:12 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/19 03:15:12 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/19 03:15:12 INFO DAGScheduler: Parents of final stage: List()
20/03/19 03:15:12 INFO DAGScheduler: Missing parents: List()
20/03/19 03:15:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/19 03:15:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 03:15:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 03:15:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:38894 (size: 5.0 KB, free: 366.3 MB)
20/03/19 03:15:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 03:15:12 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 03:15:12 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/19 03:15:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas018.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 03:15:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas018.global.standardchartered.com:33189 (size: 5.0 KB, free: 912.3 MB)
20/03/19 03:15:12 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas018.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 03:15:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 20 ms on hklpathas018.global.standardchartered.com (executor 1) (1/4)
20/03/19 03:15:12 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas018.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 03:15:12 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 7 ms on hklpathas018.global.standardchartered.com (executor 1) (2/4)
20/03/19 03:15:12 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas018.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 03:15:12 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpathas018.global.standardchartered.com (executor 1) (3/4)
20/03/19 03:15:12 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 16 ms on hklpathas018.global.standardchartered.com (executor 1) (4/4)
20/03/19 03:15:12 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/19 03:15:12 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.048 s
20/03/19 03:15:12 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.057115 s
20/03/19 03:15:12 INFO ContextCleaner: Cleaned accumulator 1
20/03/19 03:15:12 INFO ContextCleaner: Cleaned accumulator 51
20/03/19 03:15:12 INFO ContextCleaner: Cleaned accumulator 0
20/03/19 03:15:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:38894 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 03:15:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas018.global.standardchartered.com:33189 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 03:15:12 INFO ContextCleaner: Cleaned accumulator 52
20/03/19 03:15:13 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_03-15-13_216_6011753454159216357-1
20/03/19 03:15:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/19 03:15:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/19 03:15:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/19 03:15:13 INFO CodeGenerator: Code generated in 60.552118 ms
20/03/19 03:15:13 INFO CodeGenerator: Code generated in 37.993647 ms
20/03/19 03:15:13 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/19 03:15:13 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/19 03:15:13 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/19 03:15:13 INFO DAGScheduler: Parents of final stage: List()
20/03/19 03:15:13 INFO DAGScheduler: Missing parents: List()
20/03/19 03:15:13 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/19 03:15:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/19 03:15:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/19 03:15:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:38894 (size: 114.9 KB, free: 366.2 MB)
20/03/19 03:15:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/19 03:15:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/19 03:15:13 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/19 03:15:13 WARN TaskSetManager: Stage 2 contains a task of very large size (136 KB). The maximum recommended task size is 100 KB.
20/03/19 03:15:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas018.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 139531 bytes)
20/03/19 03:15:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas018.global.standardchartered.com:33189 (size: 114.9 KB, free: 912.2 MB)
20/03/19 03:15:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2136 ms on hklpathas018.global.standardchartered.com (executor 1) (1/1)
20/03/19 03:15:15 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/19 03:15:15 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.136 s
20/03/19 03:15:15 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.211172 s
20/03/19 03:15:15 INFO FileFormatWriter: Job null committed.
20/03/19 03:15:16 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/19 03:15:16 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_03-15-13_216_6011753454159216357-1/-ext-10000/ods=2020-03-19/part-00000-3003239a-b3e8-4e9a-af0d-0482b29f99c7.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19/part-00000-3003239a-b3e8-4e9a-af0d-0482b29f99c7.c000, Status:true
20/03/19 03:15:16 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_03-15-13_216_6011753454159216357-1/-ext-10000/ods=2020-03-19 with partSpec {ods=2020-03-19}
20/03/19 03:15:16 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 03:15:16 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4042
20/03/19 03:15:16 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 03:15:16 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 03:15:16 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 03:15:16 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 03:15:16 INFO YarnClientSchedulerBackend: Stopped
20/03/19 03:15:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 03:15:16 INFO MemoryStore: MemoryStore cleared
20/03/19 03:15:16 INFO BlockManager: BlockManager stopped
20/03/19 03:15:16 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 03:15:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 03:15:16 INFO SparkContext: Successfully stopped SparkContext
20/03/19 03:15:16 INFO ShutdownHookManager: Shutdown hook called
20/03/19 03:15:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-0310c59d-6705-4d20-af5f-375639c84ffe
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584558916
+ DIFF_TIME=57
+ echo 'END_TIME: ' 1584558916
+ echo 'Total time taken: ' 57
+ attempt_num=20
+ sleep 30m
+ '[' 20 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584558891
++ date +%s
+ START_TIME=1584560716
+ echo 1584560716
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584558891 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 03:45:17 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 03:45:18 INFO SparkContext: Submitted application: CoolLogtest
20/03/19 03:45:18 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 03:45:18 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 03:45:18 INFO SecurityManager: Changing view acls groups to: 
20/03/19 03:45:18 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 03:45:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 03:45:19 INFO Utils: Successfully started service 'sparkDriver' on port 44148.
20/03/19 03:45:19 INFO SparkEnv: Registering MapOutputTracker
20/03/19 03:45:19 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 03:45:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 03:45:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 03:45:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-efac8e5f-7aae-4f38-8e82-f005fa461cb9
20/03/19 03:45:19 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 03:45:19 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 03:45:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 03:45:19 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/03/19 03:45:19 INFO Utils: Successfully started service 'SparkUI' on port 4042.
20/03/19 03:45:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4042
20/03/19 03:45:19 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:44148/jars/CoolPocTest.jar with timestamp 1584560719535
20/03/19 03:45:20 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 03:45:20 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 03:45:20 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 03:45:20 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 03:45:20 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 03:45:20 INFO Client: Setting up container launch context for our AM
20/03/19 03:45:20 INFO Client: Setting up the launch environment for our AM container
20/03/19 03:45:20 INFO Client: Preparing resources for our AM container
20/03/19 03:45:20 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 03:45:20 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7095513 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 03:45:22 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 03:45:22 INFO metastore: Connected to metastore.
20/03/19 03:45:38 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ef 2e 11 8c 8a 01 71 13 3a 95 8c 8e 04 97 8e 02 7a
20/03/19 03:45:38 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 03:45:38 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 03:45:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75263/datanucleus-api-jdo-3.2.6.jar
20/03/19 03:45:39 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75263/datanucleus-core-3.2.10.jar
20/03/19 03:45:39 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75263/datanucleus-rdbms-3.2.9.jar
20/03/19 03:45:39 INFO Client: Uploading resource file:/tmp/spark-89eb159a-8dd4-4262-b41c-45d130e6298c/__spark_conf__6013277213120393116.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75263/__spark_conf__.zip
20/03/19 03:45:39 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 03:45:39 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 03:45:39 INFO SecurityManager: Changing view acls groups to: 
20/03/19 03:45:39 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 03:45:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 03:45:39 INFO Client: Submitting application application_1583994958990_75263 to ResourceManager
20/03/19 03:45:39 INFO YarnClientImpl: Submitted application application_1583994958990_75263
20/03/19 03:45:39 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_75263 and attemptId None
20/03/19 03:45:40 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:40 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584560739398
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75263/
	 user: 1619795
20/03/19 03:45:41 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:42 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:43 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:44 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:45 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:46 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:47 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:48 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:49 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:50 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:51 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:52 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:53 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:54 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:55 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:56 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:57 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:58 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:45:59 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:00 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:01 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:02 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:03 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:04 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:05 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:06 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:07 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:08 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:09 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:10 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:11 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:12 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:13 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:14 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:15 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:16 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:17 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:18 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:19 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:20 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:21 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:22 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:23 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:24 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:25 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:26 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:27 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:28 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:29 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:30 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:31 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:32 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:33 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:34 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:35 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:36 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:37 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:38 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:39 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:40 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:41 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:42 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:43 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:44 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:45 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:46 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:47 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:48 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:49 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:50 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:51 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:52 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:53 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:54 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:55 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:56 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:57 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:58 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:46:59 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:00 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:01 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:02 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:03 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:04 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:05 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:06 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:07 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:08 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:09 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:10 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:11 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:12 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:13 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:14 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:15 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:16 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:17 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:18 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:19 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:20 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:21 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:22 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:23 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:24 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:25 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:26 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:27 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:28 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:29 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:30 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:31 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:32 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:33 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:34 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:35 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:36 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:37 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:38 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:39 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:40 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:41 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:42 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:43 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:44 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:45 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:46 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:47 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:48 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:49 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:50 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:51 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:52 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:53 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:54 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:55 INFO Client: Application report for application_1583994958990_75263 (state: ACCEPTED)
20/03/19 03:47:56 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_75263,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75263), /proxy/application_1583994958990_75263
20/03/19 03:47:56 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 03:47:56 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 03:47:56 INFO Client: Application report for application_1583994958990_75263 (state: RUNNING)
20/03/19 03:47:56 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.142.68
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584560739398
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75263/
	 user: 1619795
20/03/19 03:47:56 INFO YarnClientSchedulerBackend: Application application_1583994958990_75263 has started running.
20/03/19 03:47:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39712.
20/03/19 03:47:56 INFO NettyBlockTransferService: Server created on 10.20.174.137:39712
20/03/19 03:47:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 03:47:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 39712, None)
20/03/19 03:47:56 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:39712 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 39712, None)
20/03/19 03:47:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 39712, None)
20/03/19 03:47:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 39712, None)
20/03/19 03:47:57 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_75263
20/03/19 03:47:57 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584558891
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584558891
*************************************** CURRENT TIME |1584560877|************************
20/03/19 03:47:57 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/19 03:47:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/19 03:47:57 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/19 03:47:57 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/19 03:47:58 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 03:47:58 INFO metastore: Connected to metastore.
20/03/19 03:48:01 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.20.174.139:57480) with ID 1
20/03/19 03:48:01 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas05.hk.standardchartered.com:32903 with 912.3 MB RAM, BlockManagerId(1, hklpathas05.hk.standardchartered.com, 32903, None)
20/03/19 03:48:15 INFO SessionState: Created local directory: /tmp/d8d973be-9fe6-41dc-8bf2-f93ffd7ccb19_resources
20/03/19 03:48:15 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/d8d973be-9fe6-41dc-8bf2-f93ffd7ccb19
20/03/19 03:48:15 INFO SessionState: Created local directory: /tmp/1619795/d8d973be-9fe6-41dc-8bf2-f93ffd7ccb19
20/03/19 03:48:15 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/d8d973be-9fe6-41dc-8bf2-f93ffd7ccb19/_tmp_space.db
20/03/19 03:48:15 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 03:48:15 INFO SessionState: Created local directory: /tmp/acfd7475-17c8-4795-bd93-2293eba791a1_resources
20/03/19 03:48:15 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/acfd7475-17c8-4795-bd93-2293eba791a1
20/03/19 03:48:15 INFO SessionState: Created local directory: /tmp/1619795/acfd7475-17c8-4795-bd93-2293eba791a1
20/03/19 03:48:15 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/acfd7475-17c8-4795-bd93-2293eba791a1/_tmp_space.db
20/03/19 03:48:15 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 03:48:15 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/19 03:48:16 INFO CodeGenerator: Code generated in 165.431064 ms
20/03/19 03:48:16 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/19 03:48:16 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/19 03:48:16 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/19 03:48:16 INFO DAGScheduler: Parents of final stage: List()
20/03/19 03:48:16 INFO DAGScheduler: Missing parents: List()
20/03/19 03:48:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/19 03:48:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 03:48:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 03:48:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:39712 (size: 5.0 KB, free: 366.3 MB)
20/03/19 03:48:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/19 03:48:16 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 03:48:16 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/19 03:48:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas05.hk.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 03:48:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas05.hk.standardchartered.com:32903 (size: 5.0 KB, free: 912.3 MB)
20/03/19 03:48:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas05.hk.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 03:48:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 924 ms on hklpathas05.hk.standardchartered.com (executor 1) (1/4)
20/03/19 03:48:17 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas05.hk.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 03:48:17 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 19 ms on hklpathas05.hk.standardchartered.com (executor 1) (2/4)
20/03/19 03:48:17 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas05.hk.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 03:48:17 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 11 ms on hklpathas05.hk.standardchartered.com (executor 1) (3/4)
20/03/19 03:48:17 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 200 ms on hklpathas05.hk.standardchartered.com (executor 1) (4/4)
20/03/19 03:48:17 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 03:48:17 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 1.147 s
20/03/19 03:48:17 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.322676 s
20/03/19 03:48:17 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/19 03:48:17 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/19 03:48:17 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/19 03:48:17 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/19 03:48:17 INFO DAGScheduler: Parents of final stage: List()
20/03/19 03:48:17 INFO DAGScheduler: Missing parents: List()
20/03/19 03:48:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/19 03:48:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 03:48:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 03:48:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:39712 (size: 5.0 KB, free: 366.3 MB)
20/03/19 03:48:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 03:48:17 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 03:48:17 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/19 03:48:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas05.hk.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 03:48:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas05.hk.standardchartered.com:32903 (size: 5.0 KB, free: 912.3 MB)
20/03/19 03:48:17 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas05.hk.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 03:48:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 19 ms on hklpathas05.hk.standardchartered.com (executor 1) (1/4)
20/03/19 03:48:17 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas05.hk.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 03:48:17 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 7 ms on hklpathas05.hk.standardchartered.com (executor 1) (2/4)
20/03/19 03:48:17 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas05.hk.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 03:48:17 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpathas05.hk.standardchartered.com (executor 1) (3/4)
20/03/19 03:48:17 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 19 ms on hklpathas05.hk.standardchartered.com (executor 1) (4/4)
20/03/19 03:48:17 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/19 03:48:17 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.051 s
20/03/19 03:48:17 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.063103 s
20/03/19 03:48:18 INFO ContextCleaner: Cleaned accumulator 0
20/03/19 03:48:18 INFO ContextCleaner: Cleaned accumulator 1
20/03/19 03:48:18 INFO ContextCleaner: Cleaned accumulator 51
20/03/19 03:48:18 INFO ContextCleaner: Cleaned accumulator 52
20/03/19 03:48:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:39712 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 03:48:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas05.hk.standardchartered.com:32903 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 03:48:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:39712 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 03:48:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas05.hk.standardchartered.com:32903 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 03:48:18 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_03-48-18_641_3182798454552235507-1
20/03/19 03:48:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/19 03:48:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/19 03:48:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/19 03:48:18 INFO CodeGenerator: Code generated in 75.830867 ms
20/03/19 03:48:19 INFO CodeGenerator: Code generated in 46.8927 ms
20/03/19 03:48:19 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/19 03:48:19 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/19 03:48:19 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/19 03:48:19 INFO DAGScheduler: Parents of final stage: List()
20/03/19 03:48:19 INFO DAGScheduler: Missing parents: List()
20/03/19 03:48:19 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/19 03:48:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/19 03:48:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/19 03:48:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:39712 (size: 114.9 KB, free: 366.2 MB)
20/03/19 03:48:19 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/19 03:48:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/19 03:48:19 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/19 03:48:19 WARN TaskSetManager: Stage 2 contains a task of very large size (136 KB). The maximum recommended task size is 100 KB.
20/03/19 03:48:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas05.hk.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 139531 bytes)
20/03/19 03:48:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas05.hk.standardchartered.com:32903 (size: 114.9 KB, free: 912.2 MB)
20/03/19 03:48:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2011 ms on hklpathas05.hk.standardchartered.com (executor 1) (1/1)
20/03/19 03:48:21 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/19 03:48:21 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.012 s
20/03/19 03:48:21 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.095101 s
20/03/19 03:48:21 INFO FileFormatWriter: Job null committed.
20/03/19 03:48:21 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/19 03:48:21 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_03-48-18_641_3182798454552235507-1/-ext-10000/ods=2020-03-19/part-00000-85690f0c-577c-4a77-94c1-9f95a9551ef7.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19/part-00000-85690f0c-577c-4a77-94c1-9f95a9551ef7.c000, Status:true
20/03/19 03:48:21 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_03-48-18_641_3182798454552235507-1/-ext-10000/ods=2020-03-19 with partSpec {ods=2020-03-19}
20/03/19 03:48:21 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 03:48:21 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4042
20/03/19 03:48:21 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 03:48:21 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 03:48:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 03:48:21 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 03:48:21 INFO YarnClientSchedulerBackend: Stopped
20/03/19 03:48:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 03:48:21 INFO MemoryStore: MemoryStore cleared
20/03/19 03:48:21 INFO BlockManager: BlockManager stopped
20/03/19 03:48:21 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 03:48:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 03:48:21 INFO SparkContext: Successfully stopped SparkContext
20/03/19 03:48:21 INFO ShutdownHookManager: Shutdown hook called
20/03/19 03:48:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-89eb159a-8dd4-4262-b41c-45d130e6298c
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584560902
+ DIFF_TIME=186
+ echo 'END_TIME: ' 1584560902
+ echo 'Total time taken: ' 186
+ attempt_num=21
+ sleep 30m
+ '[' 21 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584560877
++ date +%s
+ START_TIME=1584562702
+ echo 1584562702
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584560877 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 04:18:23 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 04:18:24 INFO SparkContext: Submitted application: CoolLogtest
20/03/19 04:18:24 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 04:18:24 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 04:18:24 INFO SecurityManager: Changing view acls groups to: 
20/03/19 04:18:24 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 04:18:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 04:18:24 INFO Utils: Successfully started service 'sparkDriver' on port 42126.
20/03/19 04:18:24 INFO SparkEnv: Registering MapOutputTracker
20/03/19 04:18:24 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 04:18:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 04:18:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 04:18:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b05ff0b5-f592-4c39-b08b-706e167dd476
20/03/19 04:18:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 04:18:24 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 04:18:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 04:18:24 INFO Utils: Successfully started service 'SparkUI' on port 4041.
20/03/19 04:18:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4041
20/03/19 04:18:24 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:42126/jars/CoolPocTest.jar with timestamp 1584562704885
20/03/19 04:18:25 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 04:18:26 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 04:18:26 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 04:18:26 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 04:18:26 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 04:18:26 INFO Client: Setting up container launch context for our AM
20/03/19 04:18:26 INFO Client: Setting up the launch environment for our AM container
20/03/19 04:18:26 INFO Client: Preparing resources for our AM container
20/03/19 04:18:26 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 04:18:26 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7095661 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 04:18:27 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 04:18:27 INFO metastore: Connected to metastore.
20/03/19 04:18:44 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ef 4c 5c e0 8a 01 71 13 58 e0 e0 8e 04 98 8e 02 7a
20/03/19 04:18:44 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 04:18:44 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 04:18:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75367/datanucleus-api-jdo-3.2.6.jar
20/03/19 04:18:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75367/datanucleus-core-3.2.10.jar
20/03/19 04:18:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75367/datanucleus-rdbms-3.2.9.jar
20/03/19 04:18:45 INFO Client: Uploading resource file:/tmp/spark-8534ae69-7989-4eb6-bf67-26a75ecd04ba/__spark_conf__7910703308774329909.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75367/__spark_conf__.zip
20/03/19 04:18:45 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 04:18:45 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 04:18:45 INFO SecurityManager: Changing view acls groups to: 
20/03/19 04:18:45 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 04:18:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 04:18:45 INFO Client: Submitting application application_1583994958990_75367 to ResourceManager
20/03/19 04:18:45 INFO YarnClientImpl: Submitted application application_1583994958990_75367
20/03/19 04:18:45 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_75367 and attemptId None
20/03/19 04:18:46 INFO Client: Application report for application_1583994958990_75367 (state: ACCEPTED)
20/03/19 04:18:46 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584562725258
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75367/
	 user: 1619795
20/03/19 04:18:47 INFO Client: Application report for application_1583994958990_75367 (state: ACCEPTED)
20/03/19 04:18:48 INFO Client: Application report for application_1583994958990_75367 (state: ACCEPTED)
20/03/19 04:18:49 INFO Client: Application report for application_1583994958990_75367 (state: ACCEPTED)
20/03/19 04:18:50 INFO Client: Application report for application_1583994958990_75367 (state: ACCEPTED)
20/03/19 04:18:50 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_75367,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75367), /proxy/application_1583994958990_75367
20/03/19 04:18:50 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 04:18:51 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 04:18:51 INFO Client: Application report for application_1583994958990_75367 (state: RUNNING)
20/03/19 04:18:51 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.49
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584562725258
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75367/
	 user: 1619795
20/03/19 04:18:51 INFO YarnClientSchedulerBackend: Application application_1583994958990_75367 has started running.
20/03/19 04:18:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40185.
20/03/19 04:18:51 INFO NettyBlockTransferService: Server created on 10.20.174.137:40185
20/03/19 04:18:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 04:18:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 40185, None)
20/03/19 04:18:51 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:40185 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 40185, None)
20/03/19 04:18:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 40185, None)
20/03/19 04:18:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 40185, None)
20/03/19 04:18:51 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_75367
20/03/19 04:18:53 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.49:47908) with ID 1
20/03/19 04:18:53 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/03/19 04:18:53 INFO BlockManagerMasterEndpoint: Registering block manager hklpadhas013.global.standardchartered.com:35103 with 912.3 MB RAM, BlockManagerId(1, hklpadhas013.global.standardchartered.com, 35103, None)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584560877
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584560877
*************************************** CURRENT TIME |1584562734|************************
20/03/19 04:18:54 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/19 04:18:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/19 04:18:54 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/19 04:18:54 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/19 04:18:54 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 04:18:54 INFO metastore: Connected to metastore.
20/03/19 04:19:11 INFO SessionState: Created local directory: /tmp/3dbb4bac-e201-4d5d-afb4-5c15045d8c90_resources
20/03/19 04:19:11 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/3dbb4bac-e201-4d5d-afb4-5c15045d8c90
20/03/19 04:19:11 INFO SessionState: Created local directory: /tmp/1619795/3dbb4bac-e201-4d5d-afb4-5c15045d8c90
20/03/19 04:19:11 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/3dbb4bac-e201-4d5d-afb4-5c15045d8c90/_tmp_space.db
20/03/19 04:19:11 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 04:19:13 INFO SessionState: Created local directory: /tmp/2b87b4f0-4643-4417-9b57-d41b1901e827_resources
20/03/19 04:19:13 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/2b87b4f0-4643-4417-9b57-d41b1901e827
20/03/19 04:19:13 INFO SessionState: Created local directory: /tmp/1619795/2b87b4f0-4643-4417-9b57-d41b1901e827
20/03/19 04:19:13 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/2b87b4f0-4643-4417-9b57-d41b1901e827/_tmp_space.db
20/03/19 04:19:13 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 04:19:13 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/19 04:19:14 INFO CodeGenerator: Code generated in 164.186272 ms
20/03/19 04:19:14 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/19 04:19:14 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/19 04:19:14 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/19 04:19:14 INFO DAGScheduler: Parents of final stage: List()
20/03/19 04:19:14 INFO DAGScheduler: Missing parents: List()
20/03/19 04:19:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/19 04:19:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 04:19:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 04:19:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:40185 (size: 5.0 KB, free: 366.3 MB)
20/03/19 04:19:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/19 04:19:15 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 04:19:15 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/19 04:19:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 04:19:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpadhas013.global.standardchartered.com:35103 (size: 5.0 KB, free: 912.3 MB)
20/03/19 04:19:15 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpadhas013.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 04:19:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 613 ms on hklpadhas013.global.standardchartered.com (executor 1) (1/4)
20/03/19 04:19:15 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpadhas013.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 04:19:15 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 17 ms on hklpadhas013.global.standardchartered.com (executor 1) (2/4)
20/03/19 04:19:15 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpadhas013.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 04:19:15 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 10 ms on hklpadhas013.global.standardchartered.com (executor 1) (3/4)
20/03/19 04:19:16 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 578 ms on hklpadhas013.global.standardchartered.com (executor 1) (4/4)
20/03/19 04:19:16 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 04:19:16 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 1.214 s
20/03/19 04:19:16 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.385119 s
20/03/19 04:19:16 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/19 04:19:16 INFO ContextCleaner: Cleaned accumulator 1
20/03/19 04:19:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:40185 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 04:19:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpadhas013.global.standardchartered.com:35103 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 04:19:16 INFO ContextCleaner: Cleaned accumulator 0
20/03/19 04:19:16 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/19 04:19:16 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/19 04:19:16 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/19 04:19:16 INFO DAGScheduler: Parents of final stage: List()
20/03/19 04:19:16 INFO DAGScheduler: Missing parents: List()
20/03/19 04:19:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/19 04:19:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 04:19:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 04:19:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:40185 (size: 5.0 KB, free: 366.3 MB)
20/03/19 04:19:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 04:19:16 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 04:19:16 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/19 04:19:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 04:19:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpadhas013.global.standardchartered.com:35103 (size: 5.0 KB, free: 912.3 MB)
20/03/19 04:19:16 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpadhas013.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 04:19:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 22 ms on hklpadhas013.global.standardchartered.com (executor 1) (1/4)
20/03/19 04:19:16 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpadhas013.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 04:19:16 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 7 ms on hklpadhas013.global.standardchartered.com (executor 1) (2/4)
20/03/19 04:19:16 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpadhas013.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 04:19:16 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpadhas013.global.standardchartered.com (executor 1) (3/4)
20/03/19 04:19:16 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 16 ms on hklpadhas013.global.standardchartered.com (executor 1) (4/4)
20/03/19 04:19:16 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/19 04:19:16 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.051 s
20/03/19 04:19:16 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.060254 s
20/03/19 04:19:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:40185 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 04:19:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpadhas013.global.standardchartered.com:35103 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 04:19:17 INFO ContextCleaner: Cleaned accumulator 51
20/03/19 04:19:17 INFO ContextCleaner: Cleaned accumulator 52
20/03/19 04:19:17 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_04-19-17_399_2037530165630514191-1
20/03/19 04:19:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/19 04:19:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/19 04:19:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/19 04:19:17 INFO CodeGenerator: Code generated in 60.118 ms
20/03/19 04:19:17 INFO CodeGenerator: Code generated in 42.244514 ms
20/03/19 04:19:17 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/19 04:19:17 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/19 04:19:17 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/19 04:19:17 INFO DAGScheduler: Parents of final stage: List()
20/03/19 04:19:17 INFO DAGScheduler: Missing parents: List()
20/03/19 04:19:17 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/19 04:19:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/19 04:19:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/19 04:19:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:40185 (size: 114.9 KB, free: 366.2 MB)
20/03/19 04:19:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/19 04:19:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/19 04:19:17 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/19 04:19:17 WARN TaskSetManager: Stage 2 contains a task of very large size (136 KB). The maximum recommended task size is 100 KB.
20/03/19 04:19:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 139531 bytes)
20/03/19 04:19:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpadhas013.global.standardchartered.com:35103 (size: 114.9 KB, free: 912.2 MB)
20/03/19 04:19:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2451 ms on hklpadhas013.global.standardchartered.com (executor 1) (1/1)
20/03/19 04:19:20 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/19 04:19:20 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.452 s
20/03/19 04:19:20 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.525787 s
20/03/19 04:19:20 INFO FileFormatWriter: Job null committed.
20/03/19 04:19:20 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/19 04:19:20 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_04-19-17_399_2037530165630514191-1/-ext-10000/ods=2020-03-19/part-00000-f1637dfb-a3e8-4eb2-95d5-9898e413bd22.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19/part-00000-f1637dfb-a3e8-4eb2-95d5-9898e413bd22.c000, Status:true
20/03/19 04:19:20 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_04-19-17_399_2037530165630514191-1/-ext-10000/ods=2020-03-19 with partSpec {ods=2020-03-19}
20/03/19 04:19:20 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 04:19:20 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4041
20/03/19 04:19:20 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 04:19:20 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 04:19:20 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 04:19:20 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 04:19:20 INFO YarnClientSchedulerBackend: Stopped
20/03/19 04:19:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 04:19:20 INFO MemoryStore: MemoryStore cleared
20/03/19 04:19:20 INFO BlockManager: BlockManager stopped
20/03/19 04:19:20 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 04:19:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 04:19:20 INFO SparkContext: Successfully stopped SparkContext
20/03/19 04:19:20 INFO ShutdownHookManager: Shutdown hook called
20/03/19 04:19:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-8534ae69-7989-4eb6-bf67-26a75ecd04ba
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584562761
+ DIFF_TIME=59
+ echo 'END_TIME: ' 1584562761
+ echo 'Total time taken: ' 59
+ attempt_num=22
+ sleep 30m
+ '[' 22 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584562734
++ date +%s
+ START_TIME=1584564561
+ echo 1584564561
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584562734 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 04:49:22 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 04:49:23 INFO SparkContext: Submitted application: CoolLogtest
20/03/19 04:49:23 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 04:49:23 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 04:49:23 INFO SecurityManager: Changing view acls groups to: 
20/03/19 04:49:23 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 04:49:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 04:49:23 INFO Utils: Successfully started service 'sparkDriver' on port 46511.
20/03/19 04:49:23 INFO SparkEnv: Registering MapOutputTracker
20/03/19 04:49:23 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 04:49:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 04:49:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 04:49:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-758265ff-32f4-400d-b53a-8fb2044c9137
20/03/19 04:49:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 04:49:23 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 04:49:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 04:49:23 INFO Utils: Successfully started service 'SparkUI' on port 4041.
20/03/19 04:49:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4041
20/03/19 04:49:23 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:46511/jars/CoolPocTest.jar with timestamp 1584564563947
20/03/19 04:49:24 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 04:49:25 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 04:49:25 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 04:49:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 04:49:25 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 04:49:25 INFO Client: Setting up container launch context for our AM
20/03/19 04:49:25 INFO Client: Setting up the launch environment for our AM container
20/03/19 04:49:25 INFO Client: Preparing resources for our AM container
20/03/19 04:49:25 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 04:49:25 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7095922 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 04:49:26 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 04:49:26 INFO metastore: Connected to metastore.
20/03/19 04:49:43 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ef 68 bd 8c 8a 01 71 13 75 41 8c 8e 04 99 8e 02 7a
20/03/19 04:49:43 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 04:49:43 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 04:49:43 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75515/datanucleus-api-jdo-3.2.6.jar
20/03/19 04:49:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75515/datanucleus-core-3.2.10.jar
20/03/19 04:49:44 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75515/datanucleus-rdbms-3.2.9.jar
20/03/19 04:49:44 INFO Client: Uploading resource file:/tmp/spark-d7afdbae-a904-4dfd-b130-6e6d7764dace/__spark_conf__5688149933045487422.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75515/__spark_conf__.zip
20/03/19 04:49:44 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 04:49:44 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 04:49:44 INFO SecurityManager: Changing view acls groups to: 
20/03/19 04:49:44 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 04:49:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 04:49:44 INFO Client: Submitting application application_1583994958990_75515 to ResourceManager
20/03/19 04:49:44 INFO YarnClientImpl: Submitted application application_1583994958990_75515
20/03/19 04:49:44 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_75515 and attemptId None
20/03/19 04:49:45 INFO Client: Application report for application_1583994958990_75515 (state: ACCEPTED)
20/03/19 04:49:45 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584564584580
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75515/
	 user: 1619795
20/03/19 04:49:46 INFO Client: Application report for application_1583994958990_75515 (state: ACCEPTED)
20/03/19 04:49:47 INFO Client: Application report for application_1583994958990_75515 (state: ACCEPTED)
20/03/19 04:49:48 INFO Client: Application report for application_1583994958990_75515 (state: ACCEPTED)
20/03/19 04:49:48 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_75515,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75515), /proxy/application_1583994958990_75515
20/03/19 04:49:48 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 04:49:49 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 04:49:49 INFO Client: Application report for application_1583994958990_75515 (state: RUNNING)
20/03/19 04:49:49 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.142.66
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584564584580
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75515/
	 user: 1619795
20/03/19 04:49:49 INFO YarnClientSchedulerBackend: Application application_1583994958990_75515 has started running.
20/03/19 04:49:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42356.
20/03/19 04:49:49 INFO NettyBlockTransferService: Server created on 10.20.174.137:42356
20/03/19 04:49:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 04:49:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 42356, None)
20/03/19 04:49:49 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:42356 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 42356, None)
20/03/19 04:49:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 42356, None)
20/03/19 04:49:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 42356, None)
20/03/19 04:49:49 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_75515
20/03/19 04:49:53 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.59:39212) with ID 1
20/03/19 04:49:53 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas021.global.standardchartered.com:43055 with 912.3 MB RAM, BlockManagerId(1, hklpathas021.global.standardchartered.com, 43055, None)
20/03/19 04:49:53 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584562734
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584562734
*************************************** CURRENT TIME |1584564593|************************
20/03/19 04:49:53 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/19 04:49:53 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/19 04:49:53 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/19 04:49:54 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/19 04:49:54 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 04:49:54 INFO metastore: Connected to metastore.
20/03/19 04:50:10 INFO SessionState: Created local directory: /tmp/1c17846d-9856-4a67-bcab-5b1c628636af_resources
20/03/19 04:50:10 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/1c17846d-9856-4a67-bcab-5b1c628636af
20/03/19 04:50:10 INFO SessionState: Created local directory: /tmp/1619795/1c17846d-9856-4a67-bcab-5b1c628636af
20/03/19 04:50:10 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/1c17846d-9856-4a67-bcab-5b1c628636af/_tmp_space.db
20/03/19 04:50:10 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 04:50:10 INFO SessionState: Created local directory: /tmp/763344ae-7baa-436d-9d49-718517a724a6_resources
20/03/19 04:50:10 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/763344ae-7baa-436d-9d49-718517a724a6
20/03/19 04:50:10 INFO SessionState: Created local directory: /tmp/1619795/763344ae-7baa-436d-9d49-718517a724a6
20/03/19 04:50:10 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/763344ae-7baa-436d-9d49-718517a724a6/_tmp_space.db
20/03/19 04:50:10 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 04:50:10 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/19 04:50:11 INFO CodeGenerator: Code generated in 155.346778 ms
20/03/19 04:50:11 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/19 04:50:11 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/19 04:50:11 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/19 04:50:11 INFO DAGScheduler: Parents of final stage: List()
20/03/19 04:50:11 INFO DAGScheduler: Missing parents: List()
20/03/19 04:50:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/19 04:50:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 04:50:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 04:50:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:42356 (size: 5.0 KB, free: 366.3 MB)
20/03/19 04:50:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/19 04:50:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 04:50:11 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/19 04:50:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas021.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 04:50:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas021.global.standardchartered.com:43055 (size: 5.0 KB, free: 912.3 MB)
20/03/19 04:50:12 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas021.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 04:50:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 601 ms on hklpathas021.global.standardchartered.com (executor 1) (1/4)
20/03/19 04:50:12 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas021.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 04:50:12 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 16 ms on hklpathas021.global.standardchartered.com (executor 1) (2/4)
20/03/19 04:50:12 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas021.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 04:50:12 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 9 ms on hklpathas021.global.standardchartered.com (executor 1) (3/4)
20/03/19 04:50:12 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 106 ms on hklpathas021.global.standardchartered.com (executor 1) (4/4)
20/03/19 04:50:12 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 04:50:12 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.727 s
20/03/19 04:50:12 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 0.908208 s
20/03/19 04:50:12 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/19 04:50:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:42356 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 04:50:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas021.global.standardchartered.com:43055 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 04:50:12 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/19 04:50:12 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/19 04:50:12 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/19 04:50:12 INFO DAGScheduler: Parents of final stage: List()
20/03/19 04:50:12 INFO DAGScheduler: Missing parents: List()
20/03/19 04:50:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/19 04:50:12 INFO ContextCleaner: Cleaned accumulator 0
20/03/19 04:50:12 INFO ContextCleaner: Cleaned accumulator 1
20/03/19 04:50:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 04:50:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 04:50:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:42356 (size: 5.0 KB, free: 366.3 MB)
20/03/19 04:50:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 04:50:12 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 04:50:12 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/19 04:50:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas021.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 04:50:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas021.global.standardchartered.com:43055 (size: 5.0 KB, free: 912.3 MB)
20/03/19 04:50:13 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas021.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 04:50:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 99 ms on hklpathas021.global.standardchartered.com (executor 1) (1/4)
20/03/19 04:50:13 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas021.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 04:50:13 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 7 ms on hklpathas021.global.standardchartered.com (executor 1) (2/4)
20/03/19 04:50:13 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas021.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 04:50:13 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 6 ms on hklpathas021.global.standardchartered.com (executor 1) (3/4)
20/03/19 04:50:13 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 15 ms on hklpathas021.global.standardchartered.com (executor 1) (4/4)
20/03/19 04:50:13 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/19 04:50:13 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.125 s
20/03/19 04:50:13 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.133126 s
20/03/19 04:50:13 INFO ContextCleaner: Cleaned accumulator 51
20/03/19 04:50:13 INFO ContextCleaner: Cleaned accumulator 52
20/03/19 04:50:13 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:42356 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 04:50:13 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas021.global.standardchartered.com:43055 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 04:50:13 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_04-50-13_799_2010343608284402493-1
20/03/19 04:50:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/19 04:50:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/19 04:50:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/19 04:50:14 INFO CodeGenerator: Code generated in 60.278919 ms
20/03/19 04:50:14 INFO CodeGenerator: Code generated in 39.278089 ms
20/03/19 04:50:14 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/19 04:50:14 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/19 04:50:14 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/19 04:50:14 INFO DAGScheduler: Parents of final stage: List()
20/03/19 04:50:14 INFO DAGScheduler: Missing parents: List()
20/03/19 04:50:14 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/19 04:50:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/19 04:50:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/19 04:50:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:42356 (size: 114.9 KB, free: 366.2 MB)
20/03/19 04:50:14 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/19 04:50:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/19 04:50:14 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/19 04:50:14 WARN TaskSetManager: Stage 2 contains a task of very large size (136 KB). The maximum recommended task size is 100 KB.
20/03/19 04:50:14 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas021.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 139531 bytes)
20/03/19 04:50:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas021.global.standardchartered.com:43055 (size: 114.9 KB, free: 912.2 MB)
20/03/19 04:50:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2150 ms on hklpathas021.global.standardchartered.com (executor 1) (1/1)
20/03/19 04:50:16 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/19 04:50:16 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.150 s
20/03/19 04:50:16 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.233186 s
20/03/19 04:50:16 INFO FileFormatWriter: Job null committed.
20/03/19 04:50:16 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/19 04:50:16 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_04-50-13_799_2010343608284402493-1/-ext-10000/ods=2020-03-19/part-00000-14bc1577-ead4-4546-9108-c12ed795b46c.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19/part-00000-14bc1577-ead4-4546-9108-c12ed795b46c.c000, Status:true
20/03/19 04:50:16 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_04-50-13_799_2010343608284402493-1/-ext-10000/ods=2020-03-19 with partSpec {ods=2020-03-19}
20/03/19 04:50:16 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 04:50:16 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4041
20/03/19 04:50:16 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 04:50:17 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 04:50:17 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 04:50:17 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 04:50:17 INFO YarnClientSchedulerBackend: Stopped
20/03/19 04:50:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 04:50:17 INFO MemoryStore: MemoryStore cleared
20/03/19 04:50:17 INFO BlockManager: BlockManager stopped
20/03/19 04:50:17 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 04:50:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 04:50:17 INFO SparkContext: Successfully stopped SparkContext
20/03/19 04:50:17 INFO ShutdownHookManager: Shutdown hook called
20/03/19 04:50:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7afdbae-a904-4dfd-b130-6e6d7764dace
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584564617
+ DIFF_TIME=56
+ echo 'END_TIME: ' 1584564617
+ echo 'Total time taken: ' 56
+ attempt_num=23
+ sleep 30m
+ '[' 23 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584564593
++ date +%s
+ START_TIME=1584566417
+ echo 1584566417
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584564593 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 05:20:18 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 05:20:19 INFO SparkContext: Submitted application: CoolLogtest
20/03/19 05:20:19 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 05:20:19 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 05:20:19 INFO SecurityManager: Changing view acls groups to: 
20/03/19 05:20:19 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 05:20:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 05:20:19 INFO Utils: Successfully started service 'sparkDriver' on port 40973.
20/03/19 05:20:19 INFO SparkEnv: Registering MapOutputTracker
20/03/19 05:20:19 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 05:20:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 05:20:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 05:20:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-637d0ce8-73d2-45a5-a3c9-64bcc110458e
20/03/19 05:20:19 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 05:20:19 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 05:20:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 05:20:20 INFO Utils: Successfully started service 'SparkUI' on port 4041.
20/03/19 05:20:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4041
20/03/19 05:20:20 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:40973/jars/CoolPocTest.jar with timestamp 1584566420173
20/03/19 05:20:21 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 05:20:21 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 05:20:21 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 05:20:21 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 05:20:21 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 05:20:21 INFO Client: Setting up container launch context for our AM
20/03/19 05:20:21 INFO Client: Setting up the launch environment for our AM container
20/03/19 05:20:21 INFO Client: Preparing resources for our AM container
20/03/19 05:20:21 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 05:20:21 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7097253 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 05:20:22 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 05:20:22 INFO metastore: Connected to metastore.
20/03/19 05:20:40 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ef 85 10 27 8a 01 71 13 91 94 27 8e 04 9a 8e 02 7a
20/03/19 05:20:40 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 05:20:40 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 05:20:40 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75616/datanucleus-api-jdo-3.2.6.jar
20/03/19 05:20:40 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75616/datanucleus-core-3.2.10.jar
20/03/19 05:20:40 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75616/datanucleus-rdbms-3.2.9.jar
20/03/19 05:20:40 INFO Client: Uploading resource file:/tmp/spark-d646660b-0a1c-4f06-b16d-f92d8ab14849/__spark_conf__1054796119172729194.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75616/__spark_conf__.zip
20/03/19 05:20:40 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 05:20:40 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 05:20:40 INFO SecurityManager: Changing view acls groups to: 
20/03/19 05:20:40 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 05:20:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 05:20:40 INFO Client: Submitting application application_1583994958990_75616 to ResourceManager
20/03/19 05:20:40 INFO YarnClientImpl: Submitted application application_1583994958990_75616
20/03/19 05:20:40 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_75616 and attemptId None
20/03/19 05:20:41 INFO Client: Application report for application_1583994958990_75616 (state: ACCEPTED)
20/03/19 05:20:41 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584566440655
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75616/
	 user: 1619795
20/03/19 05:20:42 INFO Client: Application report for application_1583994958990_75616 (state: ACCEPTED)
20/03/19 05:20:43 INFO Client: Application report for application_1583994958990_75616 (state: ACCEPTED)
20/03/19 05:20:44 INFO Client: Application report for application_1583994958990_75616 (state: ACCEPTED)
20/03/19 05:20:45 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_75616,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75616), /proxy/application_1583994958990_75616
20/03/19 05:20:45 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 05:20:45 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 05:20:45 INFO Client: Application report for application_1583994958990_75616 (state: RUNNING)
20/03/19 05:20:45 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.49
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584566440655
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75616/
	 user: 1619795
20/03/19 05:20:45 INFO YarnClientSchedulerBackend: Application application_1583994958990_75616 has started running.
20/03/19 05:20:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37251.
20/03/19 05:20:45 INFO NettyBlockTransferService: Server created on 10.20.174.137:37251
20/03/19 05:20:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 05:20:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 37251, None)
20/03/19 05:20:45 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:37251 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 37251, None)
20/03/19 05:20:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 37251, None)
20/03/19 05:20:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 37251, None)
20/03/19 05:20:46 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_75616
20/03/19 05:20:50 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584564593
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584564593
*************************************** CURRENT TIME |1584566450|************************
20/03/19 05:20:50 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/19 05:20:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/19 05:20:50 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/19 05:20:50 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.20.174.139:59468) with ID 1
20/03/19 05:20:50 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas05.hk.standardchartered.com:34692 with 912.3 MB RAM, BlockManagerId(1, hklpathas05.hk.standardchartered.com, 34692, None)
20/03/19 05:20:50 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/19 05:20:51 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 05:20:51 INFO metastore: Connected to metastore.
20/03/19 05:21:08 INFO SessionState: Created local directory: /tmp/e4ff4c7b-608a-4a59-ad56-685390dea479_resources
20/03/19 05:21:08 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/e4ff4c7b-608a-4a59-ad56-685390dea479
20/03/19 05:21:08 INFO SessionState: Created local directory: /tmp/1619795/e4ff4c7b-608a-4a59-ad56-685390dea479
20/03/19 05:21:08 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/e4ff4c7b-608a-4a59-ad56-685390dea479/_tmp_space.db
20/03/19 05:21:08 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 05:21:09 INFO SessionState: Created local directory: /tmp/dd43fef7-5ee8-4e43-b285-c18a0073f073_resources
20/03/19 05:21:09 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/dd43fef7-5ee8-4e43-b285-c18a0073f073
20/03/19 05:21:09 INFO SessionState: Created local directory: /tmp/1619795/dd43fef7-5ee8-4e43-b285-c18a0073f073
20/03/19 05:21:09 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/dd43fef7-5ee8-4e43-b285-c18a0073f073/_tmp_space.db
20/03/19 05:21:09 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 05:21:09 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/19 05:21:09 INFO CodeGenerator: Code generated in 157.045753 ms
20/03/19 05:21:10 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/19 05:21:10 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/19 05:21:10 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/19 05:21:10 INFO DAGScheduler: Parents of final stage: List()
20/03/19 05:21:10 INFO DAGScheduler: Missing parents: List()
20/03/19 05:21:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/19 05:21:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 05:21:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 05:21:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:37251 (size: 5.0 KB, free: 366.3 MB)
20/03/19 05:21:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/19 05:21:10 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 05:21:10 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/19 05:21:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas05.hk.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 05:21:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas05.hk.standardchartered.com:34692 (size: 5.0 KB, free: 912.3 MB)
20/03/19 05:21:10 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas05.hk.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 05:21:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 669 ms on hklpathas05.hk.standardchartered.com (executor 1) (1/4)
20/03/19 05:21:10 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas05.hk.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 05:21:10 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 22 ms on hklpathas05.hk.standardchartered.com (executor 1) (2/4)
20/03/19 05:21:10 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas05.hk.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 05:21:10 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 12 ms on hklpathas05.hk.standardchartered.com (executor 1) (3/4)
20/03/19 05:21:11 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 216 ms on hklpathas05.hk.standardchartered.com (executor 1) (4/4)
20/03/19 05:21:11 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 05:21:11 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.912 s
20/03/19 05:21:11 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.080110 s
20/03/19 05:21:11 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/19 05:21:11 INFO ContextCleaner: Cleaned accumulator 0
20/03/19 05:21:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:37251 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 05:21:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas05.hk.standardchartered.com:34692 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 05:21:11 INFO ContextCleaner: Cleaned accumulator 1
20/03/19 05:21:11 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/19 05:21:11 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/19 05:21:11 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/19 05:21:11 INFO DAGScheduler: Parents of final stage: List()
20/03/19 05:21:11 INFO DAGScheduler: Missing parents: List()
20/03/19 05:21:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/19 05:21:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 05:21:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 05:21:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:37251 (size: 5.0 KB, free: 366.3 MB)
20/03/19 05:21:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 05:21:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 05:21:11 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/19 05:21:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas05.hk.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 05:21:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas05.hk.standardchartered.com:34692 (size: 5.0 KB, free: 912.3 MB)
20/03/19 05:21:11 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas05.hk.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 05:21:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 19 ms on hklpathas05.hk.standardchartered.com (executor 1) (1/4)
20/03/19 05:21:11 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas05.hk.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 05:21:11 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 7 ms on hklpathas05.hk.standardchartered.com (executor 1) (2/4)
20/03/19 05:21:11 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas05.hk.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 05:21:11 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 6 ms on hklpathas05.hk.standardchartered.com (executor 1) (3/4)
20/03/19 05:21:11 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 16 ms on hklpathas05.hk.standardchartered.com (executor 1) (4/4)
20/03/19 05:21:11 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/19 05:21:11 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.046 s
20/03/19 05:21:11 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.053539 s
20/03/19 05:21:11 INFO ContextCleaner: Cleaned accumulator 51
20/03/19 05:21:11 INFO ContextCleaner: Cleaned accumulator 52
20/03/19 05:21:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:37251 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 05:21:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas05.hk.standardchartered.com:34692 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 05:21:12 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_05-21-12_101_8029149397785194541-1
20/03/19 05:21:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/19 05:21:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/19 05:21:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/19 05:21:12 INFO CodeGenerator: Code generated in 64.748447 ms
20/03/19 05:21:12 INFO CodeGenerator: Code generated in 42.776874 ms
20/03/19 05:21:12 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/19 05:21:12 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/19 05:21:12 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/19 05:21:12 INFO DAGScheduler: Parents of final stage: List()
20/03/19 05:21:12 INFO DAGScheduler: Missing parents: List()
20/03/19 05:21:12 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/19 05:21:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/19 05:21:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/19 05:21:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:37251 (size: 114.9 KB, free: 366.2 MB)
20/03/19 05:21:12 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/19 05:21:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/19 05:21:12 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/19 05:21:12 WARN TaskSetManager: Stage 2 contains a task of very large size (136 KB). The maximum recommended task size is 100 KB.
20/03/19 05:21:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas05.hk.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 139531 bytes)
20/03/19 05:21:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas05.hk.standardchartered.com:34692 (size: 114.9 KB, free: 912.2 MB)
20/03/19 05:21:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2515 ms on hklpathas05.hk.standardchartered.com (executor 1) (1/1)
20/03/19 05:21:15 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/19 05:21:15 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.517 s
20/03/19 05:21:15 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.593519 s
20/03/19 05:21:15 INFO FileFormatWriter: Job null committed.
20/03/19 05:21:15 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/19 05:21:15 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_05-21-12_101_8029149397785194541-1/-ext-10000/ods=2020-03-19/part-00000-b6f208f8-553e-4df1-b7f8-b9808ee89b06.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19/part-00000-b6f208f8-553e-4df1-b7f8-b9808ee89b06.c000, Status:true
20/03/19 05:21:15 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_05-21-12_101_8029149397785194541-1/-ext-10000/ods=2020-03-19 with partSpec {ods=2020-03-19}
20/03/19 05:21:15 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 05:21:15 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4041
20/03/19 05:21:15 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 05:21:15 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 05:21:15 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 05:21:15 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 05:21:15 INFO YarnClientSchedulerBackend: Stopped
20/03/19 05:21:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 05:21:15 INFO MemoryStore: MemoryStore cleared
20/03/19 05:21:15 INFO BlockManager: BlockManager stopped
20/03/19 05:21:15 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 05:21:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 05:21:15 INFO SparkContext: Successfully stopped SparkContext
20/03/19 05:21:15 INFO ShutdownHookManager: Shutdown hook called
20/03/19 05:21:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-d646660b-0a1c-4f06-b16d-f92d8ab14849
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584566476
+ DIFF_TIME=59
+ echo 'END_TIME: ' 1584566476
+ echo 'Total time taken: ' 59
+ attempt_num=24
+ sleep 30m
+ '[' 24 -le 24 ']'
+ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
cat $start_time_file
++ cat /CTRLFW/OCIR/data/yarn_logs//startTime_test.txt
+ START_TIME1=1584566450
++ date +%s
+ START_TIME=1584568276
+ echo 1584568276
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.CoolLogtest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar 1584566450 ocirappdev
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 05:51:17 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 05:51:18 INFO SparkContext: Submitted application: CoolLogtest
20/03/19 05:51:18 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 05:51:18 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 05:51:18 INFO SecurityManager: Changing view acls groups to: 
20/03/19 05:51:18 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 05:51:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 05:51:18 INFO Utils: Successfully started service 'sparkDriver' on port 43749.
20/03/19 05:51:18 INFO SparkEnv: Registering MapOutputTracker
20/03/19 05:51:18 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 05:51:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 05:51:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 05:51:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-abb8e15c-ba2a-4122-83fd-2bc71b2cd36d
20/03/19 05:51:18 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 05:51:18 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 05:51:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 05:51:18 INFO Utils: Successfully started service 'SparkUI' on port 4041.
20/03/19 05:51:18 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4041
20/03/19 05:51:18 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:43749/jars/CoolPocTest.jar with timestamp 1584568278892
20/03/19 05:51:19 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 05:51:20 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 05:51:20 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 05:51:20 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 05:51:20 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 05:51:20 INFO Client: Setting up container launch context for our AM
20/03/19 05:51:20 INFO Client: Setting up the launch environment for our AM container
20/03/19 05:51:20 INFO Client: Preparing resources for our AM container
20/03/19 05:51:20 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 05:51:20 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7098517 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 05:51:21 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 05:51:21 INFO metastore: Connected to metastore.
20/03/19 05:51:38 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ef a1 6a ad 8a 01 71 13 ad ee ad 8e 04 9c 8e 02 7a
20/03/19 05:51:38 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 05:51:38 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 05:51:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75700/datanucleus-api-jdo-3.2.6.jar
20/03/19 05:51:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75700/datanucleus-core-3.2.10.jar
20/03/19 05:51:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75700/datanucleus-rdbms-3.2.9.jar
20/03/19 05:51:38 INFO Client: Uploading resource file:/tmp/spark-cfae2f57-5c24-49fa-a31b-f19c5e4c861a/__spark_conf__8717151257856049476.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75700/__spark_conf__.zip
20/03/19 05:51:38 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 05:51:38 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 05:51:38 INFO SecurityManager: Changing view acls groups to: 
20/03/19 05:51:38 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 05:51:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 05:51:38 INFO Client: Submitting application application_1583994958990_75700 to ResourceManager
20/03/19 05:51:39 INFO YarnClientImpl: Submitted application application_1583994958990_75700
20/03/19 05:51:39 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_75700 and attemptId None
20/03/19 05:51:40 INFO Client: Application report for application_1583994958990_75700 (state: ACCEPTED)
20/03/19 05:51:40 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584568298841
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75700/
	 user: 1619795
20/03/19 05:51:41 INFO Client: Application report for application_1583994958990_75700 (state: ACCEPTED)
20/03/19 05:51:42 INFO Client: Application report for application_1583994958990_75700 (state: ACCEPTED)
20/03/19 05:51:43 INFO Client: Application report for application_1583994958990_75700 (state: ACCEPTED)
20/03/19 05:51:43 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_75700,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75700), /proxy/application_1583994958990_75700
20/03/19 05:51:43 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 05:51:43 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 05:51:44 INFO Client: Application report for application_1583994958990_75700 (state: RUNNING)
20/03/19 05:51:44 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.48
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584568298841
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75700/
	 user: 1619795
20/03/19 05:51:44 INFO YarnClientSchedulerBackend: Application application_1583994958990_75700 has started running.
20/03/19 05:51:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43551.
20/03/19 05:51:44 INFO NettyBlockTransferService: Server created on 10.20.174.137:43551
20/03/19 05:51:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 05:51:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 43551, None)
20/03/19 05:51:44 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:43551 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 43551, None)
20/03/19 05:51:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 43551, None)
20/03/19 05:51:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 43551, None)
20/03/19 05:51:44 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_75700
20/03/19 05:51:46 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.48:45918) with ID 1
20/03/19 05:51:46 INFO BlockManagerMasterEndpoint: Registering block manager hklpathas018.global.standardchartered.com:38735 with 912.3 MB RAM, BlockManagerId(1, hklpathas018.global.standardchartered.com, 38735, None)
20/03/19 05:51:46 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
***********************PRINTING URL1 *****************
http://hklpathas02.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584566450
***********************PRINTING URL2 *****************
http://hklpathas01.hk.standardchartered.com:8088/ws/v1/cluster/apps?user=ocirappdev&startedTimeBegin=1584566450
*************************************** CURRENT TIME |1584568306|************************
20/03/19 05:51:47 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.4.149-3/0/hive-site.xml
20/03/19 05:51:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse').
20/03/19 05:51:47 INFO SharedState: Warehouse path is 'file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse'.
20/03/19 05:51:47 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/03/19 05:51:47 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 05:51:47 INFO metastore: Connected to metastore.
20/03/19 05:52:04 INFO SessionState: Created local directory: /tmp/c7240958-1d06-45c3-bd67-263d7055b35b_resources
20/03/19 05:52:04 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/c7240958-1d06-45c3-bd67-263d7055b35b
20/03/19 05:52:04 INFO SessionState: Created local directory: /tmp/1619795/c7240958-1d06-45c3-bd67-263d7055b35b
20/03/19 05:52:04 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/c7240958-1d06-45c3-bd67-263d7055b35b/_tmp_space.db
20/03/19 05:52:04 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 05:52:04 INFO SessionState: Created local directory: /tmp/8a29addf-31bf-446d-8cbc-81b03bbcf338_resources
20/03/19 05:52:05 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/8a29addf-31bf-446d-8cbc-81b03bbcf338
20/03/19 05:52:05 INFO SessionState: Created local directory: /tmp/1619795/8a29addf-31bf-446d-8cbc-81b03bbcf338
20/03/19 05:52:05 INFO SessionState: Created HDFS directory: /tmp/hive/1619795/8a29addf-31bf-446d-8cbc-81b03bbcf338/_tmp_space.db
20/03/19 05:52:05 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/CTRLFW/OCIR/data/yarn_logs/spark-warehouse
20/03/19 05:52:05 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/19 05:52:05 INFO CodeGenerator: Code generated in 153.128291 ms
20/03/19 05:52:06 INFO SparkContext: Starting job: json at CoolLogtest.scala:59
20/03/19 05:52:06 INFO DAGScheduler: Got job 0 (json at CoolLogtest.scala:59) with 4 output partitions
20/03/19 05:52:06 INFO DAGScheduler: Final stage: ResultStage 0 (json at CoolLogtest.scala:59)
20/03/19 05:52:06 INFO DAGScheduler: Parents of final stage: List()
20/03/19 05:52:06 INFO DAGScheduler: Missing parents: List()
20/03/19 05:52:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59), which has no missing parents
20/03/19 05:52:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 05:52:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 05:52:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:43551 (size: 5.0 KB, free: 366.3 MB)
20/03/19 05:52:06 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
20/03/19 05:52:06 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at json at CoolLogtest.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 05:52:06 INFO YarnScheduler: Adding task set 0.0 with 4 tasks
20/03/19 05:52:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpathas018.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 05:52:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpathas018.global.standardchartered.com:38735 (size: 5.0 KB, free: 912.3 MB)
20/03/19 05:52:06 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hklpathas018.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 05:52:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 658 ms on hklpathas018.global.standardchartered.com (executor 1) (1/4)
20/03/19 05:52:06 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hklpathas018.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 05:52:06 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 17 ms on hklpathas018.global.standardchartered.com (executor 1) (2/4)
20/03/19 05:52:06 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hklpathas018.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 05:52:06 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 11 ms on hklpathas018.global.standardchartered.com (executor 1) (3/4)
20/03/19 05:52:07 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 195 ms on hklpathas018.global.standardchartered.com (executor 1) (4/4)
20/03/19 05:52:07 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 05:52:07 INFO DAGScheduler: ResultStage 0 (json at CoolLogtest.scala:59) finished in 0.876 s
20/03/19 05:52:07 INFO DAGScheduler: Job 0 finished: json at CoolLogtest.scala:59, took 1.050344 s
20/03/19 05:52:07 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/19 05:52:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.20.174.137:43551 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 05:52:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hklpathas018.global.standardchartered.com:38735 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 05:52:07 INFO ContextCleaner: Cleaned accumulator 1
20/03/19 05:52:07 INFO ContextCleaner: Cleaned accumulator 0
20/03/19 05:52:07 INFO SparkContext: Starting job: json at CoolLogtest.scala:60
20/03/19 05:52:07 INFO DAGScheduler: Got job 1 (json at CoolLogtest.scala:60) with 4 output partitions
20/03/19 05:52:07 INFO DAGScheduler: Final stage: ResultStage 1 (json at CoolLogtest.scala:60)
20/03/19 05:52:07 INFO DAGScheduler: Parents of final stage: List()
20/03/19 05:52:07 INFO DAGScheduler: Missing parents: List()
20/03/19 05:52:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60), which has no missing parents
20/03/19 05:52:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 366.3 MB)
20/03/19 05:52:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.3 MB)
20/03/19 05:52:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:43551 (size: 5.0 KB, free: 366.3 MB)
20/03/19 05:52:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 05:52:07 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at json at CoolLogtest.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20/03/19 05:52:07 INFO YarnScheduler: Adding task set 1.0 with 4 tasks
20/03/19 05:52:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, hklpathas018.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4839 bytes)
20/03/19 05:52:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpathas018.global.standardchartered.com:38735 (size: 5.0 KB, free: 912.3 MB)
20/03/19 05:52:07 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, hklpathas018.global.standardchartered.com, executor 1, partition 1, PROCESS_LOCAL, 4839 bytes)
20/03/19 05:52:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 20 ms on hklpathas018.global.standardchartered.com (executor 1) (1/4)
20/03/19 05:52:07 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, hklpathas018.global.standardchartered.com, executor 1, partition 2, PROCESS_LOCAL, 4839 bytes)
20/03/19 05:52:07 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 7 ms on hklpathas018.global.standardchartered.com (executor 1) (2/4)
20/03/19 05:52:07 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, hklpathas018.global.standardchartered.com, executor 1, partition 3, PROCESS_LOCAL, 71815 bytes)
20/03/19 05:52:07 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 7 ms on hklpathas018.global.standardchartered.com (executor 1) (3/4)
20/03/19 05:52:07 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 16 ms on hklpathas018.global.standardchartered.com (executor 1) (4/4)
20/03/19 05:52:07 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/03/19 05:52:07 INFO DAGScheduler: ResultStage 1 (json at CoolLogtest.scala:60) finished in 0.049 s
20/03/19 05:52:07 INFO DAGScheduler: Job 1 finished: json at CoolLogtest.scala:60, took 0.056588 s
20/03/19 05:52:07 INFO ContextCleaner: Cleaned accumulator 51
20/03/19 05:52:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.20.174.137:43551 in memory (size: 5.0 KB, free: 366.3 MB)
20/03/19 05:52:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hklpathas018.global.standardchartered.com:38735 in memory (size: 5.0 KB, free: 912.3 MB)
20/03/19 05:52:07 INFO ContextCleaner: Cleaned accumulator 52
20/03/19 05:52:08 INFO FileUtils: Creating directory if it doesn't exist: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_05-52-08_263_2679988905798626250-1
20/03/19 05:52:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/03/19 05:52:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/03/19 05:52:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/03/19 05:52:08 INFO CodeGenerator: Code generated in 67.059044 ms
20/03/19 05:52:08 INFO CodeGenerator: Code generated in 45.107626 ms
20/03/19 05:52:08 INFO SparkContext: Starting job: saveAsTable at CoolLogtest.scala:82
20/03/19 05:52:08 INFO DAGScheduler: Got job 2 (saveAsTable at CoolLogtest.scala:82) with 1 output partitions
20/03/19 05:52:08 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at CoolLogtest.scala:82)
20/03/19 05:52:08 INFO DAGScheduler: Parents of final stage: List()
20/03/19 05:52:08 INFO DAGScheduler: Missing parents: List()
20/03/19 05:52:08 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82), which has no missing parents
20/03/19 05:52:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 366.0 MB)
20/03/19 05:52:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 114.9 KB, free 365.9 MB)
20/03/19 05:52:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.20.174.137:43551 (size: 114.9 KB, free: 366.2 MB)
20/03/19 05:52:08 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/03/19 05:52:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at saveAsTable at CoolLogtest.scala:82) (first 15 tasks are for partitions Vector(0))
20/03/19 05:52:08 INFO YarnScheduler: Adding task set 2.0 with 1 tasks
20/03/19 05:52:08 WARN TaskSetManager: Stage 2 contains a task of very large size (136 KB). The maximum recommended task size is 100 KB.
20/03/19 05:52:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, hklpathas018.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 139531 bytes)
20/03/19 05:52:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hklpathas018.global.standardchartered.com:38735 (size: 114.9 KB, free: 912.2 MB)
20/03/19 05:52:11 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 2474 ms on hklpathas018.global.standardchartered.com (executor 1) (1/1)
20/03/19 05:52:11 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/03/19 05:52:11 INFO DAGScheduler: ResultStage 2 (saveAsTable at CoolLogtest.scala:82) finished in 2.475 s
20/03/19 05:52:11 INFO DAGScheduler: Job 2 finished: saveAsTable at CoolLogtest.scala:82, took 2.575746 s
20/03/19 05:52:11 INFO FileFormatWriter: Job null committed.
20/03/19 05:52:11 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20/03/19 05:52:11 INFO Hive: Renaming src: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_05-52-08_263_2679988905798626250-1/-ext-10000/ods=2020-03-19/part-00000-5416fa1d-28b6-4039-8a39-023a80b57d5d.c000, dest: hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/ods=2020-03-19/part-00000-5416fa1d-28b6-4039-8a39-023a80b57d5d.c000, Status:true
20/03/19 05:52:11 INFO Hive: New loading path = hdfs://nnscbhaastest/prd/edm/hadoop/ocir/sit/Tier3_OCIR/t3_gp_open/cool_yarn_logs/.hive-staging_hive_2020-03-19_05-52-08_263_2679988905798626250-1/-ext-10000/ods=2020-03-19 with partSpec {ods=2020-03-19}
20/03/19 05:52:11 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 05:52:11 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4041
20/03/19 05:52:11 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 05:52:11 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 05:52:11 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 05:52:11 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 05:52:11 INFO YarnClientSchedulerBackend: Stopped
20/03/19 05:52:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 05:52:11 INFO MemoryStore: MemoryStore cleared
20/03/19 05:52:11 INFO BlockManager: BlockManager stopped
20/03/19 05:52:11 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 05:52:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 05:52:11 INFO SparkContext: Successfully stopped SparkContext
20/03/19 05:52:11 INFO ShutdownHookManager: Shutdown hook called
20/03/19 05:52:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-cfae2f57-5c24-49fa-a31b-f19c5e4c861a
+ '[' 0 -eq 0 ']'
+ echo ' API data loaded successfully for tracking '
+ grep 'CURRENT TIME' /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ cut '-d|' -f2
++ date +%s
+ END_TIME=1584568332
+ DIFF_TIME=56
+ echo 'END_TIME: ' 1584568332
+ echo 'Total time taken: ' 56
+ attempt_num=25
+ sleep 30m
+ '[' 25 -le 24 ']'

df -h > ${file_path}/test_dfdata.txt
+ df -h
hadoop fs -df -h > ${file_path}/test_hdfsdata.txt
+ hadoop fs -df -h
sed 's/  */,/g' ${file_path}/test_dfdata.txt > ${file_path}/test_dfdata2.txt
+ sed 's/  */,/g' /CTRLFW/OCIR/data/yarn_logs//test_dfdata.txt
sed 's/  */,/g' ${file_path}/test_hdfsdata.txt > ${file_path}/test_hdfsdata2.txt
+ sed 's/  */,/g' /CTRLFW/OCIR/data/yarn_logs//test_hdfsdata.txt

START_TIME=$(date +%s)
++ date +%s
+ START_TIME=1584570136
echo ${START_TIME} >> $log_file
+ echo 1584570136

file1=${file_path}/test_dfdata2.txt
+ file1=/CTRLFW/OCIR/data/yarn_logs//test_dfdata2.txt
file2=${file_path}/test_hdfsdata2.txt
+ file2=/CTRLFW/OCIR/data/yarn_logs//test_hdfsdata2.txt

export SPARK_MAJOR_VERSION=2
+ export SPARK_MAJOR_VERSION=2
+ SPARK_MAJOR_VERSION=2
#Spark-JOB to fetch the API data and load it into table
spark-submit --class ${COOL_SPARK_JOB_DF_NAME} --master yarn --driver-memory ${SPARK_DRIVER_MEMORY} --executor-cores ${SPARK_NUM_CORES} --executor-memory ${SPARK_EXEC_MEMORY} --num-executors ${SPARK_NUM_EXECUTORS} --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=${SPARK_SHUFFLE_PARTITIONS} --conf spark.default.parallelism=${SPARK_DEFAULT_PARALLELISM}  --jars ${HIVE_JARS_PATH} $jar_path/${COOL_JAR_NAME} ${file1} ${file2} 2>&1 | tee ${log_file}
+ tee /CTRLFW/OCIR/data/yarn_logs//yarn_log20200318095241.log
+ spark-submit --class com.scb.cib.DfDatatest --master yarn --driver-memory 1G --executor-cores 1 --executor-memory 2G --num-executors 1 --conf spark.port.maxRetries=50 --conf spark.sql.shuffle.partitions=4 --conf spark.default.parallelism=4 --jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar /CTRLFW/OCIR/data/yarn_logs//CoolPocTest.jar /CTRLFW/OCIR/data/yarn_logs//test_dfdata2.txt /CTRLFW/OCIR/data/yarn_logs//test_hdfsdata2.txt
SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 06:22:17 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 06:22:18 INFO SparkContext: Submitted application: DFDatatest
20/03/19 06:22:18 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 06:22:18 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 06:22:18 INFO SecurityManager: Changing view acls groups to: 
20/03/19 06:22:18 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 06:22:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 06:22:18 INFO Utils: Successfully started service 'sparkDriver' on port 42452.
20/03/19 06:22:18 INFO SparkEnv: Registering MapOutputTracker
20/03/19 06:22:18 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 06:22:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 06:22:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 06:22:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-716b0149-8385-470a-a947-025912773c58
20/03/19 06:22:18 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 06:22:18 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 06:22:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 06:22:18 INFO Utils: Successfully started service 'SparkUI' on port 4041.
20/03/19 06:22:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4041
20/03/19 06:22:19 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:42452/jars/CoolPocTest.jar with timestamp 1584570139031
20/03/19 06:22:19 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 06:22:20 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 06:22:20 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 06:22:20 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 06:22:20 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 06:22:20 INFO Client: Setting up container launch context for our AM
20/03/19 06:22:20 INFO Client: Setting up the launch environment for our AM container
20/03/19 06:22:20 INFO Client: Preparing resources for our AM container
20/03/19 06:22:20 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 06:22:20 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7098641 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 06:22:21 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 06:22:21 INFO metastore: Connected to metastore.
20/03/19 06:22:38 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ef bd cc c9 8a 01 71 13 ca 50 c9 8e 04 9d 8e 02 7a
20/03/19 06:22:38 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 06:22:38 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 06:22:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75794/datanucleus-api-jdo-3.2.6.jar
20/03/19 06:22:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75794/datanucleus-core-3.2.10.jar
20/03/19 06:22:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75794/datanucleus-rdbms-3.2.9.jar
20/03/19 06:22:38 INFO Client: Uploading resource file:/tmp/spark-4a26900a-ff1e-4b12-97ba-81c8105b0c7f/__spark_conf__782442858377353649.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75794/__spark_conf__.zip
20/03/19 06:22:38 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 06:22:38 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 06:22:38 INFO SecurityManager: Changing view acls groups to: 
20/03/19 06:22:38 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 06:22:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 06:22:38 INFO Client: Submitting application application_1583994958990_75794 to ResourceManager
20/03/19 06:22:39 INFO YarnClientImpl: Submitted application application_1583994958990_75794
20/03/19 06:22:39 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_75794 and attemptId None
20/03/19 06:22:40 INFO Client: Application report for application_1583994958990_75794 (state: ACCEPTED)
20/03/19 06:22:40 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584570158958
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75794/
	 user: 1619795
20/03/19 06:22:41 INFO Client: Application report for application_1583994958990_75794 (state: ACCEPTED)
20/03/19 06:22:42 INFO Client: Application report for application_1583994958990_75794 (state: ACCEPTED)
20/03/19 06:22:43 INFO Client: Application report for application_1583994958990_75794 (state: ACCEPTED)
20/03/19 06:22:43 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_75794,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75794), /proxy/application_1583994958990_75794
20/03/19 06:22:43 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 06:22:44 INFO Client: Application report for application_1583994958990_75794 (state: RUNNING)
20/03/19 06:22:44 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.48
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584570158958
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75794/
	 user: 1619795
20/03/19 06:22:44 INFO YarnClientSchedulerBackend: Application application_1583994958990_75794 has started running.
20/03/19 06:22:44 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 06:22:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41996.
20/03/19 06:22:44 INFO NettyBlockTransferService: Server created on 10.20.174.137:41996
20/03/19 06:22:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 06:22:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 41996, None)
20/03/19 06:22:44 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:41996 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 41996, None)
20/03/19 06:22:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 41996, None)
20/03/19 06:22:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 41996, None)
20/03/19 06:22:44 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_75794
20/03/19 06:22:49 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
20/03/19 06:22:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 424.8 KB, free 365.9 MB)
20/03/19 06:22:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.3 KB, free 365.8 MB)
20/03/19 06:22:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:41996 (size: 38.3 KB, free: 366.3 MB)
20/03/19 06:22:49 INFO SparkContext: Created broadcast 0 from textFile at DfDatatest.scala:42
20/03/19 06:22:49 INFO FileInputFormat: Total input paths to process : 1
20/03/19 06:22:49 INFO SparkContext: Starting job: first at DfDatatest.scala:43
20/03/19 06:22:49 INFO DAGScheduler: Got job 0 (first at DfDatatest.scala:43) with 1 output partitions
20/03/19 06:22:49 INFO DAGScheduler: Final stage: ResultStage 0 (first at DfDatatest.scala:43)
20/03/19 06:22:49 INFO DAGScheduler: Parents of final stage: List()
20/03/19 06:22:49 INFO DAGScheduler: Missing parents: List()
20/03/19 06:22:49 INFO DAGScheduler: Submitting ResultStage 0 (file:///CTRLFW/OCIR/data/yarn_logs//test_dfdata2.txt MapPartitionsRDD[1] at textFile at DfDatatest.scala:42), which has no missing parents
20/03/19 06:22:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 365.8 MB)
20/03/19 06:22:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2001.0 B, free 365.8 MB)
20/03/19 06:22:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:41996 (size: 2001.0 B, free: 366.3 MB)
20/03/19 06:22:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 06:22:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (file:///CTRLFW/OCIR/data/yarn_logs//test_dfdata2.txt MapPartitionsRDD[1] at textFile at DfDatatest.scala:42) (first 15 tasks are for partitions Vector(0))
20/03/19 06:22:49 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/03/19 06:22:56 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.49:54614) with ID 1
20/03/19 06:22:56 INFO BlockManagerMasterEndpoint: Registering block manager hklpadhas013.global.standardchartered.com:38643 with 912.3 MB RAM, BlockManagerId(1, hklpadhas013.global.standardchartered.com, 38643, None)
20/03/19 06:22:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4880 bytes)
20/03/19 06:22:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpadhas013.global.standardchartered.com:38643 (size: 2001.0 B, free: 912.3 MB)
20/03/19 06:22:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpadhas013.global.standardchartered.com:38643 (size: 38.3 KB, free: 912.3 MB)
20/03/19 06:22:57 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, hklpadhas013.global.standardchartered.com, executor 1): java.io.FileNotFoundException: File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:348)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:786)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:251)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:250)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

20/03/19 06:22:57 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4880 bytes)
20/03/19 06:22:57 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on hklpadhas013.global.standardchartered.com, executor 1: java.io.FileNotFoundException (File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist) [duplicate 1]
20/03/19 06:22:57 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4880 bytes)
20/03/19 06:22:57 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on hklpadhas013.global.standardchartered.com, executor 1: java.io.FileNotFoundException (File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist) [duplicate 2]
20/03/19 06:22:57 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4880 bytes)
20/03/19 06:22:57 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on hklpadhas013.global.standardchartered.com, executor 1: java.io.FileNotFoundException (File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist) [duplicate 3]
20/03/19 06:22:57 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
20/03/19 06:22:57 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 06:22:57 INFO YarnScheduler: Cancelling stage 0
20/03/19 06:22:57 INFO DAGScheduler: ResultStage 0 (first at DfDatatest.scala:43) failed in 7.742 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, hklpadhas013.global.standardchartered.com, executor 1): java.io.FileNotFoundException: File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:348)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:786)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:251)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:250)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
20/03/19 06:22:57 INFO DAGScheduler: Job 0 failed: first at DfDatatest.scala:43, took 7.817023 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, hklpadhas013.global.standardchartered.com, executor 1): java.io.FileNotFoundException: File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:348)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:786)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:251)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:250)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)
	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1327)
	at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1368)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.first(RDD.scala:1367)
	at com.scb.cib.DfDatatest$.main(DfDatatest.scala:43)
	at com.scb.cib.DfDatatest.main(DfDatatest.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:782)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:348)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:786)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:251)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:250)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
20/03/19 06:22:57 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 06:22:57 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4041
20/03/19 06:22:57 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 06:22:57 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 06:22:57 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 06:22:57 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 06:22:57 INFO YarnClientSchedulerBackend: Stopped
20/03/19 06:22:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 06:22:57 INFO MemoryStore: MemoryStore cleared
20/03/19 06:22:57 INFO BlockManager: BlockManager stopped
20/03/19 06:22:57 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 06:22:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 06:22:57 INFO SparkContext: Successfully stopped SparkContext
20/03/19 06:22:57 INFO ShutdownHookManager: Shutdown hook called
20/03/19 06:22:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-4a26900a-ff1e-4b12-97ba-81c8105b0c7f

if [ $? -eq 0 ]; then
  echo " DF and HDFS data loaded successfully for tracking " >> ${log_file}
  #date_time=$(grep "CURRENT TIME" ${log_file} | cut -d\| -f2)
  #prepare starttime file for next run
  #grep "CURRENT TIME" ${log_file} | cut -d\| -f2 > ${start_time_file}
else
  echo "DF and HDFS data load failed" >> ${log_file}
  #exit 1
fi
+ '[' 0 -eq 0 ']'
+ echo ' DF and HDFS data loaded successfully for tracking '

END_TIME=$(date +%s)
++ date +%s
+ END_TIME=1584570177
DIFF_TIME=$(( ${END_TIME} - ${START_TIME} ))
+ DIFF_TIME=41
echo "END_TIME: " ${END_TIME} >> ${log_file}
+ echo 'END_TIME: ' 1584570177
echo "Total time taken: " ${DIFF_TIME} >> ${log_file}
+ echo 'Total time taken: ' 41


exit 0
+ exit 0
