SPARK_MAJOR_VERSION is set to 2, using Spark2
20/03/19 06:22:17 INFO SparkContext: Running Spark version 2.2.0.2.6.4.149-3
20/03/19 06:22:18 INFO SparkContext: Submitted application: DFDatatest
20/03/19 06:22:18 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 06:22:18 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 06:22:18 INFO SecurityManager: Changing view acls groups to: 
20/03/19 06:22:18 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 06:22:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 06:22:18 INFO Utils: Successfully started service 'sparkDriver' on port 42452.
20/03/19 06:22:18 INFO SparkEnv: Registering MapOutputTracker
20/03/19 06:22:18 INFO SparkEnv: Registering BlockManagerMaster
20/03/19 06:22:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/19 06:22:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/19 06:22:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-716b0149-8385-470a-a947-025912773c58
20/03/19 06:22:18 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/19 06:22:18 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/19 06:22:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/03/19 06:22:18 INFO Utils: Successfully started service 'SparkUI' on port 4041.
20/03/19 06:22:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.20.174.137:4041
20/03/19 06:22:19 INFO SparkContext: Added JAR file:/CTRLFW/OCIR/data/yarn_logs/CoolPocTest.jar at spark://10.20.174.137:42452/jars/CoolPocTest.jar with timestamp 1584570139031
20/03/19 06:22:19 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/19 06:22:20 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]
20/03/19 06:22:20 INFO Client: Requesting a new application from cluster with 17 NodeManagers
20/03/19 06:22:20 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (51200 MB per container)
20/03/19 06:22:20 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/19 06:22:20 INFO Client: Setting up container launch context for our AM
20/03/19 06:22:20 INFO Client: Setting up the launch environment for our AM container
20/03/19 06:22:20 INFO Client: Preparing resources for our AM container
20/03/19 06:22:20 INFO HadoopFSCredentialProvider: getting token for: hdfs://nnscbhaastest/user/1619795
20/03/19 06:22:20 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 7098641 for 1619795 on ha-hdfs:nnscbhaastest
20/03/19 06:22:21 INFO metastore: Trying to connect to metastore with URI thrift://hklpadhaa007.global.standardchartered.com:9083
20/03/19 06:22:21 INFO metastore: Connected to metastore.
20/03/19 06:22:38 INFO HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 18 31 36 31 39 37 39 35 40 5a 4f 4e 45 31 2e 53 43 42 44 45 56 2e 4e 45 54 04 68 69 76 65 00 8a 01 70 ef bd cc c9 8a 01 71 13 ca 50 c9 8e 04 9d 8e 02 7a
20/03/19 06:22:38 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 06:22:38 INFO Client: Source and destination file systems are the same. Not copying hdfs://nnscbhaastest/hdp/apps/2.6.4.149-3/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/19 06:22:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75794/datanucleus-api-jdo-3.2.6.jar
20/03/19 06:22:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75794/datanucleus-core-3.2.10.jar
20/03/19 06:22:38 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75794/datanucleus-rdbms-3.2.9.jar
20/03/19 06:22:38 INFO Client: Uploading resource file:/tmp/spark-4a26900a-ff1e-4b12-97ba-81c8105b0c7f/__spark_conf__782442858377353649.zip -> hdfs://nnscbhaastest/user/1619795/.sparkStaging/application_1583994958990_75794/__spark_conf__.zip
20/03/19 06:22:38 INFO SecurityManager: Changing view acls to: 1619795
20/03/19 06:22:38 INFO SecurityManager: Changing modify acls to: 1619795
20/03/19 06:22:38 INFO SecurityManager: Changing view acls groups to: 
20/03/19 06:22:38 INFO SecurityManager: Changing modify acls groups to: 
20/03/19 06:22:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1619795); groups with view permissions: Set(); users  with modify permissions: Set(1619795); groups with modify permissions: Set()
20/03/19 06:22:38 INFO Client: Submitting application application_1583994958990_75794 to ResourceManager
20/03/19 06:22:39 INFO YarnClientImpl: Submitted application application_1583994958990_75794
20/03/19 06:22:39 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1583994958990_75794 and attemptId None
20/03/19 06:22:40 INFO Client: Application report for application_1583994958990_75794 (state: ACCEPTED)
20/03/19 06:22:40 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584570158958
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75794/
	 user: 1619795
20/03/19 06:22:41 INFO Client: Application report for application_1583994958990_75794 (state: ACCEPTED)
20/03/19 06:22:42 INFO Client: Application report for application_1583994958990_75794 (state: ACCEPTED)
20/03/19 06:22:43 INFO Client: Application report for application_1583994958990_75794 (state: ACCEPTED)
20/03/19 06:22:43 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hklpathas01.hk.standardchartered.com,hklpathas02.hk.standardchartered.com, PROXY_URI_BASES -> http://hklpathas01.hk.standardchartered.com:8088/proxy/application_1583994958990_75794,http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75794), /proxy/application_1583994958990_75794
20/03/19 06:22:43 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
20/03/19 06:22:44 INFO Client: Application report for application_1583994958990_75794 (state: RUNNING)
20/03/19 06:22:44 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.23.225.48
	 ApplicationMaster RPC port: 0
	 queue: sgz1-ocirapp-haas_sit
	 start time: 1584570158958
	 final status: UNDEFINED
	 tracking URL: http://hklpathas02.hk.standardchartered.com:8088/proxy/application_1583994958990_75794/
	 user: 1619795
20/03/19 06:22:44 INFO YarnClientSchedulerBackend: Application application_1583994958990_75794 has started running.
20/03/19 06:22:44 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/19 06:22:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41996.
20/03/19 06:22:44 INFO NettyBlockTransferService: Server created on 10.20.174.137:41996
20/03/19 06:22:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/19 06:22:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.20.174.137, 41996, None)
20/03/19 06:22:44 INFO BlockManagerMasterEndpoint: Registering block manager 10.20.174.137:41996 with 366.3 MB RAM, BlockManagerId(driver, 10.20.174.137, 41996, None)
20/03/19 06:22:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.20.174.137, 41996, None)
20/03/19 06:22:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.20.174.137, 41996, None)
20/03/19 06:22:44 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1583994958990_75794
20/03/19 06:22:49 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
20/03/19 06:22:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 424.8 KB, free 365.9 MB)
20/03/19 06:22:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.3 KB, free 365.8 MB)
20/03/19 06:22:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.20.174.137:41996 (size: 38.3 KB, free: 366.3 MB)
20/03/19 06:22:49 INFO SparkContext: Created broadcast 0 from textFile at DfDatatest.scala:42
20/03/19 06:22:49 INFO FileInputFormat: Total input paths to process : 1
20/03/19 06:22:49 INFO SparkContext: Starting job: first at DfDatatest.scala:43
20/03/19 06:22:49 INFO DAGScheduler: Got job 0 (first at DfDatatest.scala:43) with 1 output partitions
20/03/19 06:22:49 INFO DAGScheduler: Final stage: ResultStage 0 (first at DfDatatest.scala:43)
20/03/19 06:22:49 INFO DAGScheduler: Parents of final stage: List()
20/03/19 06:22:49 INFO DAGScheduler: Missing parents: List()
20/03/19 06:22:49 INFO DAGScheduler: Submitting ResultStage 0 (file:///CTRLFW/OCIR/data/yarn_logs//test_dfdata2.txt MapPartitionsRDD[1] at textFile at DfDatatest.scala:42), which has no missing parents
20/03/19 06:22:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 365.8 MB)
20/03/19 06:22:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2001.0 B, free 365.8 MB)
20/03/19 06:22:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.20.174.137:41996 (size: 2001.0 B, free: 366.3 MB)
20/03/19 06:22:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/03/19 06:22:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (file:///CTRLFW/OCIR/data/yarn_logs//test_dfdata2.txt MapPartitionsRDD[1] at textFile at DfDatatest.scala:42) (first 15 tasks are for partitions Vector(0))
20/03/19 06:22:49 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/03/19 06:22:56 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.23.225.49:54614) with ID 1
20/03/19 06:22:56 INFO BlockManagerMasterEndpoint: Registering block manager hklpadhas013.global.standardchartered.com:38643 with 912.3 MB RAM, BlockManagerId(1, hklpadhas013.global.standardchartered.com, 38643, None)
20/03/19 06:22:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4880 bytes)
20/03/19 06:22:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hklpadhas013.global.standardchartered.com:38643 (size: 2001.0 B, free: 912.3 MB)
20/03/19 06:22:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hklpadhas013.global.standardchartered.com:38643 (size: 38.3 KB, free: 912.3 MB)
20/03/19 06:22:57 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, hklpadhas013.global.standardchartered.com, executor 1): java.io.FileNotFoundException: File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:348)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:786)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:251)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:250)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

20/03/19 06:22:57 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4880 bytes)
20/03/19 06:22:57 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on hklpadhas013.global.standardchartered.com, executor 1: java.io.FileNotFoundException (File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist) [duplicate 1]
20/03/19 06:22:57 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4880 bytes)
20/03/19 06:22:57 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on hklpadhas013.global.standardchartered.com, executor 1: java.io.FileNotFoundException (File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist) [duplicate 2]
20/03/19 06:22:57 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, hklpadhas013.global.standardchartered.com, executor 1, partition 0, PROCESS_LOCAL, 4880 bytes)
20/03/19 06:22:57 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on hklpadhas013.global.standardchartered.com, executor 1: java.io.FileNotFoundException (File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist) [duplicate 3]
20/03/19 06:22:57 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
20/03/19 06:22:57 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/19 06:22:57 INFO YarnScheduler: Cancelling stage 0
20/03/19 06:22:57 INFO DAGScheduler: ResultStage 0 (first at DfDatatest.scala:43) failed in 7.742 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, hklpadhas013.global.standardchartered.com, executor 1): java.io.FileNotFoundException: File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:348)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:786)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:251)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:250)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
20/03/19 06:22:57 INFO DAGScheduler: Job 0 failed: first at DfDatatest.scala:43, took 7.817023 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, hklpadhas013.global.standardchartered.com, executor 1): java.io.FileNotFoundException: File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:348)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:786)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:251)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:250)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)
	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1327)
	at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1368)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.first(RDD.scala:1367)
	at com.scb.cib.DfDatatest$.main(DfDatatest.scala:43)
	at com.scb.cib.DfDatatest.main(DfDatatest.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:782)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: File file:/CTRLFW/OCIR/data/yarn_logs/test_dfdata2.txt does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:348)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:786)
	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:251)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:250)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
20/03/19 06:22:57 INFO SparkContext: Invoking stop() from shutdown hook
20/03/19 06:22:57 INFO SparkUI: Stopped Spark web UI at http://10.20.174.137:4041
20/03/19 06:22:57 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/19 06:22:57 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/19 06:22:57 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/19 06:22:57 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/19 06:22:57 INFO YarnClientSchedulerBackend: Stopped
20/03/19 06:22:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/19 06:22:57 INFO MemoryStore: MemoryStore cleared
20/03/19 06:22:57 INFO BlockManager: BlockManager stopped
20/03/19 06:22:57 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/19 06:22:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/19 06:22:57 INFO SparkContext: Successfully stopped SparkContext
20/03/19 06:22:57 INFO ShutdownHookManager: Shutdown hook called
20/03/19 06:22:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-4a26900a-ff1e-4b12-97ba-81c8105b0c7f
 DF and HDFS data loaded successfully for tracking 
END_TIME:  1584570177
Total time taken:  41
